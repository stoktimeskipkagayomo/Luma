# api_server.py
# Luma API Backend Service

import asyncio
import json
import logging
import os
import sys
import subprocess
import time
import uuid
import re
import threading
import random
import mimetypes
from datetime import datetime
from contextlib import asynccontextmanager
from collections import deque
from threading import Lock

from pathlib import Path

import uvicorn
import requests
import aiohttp  # æ–°å¢ï¼šç”¨äºå¼‚æ­¥HTTPè¯·æ±‚
from asyncio import Semaphore
from typing import Optional, Tuple
from packaging.version import parse as parse_version
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse, Response, HTMLResponse

# --- å†…éƒ¨æ¨¡å—å¯¼å…¥ ---
from modules.file_uploader import upload_to_file_bed
from modules.monitoring import monitoring_service, MonitorConfig
from modules.token_manager import token_manager
from modules.geo_platform import geo_platform_service
from modules.logging_system import log_system, LogLevel, LogType
# å›¾åƒè‡ªåŠ¨å¢å¼ºåŠŸèƒ½å·²ç§»é™¤ï¼ˆå·²å‰¥ç¦»ä¸ºç‹¬ç«‹é¡¹ç›®ï¼‰

import urllib3
# å…¨å±€ç¦ç”¨SSLè­¦å‘Šï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- åŸºç¡€é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- æ—¥å¿—è¿‡æ»¤å™¨ ---
class EndpointFilter(logging.Filter):
    """è¿‡æ»¤æ‰ç›‘æ§ç›¸å…³çš„APIè¯·æ±‚æ—¥å¿—"""
    def filter(self, record: logging.LogRecord) -> bool:
        message = record.getMessage()
        # è¿‡æ»¤æ‰æ‰€æœ‰ /api/monitor/ å’Œ /monitor çš„GETè¯·æ±‚
        is_monitor_request = "GET /api/monitor/" in message or "GET /monitor " in message
        return not is_monitor_request

# å°†è¿‡æ»¤å™¨æ·»åŠ åˆ°uvicornçš„è®¿é—®æ—¥å¿—è®°å½•å™¨
logging.getLogger("uvicorn.access").addFilter(EndpointFilter())

# --- å…¨å±€çŠ¶æ€ä¸é…ç½® ---
CONFIG = {} # å­˜å‚¨ä» config.jsonc åŠ è½½çš„é…ç½®
# browser_ws ç”¨äºå­˜å‚¨ä¸å•ä¸ªæ²¹çŒ´è„šæœ¬çš„ WebSocket è¿æ¥ã€‚
# æ³¨æ„ï¼šæ­¤æ¶æ„å‡å®šåªæœ‰ä¸€ä¸ªæµè§ˆå™¨æ ‡ç­¾é¡µåœ¨å·¥ä½œã€‚
# å¦‚æœéœ€è¦æ”¯æŒå¤šä¸ªå¹¶å‘æ ‡ç­¾é¡µï¼Œéœ€è¦å°†æ­¤æ‰©å±•ä¸ºå­—å…¸ç®¡ç†å¤šä¸ªè¿æ¥ã€‚
browser_ws: WebSocket | None = None
# response_channels ç”¨äºå­˜å‚¨æ¯ä¸ª API è¯·æ±‚çš„å“åº”é˜Ÿåˆ—ã€‚
# é”®æ˜¯ request_idï¼Œå€¼æ˜¯ asyncio.Queueã€‚
response_channels: dict[str, asyncio.Queue] = {}
# æ–°å¢ï¼šè¯·æ±‚å…ƒæ•°æ®å­˜å‚¨ï¼ˆç”¨äºWebSocketé‡è¿åæ¢å¤è¯·æ±‚ï¼‰
request_metadata: dict[str, dict] = {}
last_activity_time = None # è®°å½•æœ€åä¸€æ¬¡æ´»åŠ¨çš„æ—¶é—´
idle_monitor_thread = None # ç©ºé—²ç›‘æ§çº¿ç¨‹
main_event_loop = None # ä¸»äº‹ä»¶å¾ªç¯
# æ–°å¢ï¼šç”¨äºè·Ÿè¸ªæ˜¯å¦å› äººæœºéªŒè¯è€Œåˆ·æ–°
IS_REFRESHING_FOR_VERIFICATION = False
# æ–°å¢ï¼šç”¨äºè‡ªåŠ¨é‡è¯•çš„è¯·æ±‚æš‚å­˜é˜Ÿåˆ—
pending_requests_queue = asyncio.Queue()
# æ–°å¢ï¼šWebSocketè¿æ¥é”ï¼Œä¿æŠ¤å¹¶å‘è®¿é—®
ws_lock = asyncio.Lock()
# æ–°å¢ï¼šå…¨å±€aiohttpä¼šè¯
aiohttp_session = None
# --- å›¾ç‰‡è‡ªåŠ¨ä¸‹è½½é…ç½® ---
IMAGE_SAVE_DIR = Path("./downloaded_images")
IMAGE_SAVE_DIR.mkdir(exist_ok=True)
# ä½¿ç”¨dequeé™åˆ¶å¤§å°ï¼Œé¿å…å†…å­˜æ³„æ¼
downloaded_image_urls = deque(maxlen=5000)  # æœ€å¤šè®°å½•5000ä¸ªURL
downloaded_urls_set = set()  # ç”¨äºå¿«é€ŸæŸ¥é‡
# æ–°å¢ï¼šç”¨äºåœ¨è¿è¡Œæ—¶ä¸´æ—¶ç¦ç”¨å¤±è´¥çš„å›¾åºŠç«¯ç‚¹
DISABLED_ENDPOINTS = {}  # æ”¹ä¸ºå­—å…¸ï¼Œè®°å½•ç¦ç”¨æ—¶é—´
# æ–°å¢ï¼šç”¨äºè½®è¯¢ç­–ç•¥çš„å…¨å±€ç´¢å¼•
ROUND_ROBIN_INDEX = 0
# æ–°å¢ï¼šå›¾åºŠæ¢å¤æ—¶é—´ï¼ˆç§’ï¼‰
FILEBED_RECOVERY_TIME = 300  # 5åˆ†é’Ÿåè‡ªåŠ¨æ¢å¤

# æ–°å¢ï¼šç”¨äºæ¨¡å‹IDæ˜ å°„è½®è¯¢çš„ç´¢å¼•å­—å…¸ï¼ˆéœ€è¦çº¿ç¨‹å®‰å…¨ä¿æŠ¤ï¼‰
MODEL_ROUND_ROBIN_INDEX = {}  # {model_name: current_index}
MODEL_ROUND_ROBIN_LOCK = Lock()  # ä¿æŠ¤è½®è¯¢ç´¢å¼•çš„çº¿ç¨‹é”

# æ–°å¢ï¼šå›¾ç‰‡Base64ç¼“å­˜ï¼ˆé¿å…é‡å¤ä¸‹è½½å’Œè½¬æ¢ï¼‰
IMAGE_BASE64_CACHE = {}  # {url: (base64_data, timestamp)}
IMAGE_CACHE_MAX_SIZE = 1000  # æœ€å¤šç¼“å­˜100å¼ å›¾ç‰‡
IMAGE_CACHE_TTL = 3600  # ç¼“å­˜æœ‰æ•ˆæœŸ1å°æ—¶ï¼ˆç§’ï¼‰

# æ–°å¢ï¼šå›¾åºŠURLç¼“å­˜ï¼ˆé¿å…ç›¸åŒå›¾ç‰‡é‡å¤ä¸Šä¼ ï¼‰
FILEBED_URL_CACHE = {}  # {image_hash: (uploaded_url, timestamp)}
FILEBED_URL_CACHE_TTL = 300  # å›¾åºŠé“¾æ¥ç¼“å­˜5åˆ†é’Ÿï¼ˆç§’ï¼‰
FILEBED_URL_CACHE_MAX_SIZE = 500  # æœ€å¤šç¼“å­˜500ä¸ªå›¾åºŠé“¾æ¥

# æ–°å¢ï¼šå¹¶å‘ä¸‹è½½æ§åˆ¶
DOWNLOAD_SEMAPHORE: Optional[Semaphore] = None
MAX_CONCURRENT_DOWNLOADS = 50  # é»˜è®¤æœ€å¤§å¹¶å‘ä¸‹è½½æ•°



# --- æ¨¡å‹æ˜ å°„ ---
# MODEL_NAME_TO_ID_MAP ç°åœ¨å°†å­˜å‚¨æ›´ä¸°å¯Œçš„å¯¹è±¡ï¼š { "model_name": {"id": "...", "type": "..."} }
MODEL_NAME_TO_ID_MAP = {}
MODEL_ENDPOINT_MAP = {} # æ–°å¢ï¼šç”¨äºå­˜å‚¨æ¨¡å‹åˆ° session/message ID çš„æ˜ å°„
DEFAULT_MODEL_ID = None # é»˜è®¤æ¨¡å‹id: None

def load_model_endpoint_map():
    """ä» model_endpoint_map.json åŠ è½½æ¨¡å‹åˆ°ç«¯ç‚¹çš„æ˜ å°„ã€‚"""
    global MODEL_ENDPOINT_MAP
    try:
        with open('model_endpoint_map.json', 'r', encoding='utf-8') as f:
            content = f.read()
            # å…è®¸ç©ºæ–‡ä»¶
            if not content.strip():
                MODEL_ENDPOINT_MAP = {}
            else:
                MODEL_ENDPOINT_MAP = json.loads(content)
        logger.info(f"æˆåŠŸä» 'model_endpoint_map.json' åŠ è½½äº† {len(MODEL_ENDPOINT_MAP)} ä¸ªæ¨¡å‹ç«¯ç‚¹æ˜ å°„ã€‚")
    except FileNotFoundError:
        logger.warning("'model_endpoint_map.json' æ–‡ä»¶æœªæ‰¾åˆ°ã€‚å°†ä½¿ç”¨ç©ºæ˜ å°„ã€‚")
        MODEL_ENDPOINT_MAP = {}
    except json.JSONDecodeError as e:
        logger.error(f"åŠ è½½æˆ–è§£æ 'model_endpoint_map.json' å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨ç©ºæ˜ å°„ã€‚")
        MODEL_ENDPOINT_MAP = {}

def _parse_jsonc(jsonc_string: str) -> dict:
    """
    ç¨³å¥åœ°è§£æ JSONC å­—ç¬¦ä¸²ï¼Œç§»é™¤æ³¨é‡Šã€‚
    æ”¹è¿›ç‰ˆï¼šæ­£ç¡®å¤„ç†å­—ç¬¦ä¸²å†…çš„ // å’Œ /* */
    """
    lines = jsonc_string.splitlines()
    no_comments_lines = []
    in_block_comment = False
    
    for line in lines:
        if in_block_comment:
            # åœ¨å—æ³¨é‡Šä¸­ï¼ŒæŸ¥æ‰¾ç»“æŸæ ‡è®°
            if '*/' in line:
                in_block_comment = False
                # ä¿ç•™å—æ³¨é‡Šç»“æŸåçš„å†…å®¹
                line = line.split('*/', 1)[1]
            else:
                continue
        
        # å¤„ç†å¯èƒ½çš„å—æ³¨é‡Šå¼€å§‹
        if '/*' in line:
            # éœ€è¦æ›´æ™ºèƒ½åœ°å¤„ç†ï¼Œé¿å…åˆ é™¤å­—ç¬¦ä¸²ä¸­çš„ /*
            before_comment, _, after_comment = line.partition('/*')
            if '*/' in after_comment:
                # å•è¡Œå—æ³¨é‡Š
                _, _, after_block = after_comment.partition('*/')
                line = before_comment + after_block
            else:
                # å¤šè¡Œå—æ³¨é‡Šå¼€å§‹
                line = before_comment
                in_block_comment = True
        
        # å¤„ç†å•è¡Œæ³¨é‡Š //ï¼Œä½†è¦é¿å…åˆ é™¤å­—ç¬¦ä¸²ä¸­çš„ //
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„æ–¹æ³•ï¼šæŸ¥æ‰¾ä¸åœ¨å¼•å·å†…çš„ //
        processed_line = ""
        in_string = False
        escape_next = False
        i = 0
        
        while i < len(line):
            char = line[i]
            
            if escape_next:
                processed_line += char
                escape_next = False
                i += 1
                continue
            
            if char == '\\':
                processed_line += char
                escape_next = True
                i += 1
                continue
            
            if char == '"' and not in_string:
                in_string = True
                processed_line += char
            elif char == '"' and in_string:
                in_string = False
                processed_line += char
            elif char == '/' and i + 1 < len(line) and line[i + 1] == '/' and not in_string:
                # æ‰¾åˆ°äº†çœŸæ­£çš„æ³¨é‡Šï¼Œåœæ­¢å¤„ç†è¿™ä¸€è¡Œ
                break
            else:
                processed_line += char
            
            i += 1
        
        # åªæœ‰éç©ºè¡Œæ‰æ·»åŠ 
        if processed_line.strip():
            no_comments_lines.append(processed_line)

    return json.loads("\n".join(no_comments_lines))

def load_config():
    """ä» config.jsonc åŠ è½½é…ç½®ï¼Œå¹¶å¤„ç† JSONC æ³¨é‡Šã€‚"""
    global CONFIG
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            content = f.read()
        CONFIG = _parse_jsonc(content)
        logger.info("æˆåŠŸä» 'config.jsonc' åŠ è½½é…ç½®ã€‚")
        # æ‰“å°å…³é”®é…ç½®çŠ¶æ€
        logger.info(f"  - é…’é¦†æ¨¡å¼ (Tavern Mode): {'âœ… å¯ç”¨' if CONFIG.get('tavern_mode_enabled') else 'âŒ ç¦ç”¨'}")
        logger.info(f"  - ç»•è¿‡æ¨¡å¼ (Bypass Mode): {'âœ… å¯ç”¨' if CONFIG.get('bypass_enabled') else 'âŒ ç¦ç”¨'}")
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"åŠ è½½æˆ–è§£æ 'config.jsonc' å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
        CONFIG = {}

def load_model_map():
    """ä» models.json åŠ è½½æ¨¡å‹æ˜ å°„ï¼Œæ”¯æŒ 'id:type' æ ¼å¼ã€‚"""
    global MODEL_NAME_TO_ID_MAP
    try:
        with open('models.json', 'r', encoding='utf-8') as f:
            raw_map = json.load(f)
            
        processed_map = {}
        for name, value in raw_map.items():
            if isinstance(value, str) and ':' in value:
                parts = value.split(':', 1)
                model_id = parts[0] if parts[0].lower() != 'null' else None
                model_type = parts[1]
                processed_map[name] = {"id": model_id, "type": model_type}
            else:
                # é»˜è®¤æˆ–æ—§æ ¼å¼å¤„ç†
                processed_map[name] = {"id": value, "type": "text"}

        MODEL_NAME_TO_ID_MAP = processed_map
        logger.info(f"æˆåŠŸä» 'models.json' åŠ è½½å¹¶è§£æäº† {len(MODEL_NAME_TO_ID_MAP)} ä¸ªæ¨¡å‹ã€‚")

    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"åŠ è½½ 'models.json' å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨ç©ºæ¨¡å‹åˆ—è¡¨ã€‚")
        MODEL_NAME_TO_ID_MAP = {}

# --- å…¬å‘Šå¤„ç† ---
def check_and_display_announcement():
    """æ£€æŸ¥å¹¶æ˜¾ç¤ºä¸€æ¬¡æ€§å…¬å‘Šã€‚"""
    announcement_file = "announcement-lmarena.json"
    if os.path.exists(announcement_file):
        try:
            logger.info("="*60)
            logger.info("ğŸ“¢ æ£€æµ‹åˆ°æ›´æ–°å…¬å‘Šï¼Œå†…å®¹å¦‚ä¸‹:")
            with open(announcement_file, 'r', encoding='utf-8') as f:
                announcement = json.load(f)
                title = announcement.get("title", "å…¬å‘Š")
                content = announcement.get("content", [])
                
                logger.info(f"   --- {title} ---")
                for line in content:
                    logger.info(f"   {line}")
                logger.info("="*60)

        except json.JSONDecodeError:
            logger.error(f"æ— æ³•è§£æå…¬å‘Šæ–‡ä»¶ '{announcement_file}'ã€‚æ–‡ä»¶å†…å®¹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„JSONã€‚")
        except Exception as e:
            logger.error(f"è¯»å–å…¬å‘Šæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            try:
                os.remove(announcement_file)
                logger.info(f"å…¬å‘Šæ–‡ä»¶ '{announcement_file}' å·²è¢«ç§»é™¤ã€‚")
            except OSError as e:
                logger.error(f"åˆ é™¤å…¬å‘Šæ–‡ä»¶ '{announcement_file}' å¤±è´¥: {e}")

# --- æ›´æ–°æ£€æŸ¥ ---
GITHUB_REPO = "zhongruichen/LMArenaBridge-mogai"  # Repository name unchanged

def download_and_extract_update(version):
    """ä¸‹è½½å¹¶è§£å‹æœ€æ–°ç‰ˆæœ¬åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ã€‚"""
    update_dir = "update_temp"
    if not os.path.exists(update_dir):
        os.makedirs(update_dir)

    try:
        zip_url = f"https://github.com/{GITHUB_REPO}/archive/refs/heads/mogai-version.zip"
        logger.info(f"æ­£åœ¨ä» {zip_url} ä¸‹è½½æ–°ç‰ˆæœ¬...")
        response = requests.get(zip_url, timeout=60)
        response.raise_for_status()

        # éœ€è¦å¯¼å…¥ zipfile å’Œ io
        import zipfile
        import io
        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            z.extractall(update_dir)
        
        logger.info(f"æ–°ç‰ˆæœ¬å·²æˆåŠŸä¸‹è½½å¹¶è§£å‹åˆ° '{update_dir}' æ–‡ä»¶å¤¹ã€‚")
        return True
    except requests.RequestException as e:
        logger.error(f"ä¸‹è½½æ›´æ–°å¤±è´¥: {e}")
    except zipfile.BadZipFile:
        logger.error("ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„zipå‹ç¼©åŒ…ã€‚")
    except Exception as e:
        logger.error(f"è§£å‹æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
    return False

def check_for_updates():
    """ä» GitHub æ£€æŸ¥æ–°ç‰ˆæœ¬ã€‚"""
    if not CONFIG.get("enable_auto_update", True):
        logger.info("è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æŸ¥ã€‚")
        return

    current_version = CONFIG.get("version", "0.0.0")
    logger.info(f"å½“å‰ç‰ˆæœ¬: {current_version}ã€‚æ­£åœ¨ä» GitHub æ£€æŸ¥æ›´æ–°...")

    try:
        config_url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/mogai-version/config.jsonc"
        response = requests.get(config_url, timeout=10)
        response.raise_for_status()

        jsonc_content = response.text
        remote_config = _parse_jsonc(jsonc_content)
        
        remote_version_str = remote_config.get("version")
        if not remote_version_str:
            logger.warning("è¿œç¨‹é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç‰ˆæœ¬å·ï¼Œè·³è¿‡æ›´æ–°æ£€æŸ¥ã€‚")
            return

        if parse_version(remote_version_str) > parse_version(current_version):
            logger.info("="*60)
            logger.info(f"ğŸ‰ å‘ç°æ–°ç‰ˆæœ¬! ğŸ‰")
            logger.info(f"  - å½“å‰ç‰ˆæœ¬: {current_version}")
            logger.info(f"  - æœ€æ–°ç‰ˆæœ¬: {remote_version_str}")
            if download_and_extract_update(remote_version_str):
                logger.info("å‡†å¤‡åº”ç”¨æ›´æ–°ã€‚æœåŠ¡å™¨å°†åœ¨5ç§’åå…³é—­å¹¶å¯åŠ¨æ›´æ–°è„šæœ¬ã€‚")
                time.sleep(5)
                update_script_path = os.path.join("modules", "update_script.py")
                # ä½¿ç”¨ Popen å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹
                subprocess.Popen([sys.executable, update_script_path])
                # ä¼˜é›…åœ°é€€å‡ºå½“å‰æœåŠ¡å™¨è¿›ç¨‹
                os._exit(0)
            else:
                logger.error(f"è‡ªåŠ¨æ›´æ–°å¤±è´¥ã€‚è¯·è®¿é—® https://github.com/{GITHUB_REPO}/releases/latest æ‰‹åŠ¨ä¸‹è½½ã€‚")
            logger.info("="*60)
        else:
            logger.info("æ‚¨çš„ç¨‹åºå·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

    except requests.RequestException as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°å¤±è´¥: {e}")
    except json.JSONDecodeError:
        logger.error("è§£æè¿œç¨‹é…ç½®æ–‡ä»¶å¤±è´¥ã€‚")
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# --- æ¨¡å‹æ›´æ–° ---
def extract_models_from_html(html_content):
    """
    ä» HTML å†…å®¹ä¸­æå–å®Œæ•´çš„æ¨¡å‹JSONå¯¹è±¡ï¼Œä½¿ç”¨æ‹¬å·åŒ¹é…ç¡®ä¿å®Œæ•´æ€§ã€‚
    """
    models = []
    model_names = set()
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹JSONå¯¹è±¡çš„èµ·å§‹ä½ç½®
    for start_match in re.finditer(r'\{\\"id\\":\\"[a-f0-9-]+\\"', html_content):
        start_index = start_match.start()
        
        # ä»èµ·å§‹ä½ç½®å¼€å§‹ï¼Œè¿›è¡ŒèŠ±æ‹¬å·åŒ¹é…
        open_braces = 0
        end_index = -1
        
        # ä¼˜åŒ–ï¼šè®¾ç½®ä¸€ä¸ªåˆç†çš„æœç´¢ä¸Šé™ï¼Œé¿å…æ— é™å¾ªç¯
        search_limit = start_index + 10000 # å‡è®¾ä¸€ä¸ªæ¨¡å‹å®šä¹‰ä¸ä¼šè¶…è¿‡10000ä¸ªå­—ç¬¦
        
        for i in range(start_index, min(len(html_content), search_limit)):
            if html_content[i] == '{':
                open_braces += 1
            elif html_content[i] == '}':
                open_braces -= 1
                if open_braces == 0:
                    end_index = i + 1
                    break
        
        if end_index != -1:
            # æå–å®Œæ•´çš„ã€è½¬ä¹‰çš„JSONå­—ç¬¦ä¸²
            json_string_escaped = html_content[start_index:end_index]
            
            # åè½¬ä¹‰
            json_string = json_string_escaped.replace('\\"', '"').replace('\\\\', '\\')
            
            try:
                model_data = json.loads(json_string)
                model_name = model_data.get('publicName')
                
                # ä½¿ç”¨publicNameå»é‡
                if model_name and model_name not in model_names:
                    models.append(model_data)
                    model_names.add(model_name)
            except json.JSONDecodeError as e:
                logger.warning(f"è§£ææå–çš„JSONå¯¹è±¡æ—¶å‡ºé”™: {e} - å†…å®¹: {json_string[:150]}...")
                continue

    if models:
        logger.info(f"æˆåŠŸæå–å¹¶è§£æäº† {len(models)} ä¸ªç‹¬ç«‹æ¨¡å‹ã€‚")
        return models
    else:
        logger.error("é”™è¯¯ï¼šåœ¨HTMLå“åº”ä¸­æ‰¾ä¸åˆ°ä»»ä½•åŒ¹é…çš„å®Œæ•´æ¨¡å‹JSONå¯¹è±¡ã€‚")
        return None

def save_available_models(new_models_list, models_path="available_models.json"):
    """
    å°†æå–åˆ°çš„å®Œæ•´æ¨¡å‹å¯¹è±¡åˆ—è¡¨ä¿å­˜åˆ°æŒ‡å®šçš„JSONæ–‡ä»¶ä¸­ã€‚
    """
    logger.info(f"æ£€æµ‹åˆ° {len(new_models_list)} ä¸ªæ¨¡å‹ï¼Œæ­£åœ¨æ›´æ–° '{models_path}'...")
    
    try:
        with open(models_path, 'w', encoding='utf-8') as f:
            # ç›´æ¥å°†å®Œæ•´çš„æ¨¡å‹å¯¹è±¡åˆ—è¡¨å†™å…¥æ–‡ä»¶
            json.dump(new_models_list, f, indent=4, ensure_ascii=False)
        logger.info(f"âœ… '{models_path}' å·²æˆåŠŸæ›´æ–°ï¼ŒåŒ…å« {len(new_models_list)} ä¸ªæ¨¡å‹ã€‚")
    except IOError as e:
        logger.error(f"âŒ å†™å…¥ '{models_path}' æ–‡ä»¶æ—¶å‡ºé”™: {e}")

# --- è‡ªåŠ¨é‡å¯é€»è¾‘ ---
def restart_server():
    """ä¼˜é›…åœ°é€šçŸ¥å®¢æˆ·ç«¯åˆ·æ–°ï¼Œç„¶åé‡å¯æœåŠ¡å™¨ã€‚"""
    logger.warning("="*60)
    logger.warning("æ£€æµ‹åˆ°æœåŠ¡å™¨ç©ºé—²è¶…æ—¶ï¼Œå‡†å¤‡è‡ªåŠ¨é‡å¯...")
    logger.warning("="*60)
    
    # 1. (å¼‚æ­¥) é€šçŸ¥æµè§ˆå™¨åˆ·æ–°
    async def notify_browser_refresh():
        if browser_ws:
            try:
                # ä¼˜å…ˆå‘é€ 'reconnect' æŒ‡ä»¤ï¼Œè®©å‰ç«¯çŸ¥é“è¿™æ˜¯ä¸€ä¸ªè®¡åˆ’å†…çš„é‡å¯
                await browser_ws.send_text(json.dumps({"command": "reconnect"}, ensure_ascii=False))
                logger.info("å·²å‘æµè§ˆå™¨å‘é€ 'reconnect' æŒ‡ä»¤ã€‚")
            except Exception as e:
                logger.error(f"å‘é€ 'reconnect' æŒ‡ä»¤å¤±è´¥: {e}")
    
    # åœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥é€šçŸ¥å‡½æ•°
    # ä½¿ç”¨`asyncio.run_coroutine_threadsafe`ç¡®ä¿çº¿ç¨‹å®‰å…¨
    if browser_ws and browser_ws.client_state.name == 'CONNECTED' and main_event_loop:
        asyncio.run_coroutine_threadsafe(notify_browser_refresh(), main_event_loop)
    
    # 2. å»¶è¿Ÿå‡ ç§’ä»¥ç¡®ä¿æ¶ˆæ¯å‘é€
    time.sleep(3)
    
    # 3. æ‰§è¡Œé‡å¯
    logger.info("æ­£åœ¨é‡å¯æœåŠ¡å™¨...")
    os.execv(sys.executable, ['python'] + sys.argv)

def idle_monitor():
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œç›‘æ§æœåŠ¡å™¨æ˜¯å¦ç©ºé—²ã€‚"""
    global last_activity_time
    
    # ç­‰å¾…ï¼Œç›´åˆ° last_activity_time è¢«é¦–æ¬¡è®¾ç½®
    while last_activity_time is None:
        time.sleep(1)
        
    logger.info("ç©ºé—²ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨ã€‚")
    
    while True:
        if CONFIG.get("enable_idle_restart", False):
            timeout = CONFIG.get("idle_restart_timeout_seconds", 300)
            
            # å¦‚æœè¶…æ—¶è®¾ç½®ä¸º-1ï¼Œåˆ™ç¦ç”¨é‡å¯æ£€æŸ¥
            if timeout == -1:
                time.sleep(10) # ä»ç„¶éœ€è¦ä¼‘çœ ä»¥é¿å…ç¹å¿™å¾ªç¯
                continue

            idle_time = (datetime.now() - last_activity_time).total_seconds()
            
            if idle_time > timeout:
                logger.info(f"æœåŠ¡å™¨ç©ºé—²æ—¶é—´ ({idle_time:.0f}s) å·²è¶…è¿‡é˜ˆå€¼ ({timeout}s)ã€‚")
                restart_server()
                break # é€€å‡ºå¾ªç¯ï¼Œå› ä¸ºè¿›ç¨‹å³å°†è¢«æ›¿æ¢
                
        # æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡
        time.sleep(10)

# --- FastAPI ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶è¿è¡Œçš„ç”Ÿå‘½å‘¨æœŸå‡½æ•°ã€‚"""
    global idle_monitor_thread, last_activity_time, main_event_loop, aiohttp_session, DOWNLOAD_SEMAPHORE, MAX_CONCURRENT_DOWNLOADS
    main_event_loop = asyncio.get_running_loop() # è·å–ä¸»äº‹ä»¶å¾ªç¯
    load_config() # é¦–å…ˆåŠ è½½é…ç½®
    
    # ä»é…ç½®ä¸­è¯»å–å¹¶å‘å’Œè¿æ¥æ± è®¾ç½®
    MAX_CONCURRENT_DOWNLOADS = CONFIG.get("max_concurrent_downloads", 50)
    pool_config = CONFIG.get("connection_pool", {})
    
    # åˆ›å»ºä¼˜åŒ–çš„å…¨å±€aiohttpä¼šè¯
    connector = aiohttp.TCPConnector(
        ssl=False,
        limit=pool_config.get("total_limit", 200),                  # å¢åŠ æ€»è¿æ¥æ•°
        limit_per_host=pool_config.get("per_host_limit", 50),      # æ¯ä¸ªä¸»æœºçš„è¿æ¥é™åˆ¶
        ttl_dns_cache=pool_config.get("dns_cache_ttl", 300),       # DNSç¼“å­˜æ—¶é—´
        force_close=False,                                          # ä¿æŒè¿æ¥
        enable_cleanup_closed=True,                                 # è‡ªåŠ¨æ¸…ç†å…³é—­çš„è¿æ¥
        keepalive_timeout=pool_config.get("keepalive_timeout", 30)  # ä¿æ´»è¶…æ—¶
    )
    
    # åˆ›å»ºä¼˜åŒ–çš„è¶…æ—¶é…ç½®
    timeout_config = CONFIG.get("download_timeout", {})
    timeout = aiohttp.ClientTimeout(
        total=timeout_config.get("total", 30),      # æ€»è¶…æ—¶æ—¶é—´
        connect=timeout_config.get("connect", 5),   # è¿æ¥è¶…æ—¶
        sock_read=timeout_config.get("sock_read", 10)  # è¯»å–è¶…æ—¶
    )
    
    aiohttp_session = aiohttp.ClientSession(
        connector=connector,
        timeout=timeout,
        trust_env=True
    )
    
    # åˆå§‹åŒ–ä¸‹è½½ä¿¡å·é‡
    DOWNLOAD_SEMAPHORE = Semaphore(MAX_CONCURRENT_DOWNLOADS)
    
    logger.info(f"å…¨å±€aiohttpä¼šè¯å·²åˆ›å»ºï¼ˆä¼˜åŒ–é…ç½®ï¼‰")
    logger.info(f"  - æœ€å¤§è¿æ¥æ•°: {pool_config.get('total_limit', 200)}")
    logger.info(f"  - æ¯ä¸»æœºè¿æ¥æ•°: {pool_config.get('per_host_limit', 50)}")
    logger.info(f"  - æœ€å¤§å¹¶å‘ä¸‹è½½: {MAX_CONCURRENT_DOWNLOADS}")
    
    # å›¾åƒè‡ªåŠ¨å¢å¼ºåŠŸèƒ½å·²ç§»é™¤ï¼ˆå·²å‰¥ç¦»ä¸ºç‹¬ç«‹é¡¹ç›®image_enhancerï¼‰
    
    # --- æ‰“å°å½“å‰çš„æ“ä½œæ¨¡å¼ ---
    mode = CONFIG.get("id_updater_last_mode", "direct_chat")
    target = CONFIG.get("id_updater_battle_target", "A")
    logger.info("="*60)
    logger.info(f"  å½“å‰æ“ä½œæ¨¡å¼: {mode.upper()}")
    if mode == 'battle':
        logger.info(f"  - Battle æ¨¡å¼ç›®æ ‡: Assistant {target}")
    logger.info("  (å¯é€šè¿‡è¿è¡Œ id_updater.py ä¿®æ”¹æ¨¡å¼)")
    logger.info("="*60)
    
    # æ·»åŠ ç›‘æ§é¢æ¿ä¿¡æ¯
    logger.info(f"ğŸ“Š ç›‘æ§é¢æ¿: http://127.0.0.1:5102/monitor")
    logger.info("="*60)

    check_for_updates() # æ£€æŸ¥ç¨‹åºæ›´æ–°
    load_model_map() # é‡æ–°å¯ç”¨æ¨¡å‹åŠ è½½
    load_model_endpoint_map() # åŠ è½½æ¨¡å‹ç«¯ç‚¹æ˜ å°„
    logger.info("æœåŠ¡å™¨å¯åŠ¨å®Œæˆã€‚ç­‰å¾…æ²¹çŒ´è„šæœ¬è¿æ¥...")

    # æ£€æŸ¥å¹¶æ˜¾ç¤ºå…¬å‘Šï¼Œæ”¾åœ¨å¯åŠ¨ä¿¡æ¯çš„æœ€åï¼Œä½¿å…¶æ›´æ˜¾çœ¼
    check_and_display_announcement()

    # åœ¨æ¨¡å‹æ›´æ–°åï¼Œæ ‡è®°æ´»åŠ¨æ—¶é—´çš„èµ·ç‚¹
    last_activity_time = datetime.now()
    
    # å¯åŠ¨ç©ºé—²ç›‘æ§çº¿ç¨‹
    if CONFIG.get("enable_idle_restart", False):
        idle_monitor_thread = threading.Thread(target=idle_monitor, daemon=True)
        idle_monitor_thread.start()
        

    # å¯åŠ¨å†…å­˜ç›‘æ§ä»»åŠ¡
    asyncio.create_task(memory_monitor())
    
    # å¯åŠ¨æ–°çš„æ—¥å¿—ç³»ç»Ÿ
    log_system.start()
    logger.info("âœ… æ–°çš„å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿå·²å¯åŠ¨")
    
    yield
    
    # æ¸…ç†èµ„æº
    if aiohttp_session:
        await aiohttp_session.close()
        logger.info("å…¨å±€aiohttpä¼šè¯å·²å…³é—­")
    
    # åœæ­¢æ—¥å¿—ç³»ç»Ÿå¹¶åˆ·æ–°æ‰€æœ‰ç¼“å†²
    await log_system.stop()
    logger.info("âœ… æ—¥å¿—ç³»ç»Ÿå·²åœæ­¢å¹¶åˆ·æ–°æ‰€æœ‰ç¼“å†²")
    
    logger.info("æœåŠ¡å™¨æ­£åœ¨å…³é—­ã€‚")

app = FastAPI(lifespan=lifespan)

# --- CORS ä¸­é—´ä»¶é…ç½® ---
# å…è®¸æ‰€æœ‰æ¥æºã€æ‰€æœ‰æ–¹æ³•ã€æ‰€æœ‰è¯·æ±‚å¤´ï¼Œè¿™å¯¹äºæœ¬åœ°å¼€å‘å·¥å…·æ˜¯å®‰å…¨çš„ã€‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- è¾…åŠ©å‡½æ•° ---
def save_config():
    """å°†å½“å‰çš„ CONFIG å¯¹è±¡å†™å› config.jsonc æ–‡ä»¶ï¼Œä¿ç•™æ³¨é‡Šã€‚"""
    try:
        # è¯»å–åŸå§‹æ–‡ä»¶ä»¥ä¿ç•™æ³¨é‡Šç­‰
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å®‰å…¨åœ°æ›¿æ¢å€¼
        def replacer(key, value, content):
            # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼ä¼šæ‰¾åˆ° keyï¼Œç„¶ååŒ¹é…å®ƒçš„ value éƒ¨åˆ†ï¼Œç›´åˆ°é€—å·æˆ–å³èŠ±æ‹¬å·
            pattern = re.compile(rf'("{key}"\s*:\s*").*?("?)(,?\s*)$', re.MULTILINE)
            replacement = rf'\g<1>{value}\g<2>\g<3>'
            if not pattern.search(content): # å¦‚æœ key ä¸å­˜åœ¨ï¼Œå°±æ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                 content = re.sub(r'}\s*$', f'  ,"{key}": "{value}"\n}}', content)
            else:
                 content = pattern.sub(replacement, content)
            return content

        content_str = "".join(lines)
        content_str = replacer("session_id", CONFIG["session_id"], content_str)
        content_str = replacer("message_id", CONFIG["message_id"], content_str)
        
        with open('config.jsonc', 'w', encoding='utf-8') as f:
            f.write(content_str)
        logger.info("âœ… æˆåŠŸå°†ä¼šè¯ä¿¡æ¯æ›´æ–°åˆ° config.jsoncã€‚")
    except Exception as e:
        logger.error(f"âŒ å†™å…¥ config.jsonc æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def _process_openai_message(message: dict) -> dict:
    """
    å¤„ç†OpenAIæ¶ˆæ¯ï¼Œåˆ†ç¦»æ–‡æœ¬å’Œé™„ä»¶ã€‚
    - å°†å¤šæ¨¡æ€å†…å®¹åˆ—è¡¨åˆ†è§£ä¸ºçº¯æ–‡æœ¬å’Œé™„ä»¶åˆ—è¡¨ã€‚
    - æ–‡ä»¶åºŠé€»è¾‘å·²ç§»è‡³ chat_completions é¢„å¤„ç†ï¼Œæ­¤å¤„ä»…å¤„ç†å¸¸è§„é™„ä»¶æ„å»ºã€‚
    - ç¡®ä¿ user è§’è‰²çš„ç©ºå†…å®¹è¢«æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä»¥é¿å… LMArena å‡ºé”™ã€‚
    - ç‰¹æ®Šå¤„ç†assistantè§’è‰²çš„å›¾ç‰‡ï¼šæ£€æµ‹Markdownå›¾ç‰‡å¹¶è½¬æ¢ä¸ºexperimental_attachments
    """
    content = message.get("content")
    role = message.get("role")
    attachments = []
    experimental_attachments = []
    text_content = ""

    # æ·»åŠ è¯Šæ–­æ—¥å¿—
    logger.debug(f"[MSG_PROCESS] å¤„ç†æ¶ˆæ¯ - è§’è‰²: {role}, å†…å®¹ç±»å‹: {type(content).__name__}")
    
    # ç‰¹æ®Šå¤„ç†assistantè§’è‰²çš„å­—ç¬¦ä¸²å†…å®¹ä¸­çš„Markdownå›¾ç‰‡
    if role == "assistant" and isinstance(content, str):
        import re
        # åŒ¹é… ![...](url) æ ¼å¼çš„Markdownå›¾ç‰‡
        markdown_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        matches = re.findall(markdown_pattern, content)
        
        if matches:
            logger.info(f"[MSG_PROCESS] åœ¨assistantæ¶ˆæ¯ä¸­æ£€æµ‹åˆ° {len(matches)} ä¸ªMarkdownå›¾ç‰‡")
            
            # ç§»é™¤Markdownå›¾ç‰‡ï¼Œåªä¿ç•™æ–‡æœ¬
            text_content = re.sub(markdown_pattern, '', content).strip()
            
            # å°†å›¾ç‰‡è½¬æ¢ä¸ºexperimental_attachmentsæ ¼å¼
            for alt_text, url in matches:
                # ç¡®å®šå†…å®¹ç±»å‹
                if url.startswith("data:"):
                    # base64æ ¼å¼
                    content_type = url.split(';')[0].split(':')[1] if ':' in url else 'image/png'
                elif url.startswith("http"):
                    # HTTP URL
                    content_type = mimetypes.guess_type(url)[0] or 'image/jpeg'
                else:
                    content_type = 'image/jpeg'
                
                # ç”Ÿæˆæ–‡ä»¶å
                if '/' in url and not url.startswith("data:"):
                    # ä»URLæå–æ–‡ä»¶å
                    filename = url.split('/')[-1].split('?')[0]
                    if '.' not in filename:
                        filename = f"image_{uuid.uuid4()}.{content_type.split('/')[-1]}"
                else:
                    filename = f"image_{uuid.uuid4()}.{content_type.split('/')[-1]}"
                
                experimental_attachment = {
                    "name": filename,
                    "contentType": content_type,
                    "url": url
                }
                experimental_attachments.append(experimental_attachment)
                logger.debug(f"[MSG_PROCESS] æ·»åŠ experimental_attachment: {filename}")
        else:
            text_content = content
    elif isinstance(content, list):
        text_parts = []
        for part in content:
            if part.get("type") == "text":
                text_parts.append(part.get("text", ""))
            elif part.get("type") == "image_url":
                # æ­¤å¤„çš„ URL å¯èƒ½æ˜¯ base64 æˆ– http URL (å·²è¢«é¢„å¤„ç†å™¨æ›¿æ¢)
                image_url_data = part.get("image_url", {})
                url = image_url_data.get("url")
                original_filename = image_url_data.get("detail")

                try:
                    # å¯¹äº base64ï¼Œæˆ‘ä»¬éœ€è¦æå– content_type
                    if url.startswith("data:"):
                        content_type = url.split(';')[0].split(':')[1]
                    else:
                        # å¯¹äº http URLï¼Œæˆ‘ä»¬å°è¯•çŒœæµ‹ content_type
                        content_type = mimetypes.guess_type(url)[0] or 'application/octet-stream'

                    file_name = original_filename or f"image_{uuid.uuid4()}.{mimetypes.guess_extension(content_type).lstrip('.') or 'png'}"
                    
                    attachment = {
                        "name": file_name,
                        "contentType": content_type,
                        "url": url
                    }
                    
                    # Assistantè§’è‰²ä½¿ç”¨experimental_attachments
                    if role == "assistant":
                        experimental_attachments.append(attachment)
                        logger.debug(f"[MSG_PROCESS] Assistantå›¾ç‰‡æ·»åŠ åˆ°experimental_attachments")
                    else:
                        attachments.append(attachment)
                        logger.debug(f"[MSG_PROCESS] {role}å›¾ç‰‡æ·»åŠ åˆ°attachments")

                except (AttributeError, IndexError, ValueError) as e:
                    logger.warning(f"å¤„ç†é™„ä»¶URLæ—¶å‡ºé”™: {url[:100]}... é”™è¯¯: {e}")

        text_content = "\n\n".join(text_parts)
    elif isinstance(content, str):
        text_content = content

    if role == "user" and not text_content.strip():
        text_content = " "

    # æ„å»ºè¿”å›ç»“æœ
    result = {
        "role": role,
        "content": text_content,
        "attachments": attachments
    }
    
    # Assistantè§’è‰²æ·»åŠ experimental_attachments
    if role == "assistant" and experimental_attachments:
        result["experimental_attachments"] = experimental_attachments
        logger.info(f"[MSG_PROCESS] Assistantæ¶ˆæ¯åŒ…å« {len(experimental_attachments)} ä¸ªexperimental_attachments")
    
    return result

async def convert_openai_to_lmarena_payload(openai_data: dict, session_id: str, message_id: str, mode_override: str = None, battle_target_override: str = None) -> dict:
    """
    å°† OpenAI è¯·æ±‚ä½“è½¬æ¢ä¸ºæ²¹çŒ´è„šæœ¬æ‰€éœ€çš„ç®€åŒ–è½½è·ï¼Œå¹¶åº”ç”¨é…’é¦†æ¨¡å¼ã€ç»•è¿‡æ¨¡å¼ä»¥åŠå¯¹æˆ˜æ¨¡å¼ã€‚
    æ–°å¢äº†æ¨¡å¼è¦†ç›–å‚æ•°ï¼Œä»¥æ”¯æŒæ¨¡å‹ç‰¹å®šçš„ä¼šè¯æ¨¡å¼ã€‚
    """
    # 0. é¢„å¤„ç†ï¼šä»å†å²æ¶ˆæ¯ä¸­å‰¥ç¦»æ€ç»´é“¾ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
    messages = openai_data.get("messages", [])
    if CONFIG.get("strip_reasoning_from_history", True) and CONFIG.get("enable_lmarena_reasoning", False):
        reasoning_mode = CONFIG.get("reasoning_output_mode", "openai")
        
        # ä»…å¯¹think_tagæ¨¡å¼æœ‰æ•ˆï¼ˆOpenAIæ¨¡å¼çš„reasoning_contentä¸åœ¨contentä¸­ï¼‰
        if reasoning_mode == "think_tag":
            import re
            think_pattern = re.compile(r'<think>.*?</think>\s*', re.DOTALL)
            
            for msg in messages:
                if msg.get("role") == "assistant" and isinstance(msg.get("content"), str):
                    original_content = msg["content"]
                    # ç§»é™¤<think>æ ‡ç­¾åŠå…¶å†…å®¹
                    cleaned_content = think_pattern.sub('', original_content).strip()
                    if cleaned_content != original_content:
                        msg["content"] = cleaned_content
                        logger.debug(f"[REASONING_STRIP] ä»å†å²æ¶ˆæ¯ä¸­å‰¥ç¦»äº†æ€ç»´é“¾å†…å®¹")
    
    # 1. è§„èŒƒåŒ–è§’è‰²å¹¶å¤„ç†æ¶ˆæ¯
    #    - å°†éæ ‡å‡†çš„ 'developer' è§’è‰²è½¬æ¢ä¸º 'system' ä»¥æé«˜å…¼å®¹æ€§ã€‚
    #    - åˆ†ç¦»æ–‡æœ¬å’Œé™„ä»¶ã€‚
    for msg in messages:
        if msg.get("role") == "developer":
            msg["role"] = "system"
            logger.info("æ¶ˆæ¯è§’è‰²è§„èŒƒåŒ–ï¼šå°† 'developer' è½¬æ¢ä¸º 'system'ã€‚")
    
    processed_messages = []
    for msg in messages:
        processed_msg = await _process_openai_message(msg.copy())
        processed_messages.append(processed_msg)

    # 2. åº”ç”¨é…’é¦†æ¨¡å¼ (Tavern Mode)
    if CONFIG.get("tavern_mode_enabled"):
        system_prompts = [msg['content'] for msg in processed_messages if msg['role'] == 'system']
        other_messages = [msg for msg in processed_messages if msg['role'] != 'system']
        
        merged_system_prompt = "\n\n".join(system_prompts)
        final_messages = []
        
        if merged_system_prompt:
            # ç³»ç»Ÿæ¶ˆæ¯ä¸åº”æœ‰é™„ä»¶
            final_messages.append({"role": "system", "content": merged_system_prompt, "attachments": []})
        
        final_messages.extend(other_messages)
        processed_messages = final_messages

    # 3. ç¡®å®šç›®æ ‡æ¨¡å‹ ID å’Œç±»å‹
    model_name = openai_data.get("model", "claude-3-5-sonnet-20241022")
    
    # ä¼˜å…ˆä» MODEL_ENDPOINT_MAP è·å–æ¨¡å‹ç±»å‹ï¼ˆå¦‚æœå®šä¹‰äº†ï¼‰
    model_type = "text"  # é»˜è®¤ç±»å‹
    endpoint_info = MODEL_ENDPOINT_MAP.get(model_name, {})
    
    # è¯Šæ–­æ—¥å¿—ï¼šè®°å½•æ¨¡å‹ç±»å‹åˆ¤æ–­è¿‡ç¨‹
    logger.info(f"[BYPASS_DEBUG] å¼€å§‹åˆ¤æ–­æ¨¡å‹ '{model_name}' çš„ç±»å‹...")
    logger.info(f"[BYPASS_DEBUG] endpoint_info ç±»å‹: {type(endpoint_info).__name__}, å†…å®¹: {endpoint_info}")
    
    if isinstance(endpoint_info, dict) and "type" in endpoint_info:
        model_type = endpoint_info.get("type", "text")
        logger.info(f"[BYPASS_DEBUG] ä» model_endpoint_map.json (dict) è·å–æ¨¡å‹ç±»å‹: {model_type}")
    elif isinstance(endpoint_info, list) and endpoint_info:
        # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç±»å‹
        first_endpoint = endpoint_info[0] if isinstance(endpoint_info[0], dict) else {}
        if "type" in first_endpoint:
            model_type = first_endpoint.get("type", "text")
            logger.info(f"[BYPASS_DEBUG] ä» model_endpoint_map.json (list) è·å–æ¨¡å‹ç±»å‹: {model_type}")
    
    # å›é€€åˆ° models.json ä¸­çš„å®šä¹‰
    model_info = MODEL_NAME_TO_ID_MAP.get(model_name, {}) # å…³é”®ä¿®å¤ï¼šç¡®ä¿ model_info æ€»æ˜¯ä¸€ä¸ªå­—å…¸
    if not endpoint_info.get("type") and model_info:
        old_type = model_type
        model_type = model_info.get("type", "text")
        logger.info(f"[BYPASS_DEBUG] ä» models.json è·å–æ¨¡å‹ç±»å‹: {old_type} -> {model_type}")
    
    logger.info(f"[BYPASS_DEBUG] æœ€ç»ˆç¡®å®šçš„æ¨¡å‹ç±»å‹: {model_type}")
    
    target_model_id = None
    if model_info:
        target_model_id = model_info.get("id")
    else:
        logger.warning(f"æ¨¡å‹ '{model_name}' åœ¨ 'models.json' ä¸­æœªæ‰¾åˆ°ã€‚è¯·æ±‚å°†ä¸å¸¦ç‰¹å®šæ¨¡å‹IDå‘é€ã€‚")

    if not target_model_id:
        logger.warning(f"æ¨¡å‹ '{model_name}' åœ¨ 'models.json' ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„IDã€‚è¯·æ±‚å°†ä¸å¸¦ç‰¹å®šæ¨¡å‹IDå‘é€ã€‚")

    # 4. æ„å»ºæ¶ˆæ¯æ¨¡æ¿
    message_templates = []
    for msg in processed_messages:
        msg_template = {
            "role": msg["role"],
            "content": msg.get("content", ""),
            "attachments": msg.get("attachments", [])
        }
        
        # å¯¹äºuserè§’è‰²ï¼Œé™„ä»¶éœ€è¦æ”¾åœ¨experimental_attachmentsä¸­
        if msg["role"] == "user" and msg.get("attachments"):
            msg_template["experimental_attachments"] = msg.get("attachments", [])
            logger.info(f"[LMARENA_CONVERT] å°†userçš„ {len(msg['attachments'])} ä¸ªé™„ä»¶æ·»åŠ åˆ°experimental_attachments")
        
        # ä¿ç•™assistantçš„experimental_attachmentså­—æ®µï¼ˆå›¾ç‰‡ç”Ÿæˆæ¨¡å‹éœ€è¦ï¼‰
        if msg["role"] == "assistant" and "experimental_attachments" in msg:
            msg_template["experimental_attachments"] = msg["experimental_attachments"]
            logger.info(f"[LMARENA_CONVERT] ä¿ç•™assistantçš„ {len(msg['experimental_attachments'])} ä¸ªexperimental_attachments")
        
        message_templates.append(msg_template)

    # 4.5 åº”ç”¨å›¾ç‰‡é™„ä»¶å®¡æŸ¥ç»•è¿‡ (Image Attachment Bypass) - ä¸“ç”¨äºimageæ¨¡å‹
    # å½“ä½¿ç”¨imageæ¨¡å‹ä¸”æœ€æ–°çš„ç”¨æˆ·è¯·æ±‚åŒ…å«å›¾ç‰‡é™„ä»¶æ—¶ï¼Œå°†æ–‡æœ¬å†…å®¹åˆ†ç¦»åˆ°æ–°è¯·æ±‚ä¸­
    # æ³¨ï¼štextæ¨¡å‹æœ‰è‡ªå·±çš„ç»•è¿‡æœºåˆ¶ï¼Œsearchæ¨¡å‹ä¸éœ€è¦ï¼ˆç©ºå†…å®¹ä¼šæŠ¥é”™ï¼‰
    if CONFIG.get("image_attachment_bypass_enabled", False) and model_type == "image":
        # æŸ¥æ‰¾æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        last_user_msg_idx = None
        for i in range(len(message_templates) - 1, -1, -1):
            if message_templates[i]["role"] == "user":
                last_user_msg_idx = i
                break
        
        if last_user_msg_idx is not None:
            last_user_msg = message_templates[last_user_msg_idx]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡é™„ä»¶
            has_image_attachment = False
            if last_user_msg.get("attachments"):
                for attachment in last_user_msg["attachments"]:
                    if attachment.get("contentType", "").startswith("image/"):
                        has_image_attachment = True
                        break
            
            # å¦‚æœåŒ…å«å›¾ç‰‡é™„ä»¶ä¸”æœ‰æ–‡æœ¬å†…å®¹ï¼Œæ‰§è¡Œåˆ†ç¦»
            if has_image_attachment and last_user_msg.get("content", "").strip():
                original_content = last_user_msg["content"]
                original_attachments = last_user_msg["attachments"]
                
                # åˆ›å»ºä¸¤æ¡æ¶ˆæ¯ï¼š
                # ç¬¬ä¸€æ¡ï¼šåªåŒ…å«å›¾ç‰‡é™„ä»¶ï¼ˆæˆä¸ºå†å²è®°å½•ï¼‰
                image_only_msg = {
                    "role": "user",
                    "content": " ",  # ç©ºå†…å®¹æˆ–ç©ºæ ¼
                    "experimental_attachments": original_attachments,
                    "attachments": original_attachments
                }
                
                # ç¬¬äºŒæ¡ï¼šåªåŒ…å«æ–‡æœ¬å†…å®¹ï¼ˆä½œä¸ºæœ€æ–°è¯·æ±‚ï¼‰
                text_only_msg = {
                    "role": "user",
                    "content": original_content,
                    "attachments": []
                }
                
                # æ›¿æ¢åŸæ¶ˆæ¯ä¸ºä¸¤æ¡åˆ†ç¦»çš„æ¶ˆæ¯
                message_templates[last_user_msg_idx] = image_only_msg
                message_templates.insert(last_user_msg_idx + 1, text_only_msg)
                
                logger.info(f"å›¾ç‰‡æ¨¡å‹å®¡æŸ¥ç»•è¿‡å·²å¯ç”¨ï¼šå°†åŒ…å« {len(original_attachments)} ä¸ªé™„ä»¶çš„è¯·æ±‚åˆ†ç¦»ä¸ºä¸¤æ¡æ¶ˆæ¯")

    # 5. åº”ç”¨ç»•è¿‡æ¨¡å¼ (Bypass Mode) - æ ¹æ®æ¨¡å‹ç±»å‹å’Œé…ç½®å†³å®šæ˜¯å¦å¯ç”¨
    # è·å–ç»†ç²’åº¦çš„ç»•è¿‡è®¾ç½®
    bypass_settings = CONFIG.get("bypass_settings", {})
    global_bypass_enabled = CONFIG.get("bypass_enabled", False)
    
    # è¯Šæ–­æ—¥å¿—ï¼šè¯¦ç»†è®°å½•ç»•è¿‡å†³ç­–è¿‡ç¨‹
    logger.info(f"[BYPASS_DEBUG] ===== ç»•è¿‡å†³ç­–å¼€å§‹ =====")
    logger.info(f"[BYPASS_DEBUG] å…¨å±€ bypass_enabled: {global_bypass_enabled}")
    logger.info(f"[BYPASS_DEBUG] bypass_settings: {bypass_settings}")
    logger.info(f"[BYPASS_DEBUG] å½“å‰æ¨¡å‹ç±»å‹: {model_type}")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®šæ˜¯å¦å¯ç”¨ç»•è¿‡
    bypass_enabled_for_type = False
    
    # ä¿®å¤ï¼šå…¨å±€bypass_enabledä¸ºFalseæ—¶ï¼Œæ— è®ºbypass_settingså¦‚ä½•è®¾ç½®éƒ½åº”è¯¥ç¦ç”¨
    if not global_bypass_enabled:
        bypass_enabled_for_type = False
        logger.info(f"[BYPASS_DEBUG] â›” å…¨å±€ bypass_enabled=Falseï¼Œå¼ºåˆ¶ç¦ç”¨æ‰€æœ‰ç»•è¿‡åŠŸèƒ½")
    elif bypass_settings:
        # å¦‚æœæœ‰ç»†ç²’åº¦é…ç½®ï¼Œæ£€æŸ¥æ˜¯å¦æ˜ç¡®å®šä¹‰äº†è¯¥ç±»å‹
        if model_type in bypass_settings:
            # å¦‚æœæ˜ç¡®å®šä¹‰äº†ï¼Œä½¿ç”¨å®šä¹‰çš„å€¼ï¼ˆä½†ä»å—å…¨å±€å¼€å…³æ§åˆ¶ï¼‰
            bypass_enabled_for_type = bypass_settings.get(model_type, False)
            logger.info(f"[BYPASS_DEBUG] ä½¿ç”¨ bypass_settings ä¸­æ˜ç¡®å®šä¹‰çš„å€¼: bypass_settings['{model_type}'] = {bypass_enabled_for_type}")
        else:
            # å¦‚æœæœªæ˜ç¡®å®šä¹‰ï¼Œé»˜è®¤ä¸ºFalseï¼ˆæ›´å®‰å…¨çš„é»˜è®¤å€¼ï¼‰
            bypass_enabled_for_type = False
            logger.info(f"[BYPASS_DEBUG] model_type '{model_type}' æœªåœ¨ bypass_settings ä¸­å®šä¹‰ï¼Œé»˜è®¤ç¦ç”¨")
    else:
        # å¦‚æœæ²¡æœ‰ç»†ç²’åº¦é…ç½®ï¼Œä½¿ç”¨å…¨å±€è®¾ç½®ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        # ä½†å¯¹äº image å’Œ search ç±»å‹ï¼Œé»˜è®¤ä¸º Falseï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
        if model_type in ["image", "search"]:
            bypass_enabled_for_type = False
            logger.info(f"[BYPASS_DEBUG] æ—  bypass_settingsï¼Œæ¨¡å‹ç±»å‹ '{model_type}' å±äº ['image', 'search']ï¼Œå¼ºåˆ¶è®¾ä¸º False")
        else:
            bypass_enabled_for_type = global_bypass_enabled
            logger.info(f"[BYPASS_DEBUG] æ—  bypass_settingsï¼Œä½¿ç”¨å…¨å±€ bypass_enabled: {bypass_enabled_for_type}")
    
    logger.info(f"[BYPASS_DEBUG] æœ€ç»ˆå†³ç­–ï¼šbypass_enabled_for_type = {bypass_enabled_for_type}")
    
    if bypass_enabled_for_type:
        # ä»é…ç½®ä¸­è¯»å–ç»•è¿‡æ³¨å…¥å†…å®¹
        bypass_injection = CONFIG.get("bypass_injection", {})
        
        # æ”¯æŒé¢„è®¾æ¨¡å¼
        bypass_presets = bypass_injection.get("presets", {})
        active_preset_name = bypass_injection.get("active_preset", "default")
        
        # å°è¯•è·å–æ¿€æ´»çš„é¢„è®¾
        injection_config = bypass_presets.get(active_preset_name)
        
        # å¦‚æœé¢„è®¾ä¸å­˜åœ¨ï¼Œå›é€€åˆ°è‡ªå®šä¹‰é…ç½®æˆ–é»˜è®¤å€¼
        if not injection_config:
            logger.warning(f"[BYPASS_DEBUG] é¢„è®¾ '{active_preset_name}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰é…ç½®")
            injection_config = bypass_injection.get("custom", {
                "role": "user",
                "content": " ",
                "participantPosition": "a"
            })
        
        # è·å–æ³¨å…¥å‚æ•°ï¼ˆå¸¦é»˜è®¤å€¼ï¼‰
        inject_role = injection_config.get("role", "user")
        inject_content = injection_config.get("content", " ")
        inject_position = injection_config.get("participantPosition", "a")
        
        logger.info(f"[BYPASS_DEBUG] âš ï¸ æ¨¡å‹ç±»å‹ '{model_type}' çš„ç»•è¿‡æ¨¡å¼å·²å¯ç”¨")
        logger.info(f"[BYPASS_DEBUG]   - ä½¿ç”¨é¢„è®¾: {active_preset_name}")
        logger.info(f"[BYPASS_DEBUG]   - æ³¨å…¥è§’è‰²: {inject_role}")
        logger.info(f"[BYPASS_DEBUG]   - æ³¨å…¥ä½ç½®: {inject_position}")
        logger.info(f"[BYPASS_DEBUG]   - æ³¨å…¥å†…å®¹: {inject_content[:50]}{'...' if len(inject_content) > 50 else ''}")
        
        message_templates.append({
            "role": inject_role,
            "content": inject_content,
            "participantPosition": inject_position,
            "attachments": []
        })
    else:
        if global_bypass_enabled or any(bypass_settings.values()) if bypass_settings else False:
            # å¦‚æœæœ‰ä»»ä½•ç»•è¿‡è®¾ç½®å¯ç”¨ï¼Œä½†å½“å‰ç±»å‹æœªå¯ç”¨ï¼Œè®°å½•æ—¥å¿—
            logger.info(f"[BYPASS_DEBUG] âœ… æ¨¡å‹ç±»å‹ '{model_type}' çš„ç»•è¿‡æ¨¡å¼å·²ç¦ç”¨ã€‚")
    
    logger.info(f"[BYPASS_DEBUG] ===== ç»•è¿‡å†³ç­–ç»“æŸ =====")

    # 6. åº”ç”¨å‚ä¸è€…ä½ç½® (Participant Position)
    # ä¼˜å…ˆä½¿ç”¨è¦†ç›–çš„æ¨¡å¼ï¼Œå¦åˆ™å›é€€åˆ°å…¨å±€é…ç½®
    mode = mode_override or CONFIG.get("id_updater_last_mode", "direct_chat")
    target_participant = battle_target_override or CONFIG.get("id_updater_battle_target", "A")
    target_participant = target_participant.lower() # ç¡®ä¿æ˜¯å°å†™

    logger.info(f"æ­£åœ¨æ ¹æ®æ¨¡å¼ '{mode}' (ç›®æ ‡: {target_participant if mode == 'battle' else 'N/A'}) è®¾ç½® Participant Positions...")

    for msg in message_templates:
        if msg['role'] == 'system':
            if mode == 'battle':
                # Battle æ¨¡å¼: system ä¸ç”¨æˆ·é€‰æ‹©çš„åŠ©æ‰‹åœ¨åŒä¸€è¾¹ (Aåˆ™a, Båˆ™b)
                msg['participantPosition'] = target_participant
            else:
                # DirectChat æ¨¡å¼: system å›ºå®šä¸º 'b'
                msg['participantPosition'] = 'b'
        elif mode == 'battle':
            # Battle æ¨¡å¼ä¸‹ï¼Œé system æ¶ˆæ¯ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç›®æ ‡ participant
            msg['participantPosition'] = target_participant
        else: # DirectChat æ¨¡å¼
            # DirectChat æ¨¡å¼ä¸‹ï¼Œé system æ¶ˆæ¯ä½¿ç”¨é»˜è®¤çš„ 'a'
            msg['participantPosition'] = 'a'

    return {
        "message_templates": message_templates,
        "target_model_id": target_model_id,
        "session_id": session_id,
        "message_id": message_id
    }

# --- OpenAI æ ¼å¼åŒ–è¾…åŠ©å‡½æ•° (ç¡®ä¿JSONåºåˆ—åŒ–ç¨³å¥) ---
def format_openai_chunk(content: str, model: str, request_id: str) -> str:
    """æ ¼å¼åŒ–ä¸º OpenAI æµå¼å—ã€‚"""
    chunk = {
        "id": request_id, "object": "chat.completion.chunk",
        "created": int(time.time()), "model": model,
        "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
    }
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

def format_openai_finish_chunk(model: str, request_id: str, reason: str = 'stop') -> str:
    """æ ¼å¼åŒ–ä¸º OpenAI ç»“æŸå—ã€‚"""
    chunk = {
        "id": request_id, "object": "chat.completion.chunk",
        "created": int(time.time()), "model": model,
        "choices": [{"index": 0, "delta": {}, "finish_reason": reason}]
    }
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\ndata: [DONE]\n\n"

def format_openai_error_chunk(error_message: str, model: str, request_id: str) -> str:
    """Format as OpenAI error chunk."""
    content = f"\n\n[Luma API Error]: {error_message}"
    return format_openai_chunk(content, model, request_id)

def format_openai_non_stream_response(content: str, model: str, request_id: str, reason: str = 'stop') -> dict:
    """æ„å»ºç¬¦åˆ OpenAI è§„èŒƒçš„éæµå¼å“åº”ä½“ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": content},
            "finish_reason": reason,
        }],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": len(content) // 4,
            "total_tokens": len(content) // 4,
        },
    }


async def save_downloaded_image_async(image_data, url, request_id):
    """ä¿å­˜å·²ä¸‹è½½çš„å›¾ç‰‡æ•°æ®åˆ°æœ¬åœ°ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰"""
    global downloaded_image_urls, downloaded_urls_set
    
    # é¿å…é‡å¤ä¿å­˜
    if url in downloaded_urls_set:
        show_full_urls = CONFIG.get("debug_show_full_urls", False)
        url_display = url if show_full_urls else url[:CONFIG.get("url_display_length", 200)]
        logger.info(f"ğŸ¨ å›¾ç‰‡å·²å­˜åœ¨è®°å½•ï¼Œè·³è¿‡ä¿å­˜: {url_display}{'...' if not show_full_urls and len(url) > CONFIG.get('url_display_length', 200) else ''}")
        return
    
    try:
        # ç›´æ¥ä½¿ç”¨å·²ä¸‹è½½çš„æ•°æ®ä¿å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
        await save_image_data(image_data, url, request_id)
        
        # æ›´æ–°å·²ä¸‹è½½è®°å½•ï¼ˆé™åˆ¶å¤§å°ï¼‰
        if url not in downloaded_urls_set:
            downloaded_image_urls.append(url)
            downloaded_urls_set.add(url)
            # å½“dequeæ»¡äº†ï¼Œè‡ªåŠ¨åˆ é™¤æœ€è€çš„è®°å½•
            if len(downloaded_urls_set) > 5000:
                # æ¸…ç†setä¸­ä¸åœ¨dequeä¸­çš„å…ƒç´ 
                downloaded_urls_set = set(downloaded_image_urls)
                
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å›¾ç‰‡å¤±è´¥: {type(e).__name__}: {e}")

async def download_image_truly_async(url, request_id):
    """[å·²åºŸå¼ƒ] çœŸæ­£çš„å¼‚æ­¥ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ° - ç°åœ¨æ”¹ä¸ºä½¿ç”¨save_downloaded_image_asyncé¿å…é‡å¤ä¸‹è½½"""
    # è¿™ä¸ªå‡½æ•°ä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹æ€§ï¼Œä½†ä¸åº”è¯¥è¢«è°ƒç”¨
    logger.warning(f"âš ï¸ download_image_truly_asyncè¢«è°ƒç”¨äº†ï¼Œè¿™æ˜¯ä¸åº”è¯¥çš„ï¼Œå› ä¸ºä¼šå¯¼è‡´é‡å¤ä¸‹è½½")
    return

async def save_image_data(image_data, url, request_id):
    """ä¿å­˜å›¾ç‰‡æ•°æ®åˆ°æ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼‰"""
    try:
        original_size_kb = len(image_data) / 1024
        
        # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹
        date_folder = datetime.now().strftime("%Y%m%d")
        date_path = IMAGE_SAVE_DIR / date_folder
        date_path.mkdir(exist_ok=True)
        logger.info(f"ğŸ“ ä½¿ç”¨æ—¥æœŸæ–‡ä»¶å¤¹: {date_folder}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ¼å¼è½¬æ¢ï¼ˆæœ¬åœ°ä¿å­˜ï¼‰
        local_format_config = CONFIG.get("local_save_format", {})
        target_ext = 'png'  # é»˜è®¤æ‰©å±•å
        
        if local_format_config.get("enabled", False):
            target_format = local_format_config.get("format", "original").lower()
            
            if target_format != "original":
                try:
                    from io import BytesIO
                    from PIL import Image
                    
                    # æ‰“å¼€å›¾ç‰‡
                    img = Image.open(BytesIO(image_data))
                    
                    # å¦‚æœæ˜¯RGBAæ¨¡å¼ä¸”è¦è½¬æ¢ä¸ºJPEGï¼Œéœ€è¦å…ˆè½¬æ¢ä¸ºRGB
                    if target_format in ['jpeg', 'jpg'] and img.mode in ('RGBA', 'LA', 'P'):
                        # åˆ›å»ºç™½è‰²èƒŒæ™¯
                        background = Image.new('RGB', img.size, (255, 255, 255))
                        if img.mode == 'P':
                            img = img.convert('RGBA')
                        background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                        img = background
                    
                    # ä¿å­˜åˆ°BytesIO
                    output = BytesIO()
                    
                    # æ ¹æ®ç›®æ ‡æ ¼å¼ä¿å­˜
                    if target_format == 'png':
                        img.save(output, format='PNG', optimize=True)
                        target_ext = 'png'
                    elif target_format in ['jpeg', 'jpg']:
                        # æœ¬åœ°ä¿å­˜ä½¿ç”¨é«˜è´¨é‡
                        jpeg_quality = local_format_config.get("jpeg_quality", 100)
                        img.save(output, format='JPEG', quality=jpeg_quality, optimize=True)
                        target_ext = 'jpg'
                    elif target_format == 'webp':
                        img.save(output, format='WEBP', quality=95, optimize=True)
                        target_ext = 'webp'
                    else:
                        # ä¸æ”¯æŒçš„æ ¼å¼ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                        output = BytesIO(image_data)
                        # ä»URLæ¨æ–­æ‰©å±•å
                        if '.jpeg' in url.lower():
                            target_ext = 'jpeg'
                        elif '.jpg' in url.lower():
                            target_ext = 'jpg'
                        elif '.png' in url.lower():
                            target_ext = 'png'
                        elif '.webp' in url.lower():
                            target_ext = 'webp'
                    
                    # è·å–è½¬æ¢åçš„æ•°æ®
                    image_data = output.getvalue()
                    
                    converted_size_kb = len(image_data) / 1024
                    logger.info(f"ğŸ”„ æœ¬åœ°ä¿å­˜å·²è½¬æ¢ä¸º {target_format.upper()} æ ¼å¼ï¼ˆ{original_size_kb:.1f}KB â†’ {converted_size_kb:.1f}KBï¼‰")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ æœ¬åœ°ä¿å­˜æ ¼å¼è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼")
                    # ä»URLæ¨æ–­æ‰©å±•å
                    if '.jpeg' in url.lower():
                        target_ext = 'jpeg'
                    elif '.jpg' in url.lower():
                        target_ext = 'jpg'
                    elif '.png' in url.lower():
                        target_ext = 'png'
                    elif '.webp' in url.lower():
                        target_ext = 'webp'
            else:
                # ä¿æŒåŸæ ¼å¼ï¼Œä»URLæ¨æ–­æ‰©å±•å
                if '.jpeg' in url.lower():
                    target_ext = 'jpeg'
                elif '.jpg' in url.lower():
                    target_ext = 'jpg'
                elif '.png' in url.lower():
                    target_ext = 'png'
                elif '.webp' in url.lower():
                    target_ext = 'webp'
        else:
            # æœªå¯ç”¨æ ¼å¼è½¬æ¢ï¼Œä»URLæ¨æ–­æ‰©å±•å
            if '.jpeg' in url.lower():
                target_ext = 'jpeg'
            elif '.jpg' in url.lower():
                target_ext = 'jpg'
            elif '.png' in url.lower():
                target_ext = 'png'
            elif '.webp' in url.lower():
                target_ext = 'webp'
            elif '.' in url:
                possible_ext = url.split('.')[-1].split('?')[0].lower()
                if possible_ext in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                    target_ext = possible_ext
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # æ·»åŠ æ¯«ç§’
        
        # ä½¿ç”¨æ—¶é—´æˆ³å’Œè¯·æ±‚IDä½œä¸ºæ–‡ä»¶å
        filename = f"{timestamp}_{request_id[:8]}.{target_ext}"
        filepath = date_path / filename  # ä½¿ç”¨æ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„
        
        # å¼‚æ­¥ä¿å­˜æ–‡ä»¶
        await asyncio.get_event_loop().run_in_executor(None, filepath.write_bytes, image_data)
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        size_kb = len(image_data) / 1024
        size_mb = size_kb / 1024
        
        if size_mb > 1:
            logger.info(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {filename} ({size_mb:.2f}MB)")
        else:
            logger.info(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {filename} ({size_kb:.1f}KB)")
        
        # æ˜¾ç¤ºå®Œæ•´è·¯å¾„
        logger.info(f"   ğŸ“ ä¿å­˜ä½ç½®: {filepath.absolute()}")
        
        # å›¾åƒè‡ªåŠ¨å¢å¼ºåŠŸèƒ½å·²ç§»é™¤ï¼ˆå¦‚éœ€å¢å¼ºè¯·ä½¿ç”¨ç‹¬ç«‹çš„image_enhanceré¡¹ç›®ï¼‰
            
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")

def download_image_async(url, request_id):
    """å…¼å®¹æ—§ç‰ˆæœ¬çš„åŒæ­¥åŒ…è£…å™¨ï¼ˆå°†è¢«å¼‚æ­¥ç‰ˆæœ¬æ›¿ä»£ï¼‰"""
    global downloaded_image_urls, downloaded_urls_set

    # é¿å…é‡å¤ä¸‹è½½
    if url in downloaded_urls_set:
        show_full_urls = CONFIG.get("debug_show_full_urls", False)
        url_display = url if show_full_urls else url[:CONFIG.get("url_display_length", 200)]
        logger.info(f"ğŸ¨ å›¾ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {url_display}{'...' if not show_full_urls and len(url) > CONFIG.get('url_display_length', 200) else ''}")
        return

    try:
        import time
        import urllib3

        # ç¦ç”¨SSLè­¦å‘Š
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        time.sleep(2)  # ç¨å¾®å»¶è¿Ÿï¼Œç¡®ä¿å›¾ç‰‡å·²ç»å®Œå…¨ç”Ÿæˆ

        # ä¸‹è½½å›¾ç‰‡ï¼ˆå…³é”®ï¼šverify=False è·³è¿‡SSLéªŒè¯ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://lmarena.ai/'
        }

        # âš ï¸ å…³é”®ä¿®æ”¹ï¼šæ·»åŠ  verify=False
        response = requests.get(url, timeout=30, headers=headers, verify=False)

        if response.status_code == 200:
            image_data = response.content
            original_size_kb = len(image_data) / 1024
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ¼å¼è½¬æ¢ï¼ˆæœ¬åœ°ä¿å­˜ï¼‰
            local_format_config = CONFIG.get("local_save_format", {})
            target_ext = 'png'  # é»˜è®¤æ‰©å±•å
            
            if local_format_config.get("enabled", False):
                target_format = local_format_config.get("format", "original").lower()
                
                if target_format != "original":
                    try:
                        from io import BytesIO
                        from PIL import Image
                        
                        # æ‰“å¼€å›¾ç‰‡
                        img = Image.open(BytesIO(image_data))
                        
                        # å¦‚æœæ˜¯RGBAæ¨¡å¼ä¸”è¦è½¬æ¢ä¸ºJPEGï¼Œéœ€è¦å…ˆè½¬æ¢ä¸ºRGB
                        if target_format in ['jpeg', 'jpg'] and img.mode in ('RGBA', 'LA', 'P'):
                            # åˆ›å»ºç™½è‰²èƒŒæ™¯
                            background = Image.new('RGB', img.size, (255, 255, 255))
                            if img.mode == 'P':
                                img = img.convert('RGBA')
                            background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                            img = background
                        
                        # ä¿å­˜åˆ°BytesIO
                        output = BytesIO()
                        
                        # æ ¹æ®ç›®æ ‡æ ¼å¼ä¿å­˜
                        if target_format == 'png':
                            img.save(output, format='PNG', optimize=True)
                            target_ext = 'png'
                        elif target_format in ['jpeg', 'jpg']:
                            # æœ¬åœ°ä¿å­˜ä½¿ç”¨é«˜è´¨é‡
                            jpeg_quality = local_format_config.get("jpeg_quality", 100)
                            img.save(output, format='JPEG', quality=jpeg_quality, optimize=True)
                            target_ext = 'jpg'
                        elif target_format == 'webp':
                            img.save(output, format='WEBP', quality=95, optimize=True)
                            target_ext = 'webp'
                        else:
                            # ä¸æ”¯æŒçš„æ ¼å¼ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                            output = BytesIO(image_data)
                            # ä»URLæ¨æ–­æ‰©å±•å
                            if '.jpeg' in url.lower():
                                target_ext = 'jpeg'
                            elif '.jpg' in url.lower():
                                target_ext = 'jpg'
                            elif '.png' in url.lower():
                                target_ext = 'png'
                            elif '.webp' in url.lower():
                                target_ext = 'webp'
                        
                        # è·å–è½¬æ¢åçš„æ•°æ®
                        image_data = output.getvalue()
                        
                        converted_size_kb = len(image_data) / 1024
                        logger.info(f"ğŸ”„ æœ¬åœ°ä¿å­˜å·²è½¬æ¢ä¸º {target_format.upper()} æ ¼å¼ï¼ˆ{original_size_kb:.1f}KB â†’ {converted_size_kb:.1f}KBï¼‰")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ æœ¬åœ°ä¿å­˜æ ¼å¼è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼")
                        # è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®å’Œæ‰©å±•å
                        image_data = response.content
                        # ä»URLæ¨æ–­æ‰©å±•å
                        if '.jpeg' in url.lower():
                            target_ext = 'jpeg'
                        elif '.jpg' in url.lower():
                            target_ext = 'jpg'
                        elif '.png' in url.lower():
                            target_ext = 'png'
                        elif '.webp' in url.lower():
                            target_ext = 'webp'
                else:
                    # ä¿æŒåŸæ ¼å¼
                    # ä»URLæ¨æ–­æ‰©å±•å
                    if '.jpeg' in url.lower():
                        target_ext = 'jpeg'
                    elif '.jpg' in url.lower():
                        target_ext = 'jpg'
                    elif '.png' in url.lower():
                        target_ext = 'png'
                    elif '.webp' in url.lower():
                        target_ext = 'webp'
            else:
                # æœªå¯ç”¨æ ¼å¼è½¬æ¢ï¼Œä»URLæ¨æ–­æ‰©å±•å
                if '.jpeg' in url.lower():
                    target_ext = 'jpeg'
                elif '.jpg' in url.lower():
                    target_ext = 'jpg'
                elif '.png' in url.lower():
                    target_ext = 'png'
                elif '.webp' in url.lower():
                    target_ext = 'webp'
                elif '.' in url:
                    possible_ext = url.split('.')[-1].split('?')[0].lower()
                    if possible_ext in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                        target_ext = possible_ext

            # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹
            date_folder = datetime.now().strftime("%Y%m%d")
            date_path = IMAGE_SAVE_DIR / date_folder
            date_path.mkdir(exist_ok=True)
            logger.info(f"ğŸ“ ä½¿ç”¨æ—¥æœŸæ–‡ä»¶å¤¹: {date_folder}")
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # æ·»åŠ æ¯«ç§’
            
            # ä½¿ç”¨æ—¶é—´æˆ³å’Œè¯·æ±‚IDä½œä¸ºæ–‡ä»¶å
            filename = f"{timestamp}_{request_id[:8]}.{target_ext}"
            filepath = date_path / filename  # ä½¿ç”¨æ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„

            # ä¿å­˜æ–‡ä»¶
            filepath.write_bytes(image_data)

            # æ›´æ–°å·²ä¸‹è½½è®°å½•ï¼ˆé™åˆ¶å¤§å°ï¼‰
            if url not in downloaded_urls_set:
                downloaded_image_urls.append(url)
                downloaded_urls_set.add(url)
                # å½“dequeæ»¡äº†ï¼Œè‡ªåŠ¨åˆ é™¤æœ€è€çš„è®°å½•
                if len(downloaded_urls_set) > 5000:
                    # æ¸…ç†setä¸­ä¸åœ¨dequeä¸­çš„å…ƒç´ 
                    downloaded_urls_set = set(downloaded_image_urls)

            # è®¡ç®—æ–‡ä»¶å¤§å°
            size_kb = len(image_data) / 1024
            size_mb = size_kb / 1024

            if size_mb > 1:
                logger.info(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {filename} ({size_mb:.2f}MB)")
            else:
                logger.info(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {filename} ({size_kb:.1f}KB)")

            # æ˜¾ç¤ºå®Œæ•´è·¯å¾„
            logger.info(f"   ğŸ“ ä¿å­˜ä½ç½®: {filepath.absolute()}")
            
            # å›¾åƒè‡ªåŠ¨å¢å¼ºåŠŸèƒ½å·²ç§»é™¤ï¼ˆå¦‚éœ€å¢å¼ºè¯·ä½¿ç”¨ç‹¬ç«‹çš„image_enhanceré¡¹ç›®ï¼‰

        else:
            logger.error(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")

    except requests.exceptions.Timeout:
        show_full_urls = CONFIG.get("debug_show_full_urls", False)
        url_display = url if show_full_urls else url[:CONFIG.get("url_display_length", 200)]
        logger.error(f"âŒ å›¾ç‰‡ä¸‹è½½è¶…æ—¶: {url_display}{'...' if not show_full_urls and len(url) > CONFIG.get('url_display_length', 200) else ''}")
    except requests.exceptions.SSLError as e:
        logger.error(f"âŒ SSLé”™è¯¯ï¼ˆå°è¯•ä½¿ç”¨verify=Falseï¼‰: {e}")
    except Exception as e:
        logger.error(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {type(e).__name__}: {e}")


async def _process_lmarena_stream(request_id: str):
    """
    æ ¸å¿ƒå†…éƒ¨ç”Ÿæˆå™¨ï¼šå¤„ç†æ¥è‡ªæµè§ˆå™¨çš„åŸå§‹æ•°æ®æµï¼Œå¹¶äº§ç”Ÿç»“æ„åŒ–äº‹ä»¶ã€‚
    äº‹ä»¶ç±»å‹: ('content', str), ('finish', str), ('error', str), ('retry_info', dict)
    """
    global IS_REFRESHING_FOR_VERIFICATION, aiohttp_session
    
    # ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„é…ç½®
    load_config()
    
    queue = response_channels.get(request_id)
    if not queue:
        logger.error(f"PROCESSOR [ID: {request_id[:8]}]: æ— æ³•æ‰¾åˆ°å“åº”é€šé“ã€‚")
        yield 'error', 'Internal server error: response channel not found.'
        return

    buffer = ""
    timeout = CONFIG.get("stream_response_timeout_seconds",360)
    text_pattern = re.compile(r'[ab]0:"((?:\\.|[^"\\])*)"')
    # æ–°å¢ï¼šç”¨äºåŒ¹é…æ€ç»´é“¾å†…å®¹çš„æ­£åˆ™è¡¨è¾¾å¼
    reasoning_pattern = re.compile(r'ag:"((?:\\.|[^"\\])*)"')
    # æ–°å¢ï¼šç”¨äºåŒ¹é…å’Œæå–å›¾ç‰‡URLçš„æ­£åˆ™è¡¨è¾¾å¼
    image_pattern = re.compile(r'[ab]2:(\[.*?\])')
    finish_pattern = re.compile(r'[ab]d:(\{.*?"finishReason".*?\})')
    error_pattern = re.compile(r'(\{\s*"error".*?\})', re.DOTALL)
    cloudflare_patterns = [r'<title>Just a moment...</title>', r'Enable JavaScript and cookies to continue']
    
    has_yielded_content = False # æ ‡è®°æ˜¯å¦å·²äº§å‡ºè¿‡æœ‰æ•ˆå†…å®¹
    
    # æ€ç»´é“¾ç›¸å…³å˜é‡
    # æ³¨æ„ï¼šæ€ç»´é“¾æ•°æ®åº”è¯¥æ€»æ˜¯è¢«æ”¶é›†ï¼ˆç”¨äºç›‘æ§å’Œæ—¥å¿—ï¼‰ï¼Œä½†æ˜¯å¦è¾“å‡ºç»™å®¢æˆ·ç«¯ç”±é…ç½®å†³å®š
    enable_reasoning_output = CONFIG.get("enable_lmarena_reasoning", False)  # æ˜¯å¦è¾“å‡ºç»™å®¢æˆ·ç«¯
    reasoning_buffer = []  # ç¼“å†²æ‰€æœ‰æ€ç»´é“¾ç‰‡æ®µ
    has_reasoning = False  # æ ‡è®°æ˜¯å¦æœ‰æ€ç»´é“¾å†…å®¹
    reasoning_ended = False  # æ ‡è®°reasoningæ˜¯å¦å·²ç»“æŸ
    
    # è¯Šæ–­ï¼šæ·»åŠ æµå¼æ€§èƒ½è¿½è¸ª
    import time as time_module
    last_yield_time = time_module.time()
    chunk_count = 0
    total_chars = 0

    try:
        while True:
            # å…³é”®ä¿®å¤ï¼šæ¯æ¬¡å¾ªç¯å¼€å§‹æ—¶é‡ç½®reasoning_foundæ ‡å¿—
            reasoning_found_in_this_chunk = False
            
            try:
                # è¯Šæ–­ï¼šè®°å½•æ¥æ”¶æ•°æ®çš„æ—¶é—´
                receive_start = time_module.time()
                raw_data = await asyncio.wait_for(queue.get(), timeout=timeout)
                receive_time = time_module.time() - receive_start
                
                if CONFIG.get("debug_stream_timing", False):
                    logger.debug(f"[STREAM_TIMING] ä»é˜Ÿåˆ—è·å–æ•°æ®è€—æ—¶: {receive_time:.3f}ç§’")
                    # è¯Šæ–­ï¼šæ˜¾ç¤ºåŸå§‹æ•°æ®çš„å‰200ä¸ªå­—ç¬¦
                    raw_data_str = str(raw_data)[:200] if raw_data else "None"
                    logger.debug(f"[STREAM_RAW] åŸå§‹æ•°æ®: {raw_data_str}...")
                    
            except asyncio.TimeoutError:
                logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: ç­‰å¾…æµè§ˆå™¨æ•°æ®è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ã€‚")
                yield 'error', f'Response timed out after {timeout} seconds.'
                return

            # --- Cloudflare äººæœºéªŒè¯å¤„ç† ---
            def handle_cloudflare_verification():
                global IS_REFRESHING_FOR_VERIFICATION
                if not IS_REFRESHING_FOR_VERIFICATION:
                    logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: é¦–æ¬¡æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œå°†å‘é€åˆ·æ–°æŒ‡ä»¤ã€‚")
                    IS_REFRESHING_FOR_VERIFICATION = True
                    if browser_ws:
                        asyncio.create_task(browser_ws.send_text(json.dumps({"command": "refresh"}, ensure_ascii=False)))
                    return "æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œå·²å‘é€åˆ·æ–°æŒ‡ä»¤ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                else:
                    logger.info(f"PROCESSOR [ID: {request_id[:8]}]: æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œä½†å·²åœ¨åˆ·æ–°ä¸­ï¼Œå°†ç­‰å¾…ã€‚")
                    return "æ­£åœ¨ç­‰å¾…äººæœºéªŒè¯å®Œæˆ..."

            # 1. æ£€æŸ¥æ¥è‡ª WebSocket ç«¯çš„ç›´æ¥é”™è¯¯æˆ–é‡è¯•ä¿¡æ¯
            if isinstance(raw_data, dict):
                # å¤„ç†é‡è¯•ä¿¡æ¯
                if 'retry_info' in raw_data:
                    retry_info = raw_data.get('retry_info', {})
                    logger.info(f"PROCESSOR [ID: {request_id[:8]}]: æ”¶åˆ°é‡è¯•ä¿¡æ¯ - å°è¯• {retry_info.get('attempt')}/{retry_info.get('max_attempts')}")
                    # å¯ä»¥é€‰æ‹©å°†é‡è¯•ä¿¡æ¯ä¼ é€’ç»™å®¢æˆ·ç«¯
                    yield 'retry_info', retry_info
                    continue
                
                # å¤„ç†é”™è¯¯
                if 'error' in raw_data:
                    error_msg = raw_data.get('error', 'Unknown browser error')
                if isinstance(error_msg, str):
                    if '413' in error_msg or 'too large' in error_msg.lower():
                        friendly_error_msg = "ä¸Šä¼ å¤±è´¥ï¼šé™„ä»¶å¤§å°è¶…è¿‡äº† LMArena æœåŠ¡å™¨çš„é™åˆ¶ (é€šå¸¸æ˜¯ 5MBå·¦å³)ã€‚è¯·å°è¯•å‹ç¼©æ–‡ä»¶æˆ–ä¸Šä¼ æ›´å°çš„æ–‡ä»¶ã€‚"
                        logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: æ£€æµ‹åˆ°é™„ä»¶è¿‡å¤§é”™è¯¯ (413)ã€‚")
                        yield 'error', friendly_error_msg
                        return
                    if any(re.search(p, error_msg, re.IGNORECASE) for p in cloudflare_patterns):
                        yield 'error', handle_cloudflare_verification()
                        return
                yield 'error', error_msg
                return

            # 2. æ£€æŸ¥ [DONE] ä¿¡å·
            if raw_data == "[DONE]":
                # çŠ¶æ€é‡ç½®é€»è¾‘å·²ç§»è‡³ websocket_endpointï¼Œä»¥ç¡®ä¿è¿æ¥æ¢å¤æ—¶çŠ¶æ€ä¸€å®šè¢«é‡ç½®
                if has_yielded_content and IS_REFRESHING_FOR_VERIFICATION:
                     logger.info(f"PROCESSOR [ID: {request_id[:8]}]: è¯·æ±‚æˆåŠŸï¼ŒäººæœºéªŒè¯çŠ¶æ€å°†åœ¨ä¸‹æ¬¡è¿æ¥æ—¶é‡ç½®ã€‚")
                break

            # 3. ç´¯åŠ ç¼“å†²åŒºå¹¶æ£€æŸ¥å†…å®¹
            buffer += "".join(str(item) for item in raw_data) if isinstance(raw_data, list) else raw_data
            
            # è¯Šæ–­ï¼šæ˜¾ç¤ºç¼“å†²åŒºå¤§å°
            if CONFIG.get("debug_stream_timing", False):
                logger.debug(f"[STREAM_BUFFER] ç¼“å†²åŒºå¤§å°: {len(buffer)} å­—ç¬¦")

            if any(re.search(p, buffer, re.IGNORECASE) for p in cloudflare_patterns):
                yield 'error', handle_cloudflare_verification()
                return
            
            if (error_match := error_pattern.search(buffer)):
                try:
                    error_json = json.loads(error_match.group(1))
                    yield 'error', error_json.get("error", "æ¥è‡ª LMArena çš„æœªçŸ¥é”™è¯¯")
                    return
                except json.JSONDecodeError: pass

            # ä¼˜å…ˆå¤„ç†æ€ç»´é“¾å†…å®¹ï¼ˆagå‰ç¼€ï¼‰
            # æ³¨æ„ï¼šæ€ç»´é“¾å§‹ç»ˆè¢«è§£æå’Œæ”¶é›†ï¼ˆç”¨äºç›‘æ§ï¼‰ï¼Œä½†åªåœ¨é…ç½®å¯ç”¨æ—¶æ‰è¾“å‡ºç»™å®¢æˆ·ç«¯
            reasoning_found_in_this_chunk = False
            while (match := reasoning_pattern.search(buffer)):
                try:
                    reasoning_content = json.loads(f'"{match.group(1)}"')
                    if reasoning_content:
                        # è­¦å‘Šï¼šæ£€æµ‹åˆ°reasoningåœ¨contentä¹‹åå‡ºç°ï¼ˆå¼‚å¸¸æƒ…å†µï¼‰
                        if reasoning_ended:
                            logger.warning(f"[REASONING_WARN] æ£€æµ‹åˆ°reasoningåœ¨contentä¹‹åç»§ç»­å‡ºç°ï¼Œè¿™å¯èƒ½å¯¼è‡´think_tagæ¨¡å¼ä¸‹å†…å®¹ä¸¢å¤±ï¼")
                        
                        # æ€»æ˜¯æ”¶é›†æ€ç»´é“¾ï¼ˆç”¨äºç›‘æ§å’Œæ—¥å¿—ï¼‰
                        has_reasoning = True
                        reasoning_buffer.append(reasoning_content)
                        reasoning_found_in_this_chunk = True
                        
                        # åªåœ¨é…ç½®å¯ç”¨æ—¶æ‰è¾“å‡ºç»™å®¢æˆ·ç«¯
                        if enable_reasoning_output and CONFIG.get("preserve_streaming", True):
                            # æµå¼è¾“å‡ºæ€ç»´é“¾
                            yield 'reasoning', reasoning_content
                        
                except (ValueError, json.JSONDecodeError) as e:
                    if CONFIG.get("debug_stream_timing", False):
                        logger.debug(f"[REASONING_ERROR] è§£æé”™è¯¯: {e}")
                    pass
                buffer = buffer[match.end():]
            
            # å¤„ç†æ–‡æœ¬å†…å®¹ï¼ˆa0å‰ç¼€ï¼‰- æ·»åŠ è¯Šæ–­
            process_start = time_module.time()
            chunks_in_buffer = 0
            
            # è¯Šæ–­ï¼šæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…
            if CONFIG.get("debug_stream_timing", False):
                matches_found = text_pattern.findall(buffer)
                if matches_found:
                    logger.debug(f"[STREAM_MATCH] æ‰¾åˆ° {len(matches_found)} ä¸ªæ–‡æœ¬åŒ¹é…")
                    for idx, match in enumerate(matches_found[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        logger.debug(f"  åŒ¹é…#{idx+1}: {match[:50]}...")
            
            while (match := text_pattern.search(buffer)):
                try:
                    text_content = json.loads(f'"{match.group(1)}"')
                    if text_content:
                        # å…³é”®ä¿®å¤ï¼šåœ¨ç¬¬ä¸€ä¸ªcontentåˆ°æ¥æ—¶ï¼Œå¦‚æœæœ‰reasoningä¸”æœªç»“æŸï¼Œåˆ™æ ‡è®°ç»“æŸ
                        if has_reasoning and not reasoning_ended and not reasoning_found_in_this_chunk:
                            reasoning_ended = True
                            logger.info(f"[REASONING_END] æ£€æµ‹åˆ°reasoningç»“æŸï¼ˆå…±{len(reasoning_buffer)}ä¸ªç‰‡æ®µï¼‰")
                            # åªåœ¨å¯ç”¨è¾“å‡ºæ—¶æ‰å‘é€ç»“æŸäº‹ä»¶
                            if enable_reasoning_output:
                                yield 'reasoning_end', None
                        
                        has_yielded_content = True
                        chunk_count += 1
                        total_chars += len(text_content)
                        chunks_in_buffer += 1
                        
                        # è¯Šæ–­ï¼šè®°å½•yieldé—´éš”
                        current_time = time_module.time()
                        yield_interval = current_time - last_yield_time
                        last_yield_time = current_time
                        
                        if CONFIG.get("debug_stream_timing", False):
                            logger.debug(f"[STREAM_TIMING] Yieldé—´éš”: {yield_interval:.3f}ç§’, "
                                       f"å—#{chunk_count}, å­—ç¬¦æ•°: {len(text_content)}, "
                                       f"ç´¯è®¡å­—ç¬¦: {total_chars}")
                        
                        yield 'content', text_content
                        
                        # ç«‹å³å¤„ç†ï¼Œä¸è¦ç­‰å¾…
                        await asyncio.sleep(0)
                        
                except (ValueError, json.JSONDecodeError) as e:
                    if CONFIG.get("debug_stream_timing", False):
                        logger.debug(f"[STREAM_ERROR] è§£æé”™è¯¯: {e}")
                    pass
                buffer = buffer[match.end():]
            
            # è¯Šæ–­ï¼šè®°å½•å¤„ç†æ—¶é—´
            if chunks_in_buffer > 0 and CONFIG.get("debug_stream_timing", False):
                process_time = time_module.time() - process_start
                logger.debug(f"[STREAM_TIMING] å¤„ç†{chunks_in_buffer}ä¸ªæ–‡æœ¬å—è€—æ—¶: {process_time:.3f}ç§’")

            # æ–°å¢ï¼šå¤„ç†å›¾ç‰‡å†…å®¹
            while (match := image_pattern.search(buffer)):
                try:
                    image_data_list = json.loads(match.group(1))
                    if isinstance(image_data_list, list) and image_data_list:
                        image_info = image_data_list[0]
                        if image_info.get("type") == "image" and "image" in image_info:
                            image_url = image_info['image']
                            
                            # å°†LMArenaè¿”å›çš„å›¾ç‰‡URLè½¬æ¢ä¸ºbase64è¿”å›ç»™å®¢æˆ·ç«¯
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºå®Œæ•´URLï¼ˆä»é…ç½®ä¸­è¯»å–ï¼‰
                            show_full_urls = CONFIG.get("debug_show_full_urls", False)
                            if show_full_urls:
                                logger.info(f"ğŸ“¥ LMArenaè¿”å›å›¾ç‰‡URLï¼ˆå®Œæ•´ï¼‰: {image_url}")
                            else:
                                # æ˜¾ç¤ºæ›´å¤šå­—ç¬¦ï¼Œé»˜è®¤200ä¸ªï¼ˆé€šå¸¸è¶³å¤Ÿçœ‹åˆ°å®Œæ•´çš„URLï¼‰
                                display_length = CONFIG.get("url_display_length", 200)
                                if len(image_url) <= display_length:
                                    logger.info(f"ğŸ“¥ LMArenaè¿”å›å›¾ç‰‡URL: {image_url}")
                                else:
                                    logger.info(f"ğŸ“¥ LMArenaè¿”å›å›¾ç‰‡URL: {image_url[:display_length]}...")
                                    logger.debug(f"   å®Œæ•´URL: {image_url}")  # åœ¨DEBUGçº§åˆ«è®°å½•å®Œæ•´URL
                            
                            # è®°å½•å¼€å§‹æ—¶é—´
                            import time as time_module
                            import base64
                            process_start_time = time_module.time()
                            
                            # è·å–è¿”å›æ¨¡å¼é…ç½®
                            return_format_config = CONFIG.get("image_return_format", {})
                            return_mode = return_format_config.get("mode", "base64")
                            save_locally = CONFIG.get("save_images_locally", True)
                            
                            # è¯Šæ–­æ—¥å¿—ï¼šè®°å½•å¤„ç†å¼€å§‹
                            logger.info(f"[IMG_PROCESS] å¼€å§‹å¤„ç†å›¾ç‰‡")
                            logger.info(f"  - è¿”å›æ¨¡å¼: {return_mode}")
                            logger.info(f"  - æœ¬åœ°ä¿å­˜: {save_locally}")
                            
                            # URLæ¨¡å¼ï¼šç«‹å³è¿”å›ï¼Œä¸é˜»å¡
                            if return_mode == "url":
                                logger.info(f"[IMG_PROCESS] URLæ¨¡å¼ - ç«‹å³è¿”å›URLç»™å®¢æˆ·ç«¯")
                                # ç«‹å³è¿”å›URLç»™å®¢æˆ·ç«¯ï¼Œä¸ç­‰å¾…ä»»ä½•ä¸‹è½½
                                yield 'content', f"![Image]({image_url})"
                                
                                # å¦‚æœéœ€è¦ä¿å­˜åˆ°æœ¬åœ°ï¼Œåˆ›å»ºåå°ä»»åŠ¡ï¼ˆä¸é˜»å¡å“åº”ï¼‰
                                if save_locally:
                                    logger.info(f"[IMG_PROCESS] å¯åŠ¨åå°ä»»åŠ¡å¼‚æ­¥ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡")
                                    
                                    async def async_download_and_save():
                                        try:
                                            download_start = time_module.time()
                                            img_data, err = await _download_image_data_with_retry(image_url)
                                            download_time = time_module.time() - download_start
                                            
                                            if img_data:
                                                logger.info(f"[IMG_PROCESS] åå°ä¸‹è½½æˆåŠŸï¼Œè€—æ—¶: {download_time:.2f}ç§’")
                                                await save_downloaded_image_async(img_data, image_url, request_id)
                                                logger.info(f"[IMG_PROCESS] å›¾ç‰‡å·²ä¿å­˜åˆ°æœ¬åœ°")
                                            else:
                                                logger.error(f"[IMG_PROCESS] åå°ä¸‹è½½å¤±è´¥: {err}")
                                        except Exception as e:
                                            logger.error(f"[IMG_PROCESS] åå°ä»»åŠ¡å¼‚å¸¸: {e}")
                                    
                                    # åˆ›å»ºåå°ä»»åŠ¡ï¼Œä¸ç­‰å¾…å®Œæˆ
                                    asyncio.create_task(async_download_and_save())
                                else:
                                    logger.info(f"[IMG_PROCESS] save_images_locally=falseï¼Œè·³è¿‡ä¸‹è½½")
                                
                                # URLæ¨¡å¼å¤„ç†å®Œæˆï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ¶ˆæ¯
                                continue
                            
                            # Base64æ¨¡å¼ï¼šå¿…é¡»å…ˆä¸‹è½½æ‰èƒ½è½¬æ¢
                            logger.info(f"[IMG_PROCESS] Base64æ¨¡å¼ - éœ€è¦ä¸‹è½½å›¾ç‰‡è¿›è¡Œè½¬æ¢")
                            
                            # ä¸‹è½½å›¾ç‰‡æ•°æ®
                            download_start_time = time_module.time()
                            image_data, download_error = await _download_image_data_with_retry(image_url)
                            download_time = time_module.time() - download_start_time
                            logger.info(f"[IMG_PROCESS] å›¾ç‰‡ä¸‹è½½å®Œæˆï¼Œè€—æ—¶: {download_time:.2f}ç§’")
                            
                            # å¦‚æœéœ€è¦ä¿å­˜åˆ°æœ¬åœ°
                            if save_locally and image_data:
                                logger.info(f"[IMG_PROCESS] å¼‚æ­¥ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°")
                                asyncio.create_task(save_downloaded_image_async(image_data, image_url, request_id))
                            elif not save_locally:
                                logger.info(f"[IMG_PROCESS] save_images_locally=falseï¼Œè·³è¿‡æœ¬åœ°ä¿å­˜")
                            
                            # Base64è½¬æ¢
                            if True:  # è¿™é‡Œç¡®å®šæ˜¯base64æ¨¡å¼
                                if image_data:
                                    # --- Base64 è½¬æ¢å’Œç¼“å­˜é€»è¾‘ ---
                                    cache_key = image_url
                                    current_time = time_module.time()
                                    
                                    # æ¸…ç†è¿‡æœŸç¼“å­˜
                                    if len(IMAGE_BASE64_CACHE) > IMAGE_CACHE_MAX_SIZE:
                                        sorted_items = sorted(IMAGE_BASE64_CACHE.items(), key=lambda x: x[1][1])
                                        for url, _ in sorted_items[:IMAGE_CACHE_MAX_SIZE // 2]:
                                            del IMAGE_BASE64_CACHE[url]
                                        logger.info(f"  ğŸ§¹ æ¸…ç†äº† {IMAGE_CACHE_MAX_SIZE // 2} ä¸ªæ—§ç¼“å­˜")

                                    # æ£€æŸ¥ç¼“å­˜
                                    if cache_key in IMAGE_BASE64_CACHE:
                                        cached_data, cache_time = IMAGE_BASE64_CACHE[cache_key]
                                        if current_time - cache_time < IMAGE_CACHE_TTL:
                                            logger.info(f"  âš¡ ä»ç¼“å­˜è·å–å›¾ç‰‡Base64")
                                            yield 'content', cached_data
                                            continue
                                    
                                    # æ‰§è¡Œè½¬æ¢
                                    content_type = mimetypes.guess_type(image_url)[0] or 'image/png'
                                    image_base64 = base64.b64encode(image_data).decode('ascii')
                                    data_url = f"data:{content_type};base64,{image_base64}"
                                    markdown_image = f"![Image]({data_url})"
                                    
                                    # å­˜å…¥ç¼“å­˜
                                    IMAGE_BASE64_CACHE[cache_key] = (markdown_image, current_time)
                                    
                                    # è®¡ç®—æ€»è€—æ—¶
                                    total_time = time_module.time() - process_start_time
                                    logger.info(f"[IMG_PROCESS] Base64è½¬æ¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
                                    
                                    yield 'content', markdown_image
                                else:
                                    # ä¸‹è½½å¤±è´¥ï¼Œé™çº§è¿”å›URL
                                    logger.error(f"[IMG_PROCESS] âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ ({download_error})ï¼Œé™çº§è¿”å›åŸå§‹URL")
                                    total_time = time_module.time() - process_start_time
                                    logger.info(f"[IMG_PROCESS] å¤„ç†å®Œæˆï¼ˆå¤±è´¥é™çº§ï¼‰ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
                                    yield 'content', f"![Image]({image_url})"

                except (json.JSONDecodeError, IndexError) as e:
                    logger.warning(f"è§£æå›¾ç‰‡URLæ—¶å‡ºé”™: {e}, buffer: {buffer[:150]}")
                buffer = buffer[match.end():]

            if (finish_match := finish_pattern.search(buffer)):
                try:
                    finish_data = json.loads(finish_match.group(1))
                    yield 'finish', finish_data.get("finishReason", "stop")
                except (json.JSONDecodeError, IndexError): pass
                buffer = buffer[finish_match.end():]

    except asyncio.CancelledError:
        logger.info(f"PROCESSOR [ID: {request_id[:8]}]: ä»»åŠ¡è¢«å–æ¶ˆã€‚")
    finally:
        # åœ¨æ¸…ç†å‰ï¼Œå¦‚æœæœ‰æ€ç»´é“¾å†…å®¹ä¸”æœªæµå¼è¾“å‡ºï¼Œåˆ™ä¸€æ¬¡æ€§è¾“å‡º
        if enable_reasoning_output and has_reasoning and not CONFIG.get("preserve_streaming", True):
            # éæµå¼æ¨¡å¼ï¼šåœ¨æœ€åä¸€æ¬¡æ€§è¾“å‡ºå®Œæ•´æ€ç»´é“¾
            full_reasoning = "".join(reasoning_buffer)
            yield 'reasoning_complete', full_reasoning
        
        # è¯Šæ–­ï¼šè¾“å‡ºæµå¼æ€§èƒ½ç»Ÿè®¡
        if chunk_count > 0 and CONFIG.get("debug_stream_timing", False):
            total_time = time_module.time() - (last_yield_time - yield_interval if 'yield_interval' in locals() else last_yield_time)
            logger.info(f"[STREAM_STATS] è¯·æ±‚ID: {request_id[:8]}")
            logger.info(f"  - æ€»å—æ•°: {chunk_count}")
            logger.info(f"  - æ€»å­—ç¬¦æ•°: {total_chars}")
            logger.info(f"  - å¹³å‡å—å¤§å°: {total_chars/chunk_count:.1f}å­—ç¬¦")
            logger.info(f"  - å¹³å‡yieldé—´éš”: {total_time/chunk_count:.3f}ç§’")
            
        if request_id in response_channels:
            del response_channels[request_id]
            logger.info(f"PROCESSOR [ID: {request_id[:8]}]: å“åº”é€šé“å·²æ¸…ç†ã€‚")
        
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ¸…ç†request_metadataï¼Œå› ä¸ºtoken loggingéœ€è¦ç”¨åˆ°å®ƒ
        # request_metadataå°†åœ¨stream_generatorå’Œnon_stream_responseä¸­æ¸…ç†

async def stream_generator(request_id: str, model: str):
    """å°†å†…éƒ¨äº‹ä»¶æµæ ¼å¼åŒ–ä¸º OpenAI SSE å“åº”ã€‚"""
    response_id = f"chatcmpl-{uuid.uuid4()}"
    logger.info(f"STREAMER [ID: {request_id[:8]}]: æµå¼ç”Ÿæˆå™¨å¯åŠ¨ã€‚")
    
    finish_reason_to_send = 'stop'  # é»˜è®¤çš„ç»“æŸåŸå› 
    collected_content = []  # æ”¶é›†å“åº”å†…å®¹ç”¨äºå­˜å‚¨
    reasoning_content = []  # æ”¶é›†æ€ç»´é“¾å†…å®¹
    
    # è¯Šæ–­ï¼šæ·»åŠ æµå¼æ€§èƒ½è¿½è¸ª
    import time as time_module
    stream_start_time = time_module.time()
    chunks_sent = 0
    
    # æ€ç»´é“¾é…ç½®
    # æ³¨æ„ï¼šæ€ç»´é“¾æ•°æ®æ€»æ˜¯è¢«æ”¶é›†ï¼Œä½†åªåœ¨å¯ç”¨æ—¶æ‰è¾“å‡ºç»™å®¢æˆ·ç«¯
    enable_reasoning_output = CONFIG.get("enable_lmarena_reasoning", False)
    reasoning_mode = CONFIG.get("reasoning_output_mode", "openai")
    preserve_streaming = CONFIG.get("preserve_streaming", True)

    async for event_type, data in _process_lmarena_stream(request_id):
        if event_type == 'retry_info':
            # å¤„ç†é‡è¯•ä¿¡æ¯ï¼Œå¯ä»¥å‘é€ç»™å®¢æˆ·ç«¯ä½œä¸ºæ³¨é‡Š
            retry_msg = f"\n[é‡è¯•ä¿¡æ¯] å°è¯• {data.get('attempt')}/{data.get('max_attempts')}ï¼ŒåŸå› : {data.get('reason')}ï¼Œç­‰å¾… {data.get('delay')/1000}ç§’...\n"
            logger.info(f"STREAMER [ID: {request_id[:8]}]: {retry_msg.strip()}")
            # å¯é€‰ï¼šå°†é‡è¯•ä¿¡æ¯ä½œä¸ºæ³¨é‡Šå‘é€ç»™å®¢æˆ·ç«¯
            if CONFIG.get("show_retry_info_to_client", False):
                yield format_openai_chunk(retry_msg, model, response_id)
        elif event_type == 'reasoning':
            # å¤„ç†æ€ç»´é“¾ç‰‡æ®µ
            # æ€»æ˜¯æ”¶é›†æ€ç»´é“¾ï¼ˆç”¨äºç›‘æ§ï¼‰ï¼Œä½†åªåœ¨å¯ç”¨æ—¶è¾“å‡ºç»™å®¢æˆ·ç«¯
            reasoning_content.append(data)
            
            if enable_reasoning_output:
                if reasoning_mode == "openai" and preserve_streaming:
                    # OpenAIæ¨¡å¼ä¸”å¯ç”¨æµå¼ï¼šå‘é€reasoning delta
                    chunk = {
                        "id": response_id,
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"reasoning_content": data},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                # think_tagæ¨¡å¼ï¼šæ”¶é›†ä¸ç«‹å³è¾“å‡º
                
        elif event_type == 'reasoning_end':
            # æ–°å¢ï¼šreasoningç»“æŸäº‹ä»¶ï¼ˆthink_tagæ¨¡å¼ä¸“ç”¨ï¼‰
            if enable_reasoning_output and reasoning_mode == "think_tag" and reasoning_content:
                # ç«‹å³è¾“å‡ºå®Œæ•´çš„reasoning
                full_reasoning = "".join(reasoning_content)
                wrapped_reasoning = f"<think>{full_reasoning}</think>\n\n"
                yield format_openai_chunk(wrapped_reasoning, model, response_id)
                logger.info(f"[THINK_TAG] å·²è¾“å‡ºå®Œæ•´reasoningï¼ˆ{len(reasoning_content)}ä¸ªç‰‡æ®µï¼‰")
                
        elif event_type == 'reasoning_complete':
            # å¤„ç†å®Œæ•´æ€ç»´é“¾ï¼ˆéæµå¼æ¨¡å¼ï¼‰
            # æ€»æ˜¯æ”¶é›†ï¼Œä½†åªåœ¨å¯ç”¨æ—¶è¾“å‡º
            full_reasoning = data
            reasoning_content.append(full_reasoning)
            
            if enable_reasoning_output and not preserve_streaming:
                if reasoning_mode == "openai":
                    # OpenAIæ¨¡å¼ï¼šå‘é€å®Œæ•´reasoning
                    chunk = {
                        "id": response_id,
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"reasoning_content": full_reasoning},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                elif reasoning_mode == "think_tag":
                    # think_tagæ¨¡å¼ï¼šåŒ…è£¹åä½œä¸ºcontentè¾“å‡º
                    wrapped_reasoning = f"<think>{full_reasoning}</think>\n\n"
                    yield format_openai_chunk(wrapped_reasoning, model, response_id)
                    
        elif event_type == 'content':
            
            collected_content.append(data)  # æ”¶é›†å†…å®¹
            chunks_sent += 1
            
            # ç«‹å³ç”Ÿæˆå¹¶å‘é€æ•°æ®å—ï¼Œä¸è¦ç´¯ç§¯
            chunk_data = format_openai_chunk(data, model, response_id)
            
            if CONFIG.get("debug_stream_timing", False):
                logger.debug(f"[STREAM_OUTPUT] å‘é€å—#{chunks_sent}, å¤§å°: {len(chunk_data)}å­—èŠ‚")
            
            # é‡è¦ï¼šç«‹å³yieldï¼Œä¸è¦ç­‰å¾…
            yield chunk_data
            
            # å¦‚æœå¯ç”¨å¼ºåˆ¶åˆ·æ–°ï¼ˆé’ˆå¯¹æŸäº›å®¢æˆ·ç«¯ï¼‰
            if CONFIG.get("force_stream_flush", True):
                # æ·»åŠ ä¸€ä¸ªå¾®å°çš„å¼‚æ­¥æš‚åœæ¥å¼ºåˆ¶åˆ·æ–°
                await asyncio.sleep(0)
        elif event_type == 'finish':
            # è®°å½•ç»“æŸåŸå› ï¼Œä½†ä¸è¦ç«‹å³è¿”å›ï¼Œç­‰å¾…æµè§ˆå™¨å‘é€ [DONE]
            finish_reason_to_send = data
            if data == 'content-filter':
                warning_msg = "\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥ï¼ˆå¤§æ¦‚ç‡ï¼‰çš„åŸå› "
                collected_content.append(warning_msg)  # ä¹Ÿæ”¶é›†è­¦å‘Šä¿¡æ¯
                yield format_openai_chunk(warning_msg, model, response_id)
        elif event_type == 'error':
            logger.error(f"STREAMER [ID: {request_id[:8]}]: æµä¸­å‘ç”Ÿé”™è¯¯: {data}")
            monitoring_service.request_end(
                request_id,
                success=False,
                error=str(data),
                response_content="".join(collected_content) if collected_content else None
            )
            await monitoring_service.broadcast_to_monitors({
                "type": "request_end",
                "request_id": request_id,
                "success": False
            })
            yield format_openai_error_chunk(str(data), model, response_id)
            yield format_openai_finish_chunk(model, response_id, reason='stop')
            return # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œå¯ä»¥ç«‹å³ç»ˆæ­¢

    # åªæœ‰åœ¨ _process_lmarena_stream è‡ªç„¶ç»“æŸå (å³æ”¶åˆ° [DONE]) æ‰æ‰§è¡Œ
    yield format_openai_finish_chunk(model, response_id, reason=finish_reason_to_send)
    
    # è¯Šæ–­ï¼šè¾“å‡ºæµå¼è¾“å‡ºç»Ÿè®¡
    if CONFIG.get("debug_stream_timing", False) and chunks_sent > 0:
        total_time = time_module.time() - stream_start_time
        logger.info(f"[STREAM_OUTPUT_STATS] è¯·æ±‚ID: {request_id[:8]}")
        logger.info(f"  - å‘é€å—æ•°: {chunks_sent}")
        logger.info(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"  - å¹³å‡å‘é€é—´éš”: {total_time/chunks_sent:.3f}ç§’/å—")
    
    logger.info(f"STREAMER [ID: {request_id[:8]}]: æµå¼ç”Ÿæˆå™¨æ­£å¸¸ç»“æŸã€‚")

    # è®°å½•è¯·æ±‚æˆåŠŸï¼ˆåŒ…å«å“åº”å†…å®¹ï¼‰
    full_response = "".join(collected_content)
    full_reasoning = "".join(reasoning_content) if reasoning_content else None
    
    # ä»response_channelsè·å–è¯·æ±‚çš„åŸå§‹ä¿¡æ¯æ¥è®¡ç®—è¾“å…¥token
    input_tokens = 0
    if hasattr(monitoring_service, 'active_requests') and request_id in monitoring_service.active_requests:
        request_info = monitoring_service.active_requests[request_id]
        # è®¡ç®—è¾“å…¥tokenæ•°ï¼ˆç®€å•ä¼°ç®—ï¼šæ‰€æœ‰æ¶ˆæ¯å†…å®¹é•¿åº¦é™¤ä»¥4ï¼‰
        if request_info.request_messages:
            for msg in request_info.request_messages:
                if isinstance(msg, dict) and 'content' in msg:
                    content = msg.get('content', '')
                    if isinstance(content, str):
                        input_tokens += len(content) // 4
                    elif isinstance(content, list):
                        # å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯
                        for part in content:
                            if isinstance(part, dict) and part.get('type') == 'text':
                                input_tokens += len(part.get('text', '')) // 4
    
    monitoring_service.request_end(
        request_id,
        success=True,
        response_content=full_response,
        reasoning_content=full_reasoning,  # æ·»åŠ æ€ç»´é“¾å†…å®¹
        input_tokens=input_tokens,  # æ·»åŠ è¾“å…¥token
        output_tokens=len(full_response) // 4  # ç®€å•ä¼°ç®—è¾“å‡ºtokenæ•°
    )
    await monitoring_service.broadcast_to_monitors({
        "type": "request_end",
        "request_id": request_id,
        "success": True
    })
    
    # Log token usage to database
    try:
        if request_id in request_metadata:
            metadata = request_metadata[request_id]
            token_info = metadata.get('token_info')
            
            if token_info:
                logger.info(f"[TOKEN_LOG] Logging usage for request {request_id[:8]}")
                # Log the usage using pre-stored metadata
                await token_manager.log_usage(
                    token_id=token_info['id'],
                    ip_address=metadata.get('client_ip', 'Unknown'),
                    user_agent=metadata.get('user_agent', 'Unknown'),
                    model=metadata.get('model_name', 'unknown'),
                    endpoint='/v1/chat/completions',
                    success=True,
                    input_tokens=input_tokens,
                    output_tokens=len(full_response) // 4,
                    duration=None,  # Duration will be calculated from timestamp
                    country=metadata.get('country'),
                    city=metadata.get('city'),
                    platform=metadata.get('platform')
                )
                logger.info(f"[TOKEN_LOG] âœ… Token usage logged successfully for request {request_id[:8]}")
            else:
                logger.warning(f"[TOKEN_LOG] âš ï¸ No token_info in metadata for request {request_id[:8]}")
        else:
            logger.warning(f"[TOKEN_LOG] âš ï¸ Request {request_id[:8]} not found in request_metadata")
    except Exception as e:
        logger.error(f"[TOKEN_LOG] âŒ Failed to log token usage for request {request_id[:8]}: {e}", exc_info=True)
    finally:
        # æ¸…ç†è¯·æ±‚å…ƒæ•°æ®ï¼ˆåœ¨token loggingä¹‹åï¼‰
        if request_id in request_metadata:
            del request_metadata[request_id]
            logger.debug(f"STREAMER [ID: {request_id[:8]}]: è¯·æ±‚å…ƒæ•°æ®å·²æ¸…ç†ã€‚")
    

async def non_stream_response(request_id: str, model: str):
    """èšåˆå†…éƒ¨äº‹ä»¶æµå¹¶è¿”å›å•ä¸ª OpenAI JSON å“åº”ã€‚"""
    response_id = f"chatcmpl-{uuid.uuid4()}"
    logger.info(f"NON-STREAM [ID: {request_id[:8]}]: å¼€å§‹å¤„ç†éæµå¼å“åº”ã€‚")
    
    full_content = []
    reasoning_content = []
    finish_reason = "stop"
    
    # æ€ç»´é“¾é…ç½®
    # æ³¨æ„ï¼šæ€ç»´é“¾æ•°æ®æ€»æ˜¯è¢«æ”¶é›†ï¼Œä½†åªåœ¨å¯ç”¨æ—¶æ‰è¾“å‡ºç»™å®¢æˆ·ç«¯
    enable_reasoning_output = CONFIG.get("enable_lmarena_reasoning", False)
    reasoning_mode = CONFIG.get("reasoning_output_mode", "openai")
    
    async for event_type, data in _process_lmarena_stream(request_id):
        if event_type == 'retry_info':
            # éæµå¼å“åº”ä¸­è®°å½•é‡è¯•ä¿¡æ¯
            logger.info(f"NON-STREAM [ID: {request_id[:8]}]: é‡è¯•ä¿¡æ¯ - å°è¯• {data.get('attempt')}/{data.get('max_attempts')}")
            # éæµå¼å“åº”é€šå¸¸ä¸ä¼šå°†é‡è¯•ä¿¡æ¯è¿”å›ç»™å®¢æˆ·ç«¯
        elif event_type == 'reasoning' or event_type == 'reasoning_complete':
            # æ”¶é›†æ€ç»´é“¾å†…å®¹ï¼ˆæ€»æ˜¯æ”¶é›†ï¼Œç”¨äºç›‘æ§ï¼‰
            reasoning_content.append(data)
        elif event_type == 'content':
            full_content.append(data)
        elif event_type == 'finish':
            finish_reason = data
            if data == 'content-filter':
                full_content.append("\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥ï¼ˆå¤§æ¦‚ç‡ï¼‰çš„åŸå› ")
            # ä¸è¦åœ¨è¿™é‡Œ breakï¼Œç»§ç»­ç­‰å¾…æ¥è‡ªæµè§ˆå™¨çš„ [DONE] ä¿¡å·ï¼Œä»¥é¿å…ç«æ€æ¡ä»¶
        elif event_type == 'error':
            logger.error(f"NON-STREAM [ID: {request_id[:8]}]: å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {data}")
            
            monitoring_service.request_end(
                request_id,
                success=False,
                error=str(data),
                response_content="".join(full_content) if full_content else None
            )
            await monitoring_service.broadcast_to_monitors({
                "type": "request_end",
                "request_id": request_id,
                "success": False
            })
            
            # ç»Ÿä¸€æµå¼å’Œéæµå¼å“åº”çš„é”™è¯¯çŠ¶æ€ç 
            status_code = 413 if "é™„ä»¶å¤§å°è¶…è¿‡äº†" in str(data) else 500

            error_response = {
                "error": {
                    "message": f"[Luma API Error]: {data}",
                    "type": "api_error",
                    "code": "attachment_too_large" if status_code == 413 else "processing_error"
                }
            }
            return Response(content=json.dumps(error_response, ensure_ascii=False), status_code=status_code, media_type="application/json")

    # å¤„ç†æ€ç»´é“¾å†…å®¹
    # æ€ç»´é“¾æ€»æ˜¯è¢«æ”¶é›†ï¼ˆç”¨äºç›‘æ§ï¼‰ï¼Œä½†åªåœ¨å¯ç”¨æ—¶æ‰è¾“å‡ºç»™å®¢æˆ·ç«¯
    if enable_reasoning_output and reasoning_content:
        full_reasoning = "".join(reasoning_content)
        
        if reasoning_mode == "openai":
            # OpenAIæ¨¡å¼ï¼šæ·»åŠ reasoning_contentå­—æ®µ
            final_content_str = "".join(full_content)
            response_data = {
                "id": response_id,
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": final_content_str,
                        "reasoning_content": full_reasoning  # æ·»åŠ æ€ç»´é“¾
                    },
                    "finish_reason": finish_reason,
                }],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": len(final_content_str) // 4,
                    "total_tokens": len(final_content_str) // 4,
                },
            }
        elif reasoning_mode == "think_tag":
            # think_tagæ¨¡å¼ï¼šå°†æ€ç»´é“¾åŒ…è£¹åæ”¾åœ¨contentå‰é¢
            wrapped_reasoning = f"<think>{full_reasoning}</think>\n\n"
            final_content_str = wrapped_reasoning + "".join(full_content)
            response_data = format_openai_non_stream_response(final_content_str, model, response_id, reason=finish_reason)
    else:
        # æ²¡æœ‰å¯ç”¨æ€ç»´é“¾è¾“å‡ºï¼Œæˆ–è€…æ²¡æœ‰æ€ç»´é“¾å†…å®¹ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
        final_content_str = "".join(full_content)
        response_data = format_openai_non_stream_response(final_content_str, model, response_id, reason=finish_reason)
    
    logger.info(f"NON-STREAM [ID: {request_id[:8]}]: å“åº”èšåˆå®Œæˆã€‚")
    
    # è®°å½•è¯·æ±‚æˆåŠŸï¼ˆéæµå¼å“åº”ï¼‰
    # è®¡ç®—è¾“å…¥token
    input_tokens = 0
    if hasattr(monitoring_service, 'active_requests') and request_id in monitoring_service.active_requests:
        request_info = monitoring_service.active_requests[request_id]
        if request_info.request_messages:
            for msg in request_info.request_messages:
                if isinstance(msg, dict) and 'content' in msg:
                    content = msg.get('content', '')
                    if isinstance(content, str):
                        input_tokens += len(content) // 4
                    elif isinstance(content, list):
                        for part in content:
                            if isinstance(part, dict) and part.get('type') == 'text':
                                input_tokens += len(part.get('text', '')) // 4
    
    # è®¡ç®—å®Œæ•´å“åº”å†…å®¹ï¼ˆåŒ…æ‹¬æ€ç»´é“¾ï¼‰
    full_response_for_monitoring = final_content_str if 'final_content_str' in locals() else "".join(full_content)
    full_reasoning_for_monitoring = "".join(reasoning_content) if reasoning_content else None
    
    monitoring_service.request_end(
        request_id,
        success=True,
        response_content=full_response_for_monitoring,
        reasoning_content=full_reasoning_for_monitoring,  # æ·»åŠ æ€ç»´é“¾å†…å®¹
        input_tokens=input_tokens,
        output_tokens=len(full_response_for_monitoring) // 4
    )
    
    # Log token usage to database (non-streaming)
    try:
        if request_id in request_metadata:
            metadata = request_metadata[request_id]
            token_info = metadata.get('token_info')
            
            if token_info:
                logger.info(f"[TOKEN_LOG] Logging usage for non-stream request {request_id[:8]}")
                # Log the usage using pre-stored metadata
                await token_manager.log_usage(
                    token_id=token_info['id'],
                    ip_address=metadata.get('client_ip', 'Unknown'),
                    user_agent=metadata.get('user_agent', 'Unknown'),
                    model=metadata.get('model_name', 'unknown'),
                    endpoint='/v1/chat/completions',
                    success=True,
                    input_tokens=input_tokens,
                    output_tokens=len(full_response_for_monitoring) // 4,
                    duration=None,  # Duration will be calculated from timestamp
                    country=metadata.get('country'),
                    city=metadata.get('city'),
                    platform=metadata.get('platform')
                )
                logger.info(f"[TOKEN_LOG] âœ… Token usage logged successfully for non-stream request {request_id[:8]}")
            else:
                logger.warning(f"[TOKEN_LOG] âš ï¸ No token_info in metadata for non-stream request {request_id[:8]}")
        else:
            logger.warning(f"[TOKEN_LOG] âš ï¸ Non-stream request {request_id[:8]} not found in request_metadata")
    except Exception as e:
        logger.error(f"[TOKEN_LOG] âŒ Failed to log token usage for non-stream request {request_id[:8]}: {e}", exc_info=True)
    
    return Response(content=json.dumps(response_data, ensure_ascii=False), media_type="application/json")

# --- WebSocket ç«¯ç‚¹ ---
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """å¤„ç†æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„ WebSocket è¿æ¥ã€‚"""
    global browser_ws, IS_REFRESHING_FOR_VERIFICATION
    await websocket.accept()
    
    # ä½¿ç”¨é”ä¿æŠ¤WebSocketè¿æ¥çš„ä¿®æ”¹
    async with ws_lock:
        if browser_ws is not None:
            logger.warning("æ£€æµ‹åˆ°æ–°çš„æ²¹çŒ´è„šæœ¬è¿æ¥ï¼Œæ—§çš„è¿æ¥å°†è¢«æ›¿æ¢ã€‚")
            logger.info(f"[WS_CONN] æ›¿æ¢è¿æ¥ï¼Œå½“å‰response_channelsæ•°é‡: {len(response_channels)}")
        
        # åªè¦æœ‰æ–°çš„è¿æ¥å»ºç«‹ï¼Œå°±æ„å‘³ç€äººæœºéªŒè¯æµç¨‹å·²ç»“æŸï¼ˆæˆ–ä»æœªå¼€å§‹ï¼‰
        if IS_REFRESHING_FOR_VERIFICATION:
            logger.info("âœ… æ–°çš„ WebSocket è¿æ¥å·²å»ºç«‹ï¼ŒäººæœºéªŒè¯çŠ¶æ€å·²è‡ªåŠ¨é‡ç½®ã€‚")
            IS_REFRESHING_FOR_VERIFICATION = False
            
        logger.info("âœ… æ²¹çŒ´è„šæœ¬å·²æˆåŠŸè¿æ¥ WebSocketã€‚")
        logger.info(f"[WS_CONN] æ–°è¿æ¥å»ºç«‹ï¼Œå½“å‰response_channelsæ•°é‡: {len(response_channels)}")
        browser_ws = websocket
    
    # å¹¿æ’­æµè§ˆå™¨è¿æ¥çŠ¶æ€åˆ°ç›‘æ§é¢æ¿
    await monitoring_service.broadcast_to_monitors({
        "type": "browser_status",
        "connected": True
    })
    
    # --- å¢å¼ºï¼šå¤„ç†æ‰€æœ‰å¾…æ¢å¤çš„è¯·æ±‚ï¼ˆåŒ…æ‹¬pending_requests_queueå’Œresponse_channelsï¼‰---
    if CONFIG.get("enable_auto_retry", False):
        # 1. é¦–å…ˆå¤„ç†pending_requests_queueä¸­çš„è¯·æ±‚
        if not pending_requests_queue.empty():
            logger.info(f"æ£€æµ‹åˆ° {pending_requests_queue.qsize()} ä¸ªæš‚å­˜çš„è¯·æ±‚ï¼Œå°†åœ¨åå°è‡ªåŠ¨é‡è¯•...")
            asyncio.create_task(process_pending_requests())
        
        # 2. ç„¶åå¤„ç†response_channelsä¸­æœªå®Œæˆçš„è¯·æ±‚ï¼ˆå…³é”®ä¿®å¤ï¼‰
        if len(response_channels) > 0:
            logger.info(f"[REQUEST_RECOVERY] æ£€æµ‹åˆ° {len(response_channels)} ä¸ªæœªå®Œæˆçš„è¯·æ±‚ï¼Œå‡†å¤‡æ¢å¤...")
            
            # è·å–æ‰€æœ‰æœªå®Œæˆè¯·æ±‚çš„ID
            pending_request_ids = list(response_channels.keys())
            
            for request_id in pending_request_ids:
                # å°è¯•ä»å¤šä¸ªæ¥æºè·å–è¯·æ±‚æ•°æ®
                request_data = None
                
                # æ¥æº1ï¼šrequest_metadataï¼ˆæ–°å¢çš„å­˜å‚¨ï¼‰
                if request_id in request_metadata:
                    request_data = request_metadata[request_id]["openai_request"]
                    logger.info(f"[REQUEST_RECOVERY] ä»request_metadataæ¢å¤è¯·æ±‚ {request_id[:8]}")
                
                # æ¥æº2ï¼šmonitoring_service.active_requestsï¼ˆå¤‡ç”¨ï¼‰
                elif hasattr(monitoring_service, 'active_requests') and request_id in monitoring_service.active_requests:
                    active_req = monitoring_service.active_requests[request_id]
                    # é‡å»ºOpenAIè¯·æ±‚æ ¼å¼
                    request_data = {
                        "model": active_req.model,
                        "messages": active_req.request_messages if hasattr(active_req, 'request_messages') else [],
                        "stream": active_req.params.get("streaming", False) if hasattr(active_req, 'params') else False,
                        "temperature": active_req.params.get("temperature") if hasattr(active_req, 'params') else None,
                        "top_p": active_req.params.get("top_p") if hasattr(active_req, 'params') else None,
                        "max_tokens": active_req.params.get("max_tokens") if hasattr(active_req, 'params') else None,
                    }
                    logger.info(f"[REQUEST_RECOVERY] ä»monitoring_serviceæ¢å¤è¯·æ±‚ {request_id[:8]}")
                else:
                    logger.warning(f"[REQUEST_RECOVERY] âš ï¸ æ— æ³•æ¢å¤è¯·æ±‚ {request_id[:8]}ï¼šæ‰¾ä¸åˆ°åŸå§‹æ•°æ®")
                    # æ¸…ç†è¿™ä¸ªæ— æ³•æ¢å¤çš„è¯·æ±‚
                    if request_id in response_channels:
                        await response_channels[request_id].put({"error": "Request data lost during reconnection"})
                        await response_channels[request_id].put("[DONE]")
                    continue
                
                # å¦‚æœæˆåŠŸè·å–åˆ°è¯·æ±‚æ•°æ®ï¼Œå°†å…¶åŠ å…¥é‡è¯•é˜Ÿåˆ—
                if request_data:
                    # åˆ›å»ºä¸€ä¸ªæ–°çš„futureæ¥ç­‰å¾…é‡è¯•ç»“æœ
                    future = asyncio.get_event_loop().create_future()
                    
                    # å°†è¯·æ±‚æ”¾å…¥pendingé˜Ÿåˆ—
                    await pending_requests_queue.put({
                        "future": future,
                        "request_data": request_data,
                        "original_request_id": request_id  # ä¿ç•™åŸå§‹è¯·æ±‚IDç”¨äºè¿½è¸ª
                    })
                    
                    logger.info(f"[REQUEST_RECOVERY] âœ… è¯·æ±‚ {request_id[:8]} å·²åŠ å…¥é‡è¯•é˜Ÿåˆ—")
            
            # å¯åŠ¨æ¢å¤å¤„ç†
            if not pending_requests_queue.empty():
                logger.info(f"[REQUEST_RECOVERY] å¼€å§‹å¤„ç† {pending_requests_queue.qsize()} ä¸ªæ¢å¤çš„è¯·æ±‚...")
                asyncio.create_task(process_pending_requests())
            else:
                logger.info(f"[REQUEST_RECOVERY] æ²¡æœ‰å¯æ¢å¤çš„è¯·æ±‚")

    try:
        while True:
            # ç­‰å¾…å¹¶æ¥æ”¶æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„æ¶ˆæ¯
            message_str = await websocket.receive_text()
            message = json.loads(message_str)
            
            request_id = message.get("request_id")
            data = message.get("data")

            if not request_id or data is None:
                logger.warning(f"æ”¶åˆ°æ¥è‡ªæµè§ˆå™¨çš„æ— æ•ˆæ¶ˆæ¯: {message}")
                continue

            # è¯Šæ–­ï¼šè®°å½•WebSocketæ¶ˆæ¯
            if CONFIG.get("debug_stream_timing", False):
                import time as time_module
                current_time = time_module.time()
                data_preview = str(data)[:200] if data else "None"
                logger.debug(f"[WS_MSG] æ—¶é—´: {current_time:.3f}, è¯·æ±‚ID: {request_id[:8]}, æ•°æ®é¢„è§ˆ: {data_preview}...")
                
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ•°æ®ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªæ–‡æœ¬å—
                if isinstance(data, str) and 'a0:"' in data:
                    import re
                    text_pattern = re.compile(r'[ab]0:"((?:\\.|[^"\\])*)"')
                    matches = text_pattern.findall(data)
                    logger.debug(f"[WS_MSG] å•ä¸ªWebSocketæ¶ˆæ¯ä¸­åŒ…å« {len(matches)} ä¸ªæ–‡æœ¬å—ï¼ˆé—®é¢˜ï¼šæ•°æ®è¢«ç´¯ç§¯ï¼ï¼‰")
                    if len(matches) > 1:
                        logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æµå¼æ•°æ®è¢«ç´¯ç§¯ï¼å•ä¸ªWebSocketæ¶ˆæ¯åŒ…å«äº† {len(matches)} ä¸ªæ–‡æœ¬å—")
                        logger.warning(f"   è¿™è¯´æ˜æ²¹çŒ´è„šæœ¬ç«¯ç´¯ç§¯äº†å¤šä¸ªå“åº”å—åæ‰å‘é€")

            # å°†æ”¶åˆ°çš„æ•°æ®æ”¾å…¥å¯¹åº”çš„å“åº”é€šé“
            if request_id in response_channels:
                await response_channels[request_id].put(data)
            else:
                logger.warning(f"âš ï¸ æ”¶åˆ°æœªçŸ¥æˆ–å·²å…³é—­è¯·æ±‚çš„å“åº”: {request_id}")

    except WebSocketDisconnect:
        logger.warning("âŒ æ²¹çŒ´è„šæœ¬å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥ã€‚")
    except Exception as e:
        logger.error(f"WebSocket å¤„ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
    finally:
        async with ws_lock:
            browser_ws = None
            logger.info(f"[WS_CONN] è¿æ¥æ–­å¼€ï¼Œæœªå¤„ç†è¯·æ±‚æ•°: {len(response_channels)}")
            
        # å¹¿æ’­æµè§ˆå™¨æ–­å¼€çŠ¶æ€åˆ°ç›‘æ§é¢æ¿
        await monitoring_service.broadcast_to_monitors({
            "type": "browser_status",
            "connected": False
        })
        
        # å¦‚æœç¦ç”¨äº†è‡ªåŠ¨é‡è¯•ï¼Œåˆ™åƒä»¥å‰ä¸€æ ·æ¸…ç†é€šé“
        if not CONFIG.get("enable_auto_retry", False):
            # æ¸…ç†æ‰€æœ‰ç­‰å¾…çš„å“åº”é€šé“ï¼Œä»¥é˜²è¯·æ±‚è¢«æŒ‚èµ·
            for queue in response_channels.values():
                await queue.put({"error": "Browser disconnected during operation"})
            response_channels.clear()
            logger.info("WebSocket è¿æ¥å·²æ¸…ç†ï¼ˆè‡ªåŠ¨é‡è¯•å·²ç¦ç”¨ï¼‰ã€‚")
        else:
            logger.info("WebSocket è¿æ¥å·²å…³é—­ï¼ˆè‡ªåŠ¨é‡è¯•å·²å¯ç”¨ï¼Œè¯·æ±‚å°†ç­‰å¾…é‡è¿ï¼‰ã€‚")

# --- OpenAI å…¼å®¹ API ç«¯ç‚¹ ---
@app.get("/v1/models")
async def get_models():
    """æä¾›å…¼å®¹ OpenAI çš„æ¨¡å‹åˆ—è¡¨ - è¿”å› model_endpoint_map.json ä¸­é…ç½®çš„æ¨¡å‹ã€‚"""
    # ä¼˜å…ˆè¿”å› MODEL_ENDPOINT_MAP ä¸­çš„æ¨¡å‹ï¼ˆå·²é…ç½®ä¼šè¯çš„æ¨¡å‹ï¼‰
    if MODEL_ENDPOINT_MAP:
        return {
            "object": "list",
            "data": [
                {
                    "id": model_name,
                    "object": "model",
                    "created": int(time.time()),
                    "owned_by": "LMArenaBridge"
                }
                for model_name in MODEL_ENDPOINT_MAP.keys()
            ],
        }
    # å¦‚æœ MODEL_ENDPOINT_MAP ä¸ºç©ºï¼Œåˆ™è¿”å› models.json ä¸­çš„æ¨¡å‹ä½œä¸ºå¤‡ç”¨
    elif MODEL_NAME_TO_ID_MAP:
        return {
            "object": "list",
            "data": [
                {
                    "id": model_name,
                    "object": "model",
                    "created": int(time.time()),
                    "owned_by": "LMArenaBridge"
                }
                for model_name in MODEL_NAME_TO_ID_MAP.keys()
            ],
        }
    else:
        return JSONResponse(
            status_code=404,
            content={"error": "æ¨¡å‹åˆ—è¡¨ä¸ºç©ºã€‚è¯·é…ç½® 'model_endpoint_map.json' æˆ– 'models.json'ã€‚"}
        )

@app.post("/internal/request_model_update")
async def request_model_update():
    """
    æ¥æ”¶æ¥è‡ª model_updater.py çš„è¯·æ±‚ï¼Œå¹¶é€šè¿‡ WebSocket æŒ‡ä»¤
    è®©æ²¹çŒ´è„šæœ¬å‘é€é¡µé¢æºç ã€‚
    """
    if not browser_ws:
        logger.warning("MODEL UPDATE: æ”¶åˆ°æ›´æ–°è¯·æ±‚ï¼Œä½†æ²¡æœ‰æµè§ˆå™¨è¿æ¥ã€‚")
        raise HTTPException(status_code=503, detail="Browser client not connected.")
    
    try:
        logger.info("MODEL UPDATE: æ”¶åˆ°æ›´æ–°è¯·æ±‚ï¼Œæ­£åœ¨é€šè¿‡ WebSocket å‘é€æŒ‡ä»¤...")
        await browser_ws.send_text(json.dumps({"command": "send_page_source"}))
        logger.info("MODEL UPDATE: 'send_page_source' æŒ‡ä»¤å·²æˆåŠŸå‘é€ã€‚")
        return JSONResponse({"status": "success", "message": "Request to send page source sent."})
    except Exception as e:
        logger.error(f"MODEL UPDATE: å‘é€æŒ‡ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to send command via WebSocket.")

@app.post("/internal/update_available_models")
async def update_available_models_endpoint(request: Request):
    """
    æ¥æ”¶æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢ HTMLï¼Œæå–å¹¶æ›´æ–° available_models.jsonã€‚
    """
    html_content = await request.body()
    if not html_content:
        logger.warning("æ¨¡å‹æ›´æ–°è¯·æ±‚æœªæ”¶åˆ°ä»»ä½• HTML å†…å®¹ã€‚")
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "No HTML content received."}
        )
    
    logger.info("æ”¶åˆ°æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢å†…å®¹ï¼Œå¼€å§‹æå–å¯ç”¨æ¨¡å‹...")
    new_models_list = extract_models_from_html(html_content.decode('utf-8'))
    
    if new_models_list:
        save_available_models(new_models_list)
        return JSONResponse({"status": "success", "message": "Available models file updated."})
    else:
        logger.error("æœªèƒ½ä»æ²¹çŒ´è„šæœ¬æä¾›çš„ HTML ä¸­æå–æ¨¡å‹æ•°æ®ã€‚")
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "Could not extract model data from HTML."}
        )


@app.get("/", response_class=HTMLResponse)
async def root():
    """è¿”å›ç”¨æˆ·ä»ªè¡¨æ¿HTMLé¡µé¢ï¼ˆæ ¹è·¯å¾„ï¼‰"""
    try:
        with open('templates/user_dashboard.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        return HTMLResponse(content=html_content)
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>User Dashboard Not Found</h1><p>Please ensure templates/user_dashboard.html exists.</p>",
            status_code=404
        )

@app.get("/user-dashboard", response_class=HTMLResponse)
async def user_dashboard():
    """è¿”å›ç”¨æˆ·ä»ªè¡¨æ¿HTMLé¡µé¢ï¼ˆå¤‡ç”¨è·¯å¾„ï¼‰"""
    try:
        with open('templates/user_dashboard.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        return HTMLResponse(content=html_content)
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>ç”¨æˆ·ä»ªè¡¨æ¿æ–‡ä»¶æœªæ‰¾åˆ°</h1><p>è¯·ç¡®ä¿ templates/user_dashboard.html æ–‡ä»¶å­˜åœ¨ã€‚</p>",
            status_code=404
        )

@app.get("/api/user/token-stats")
async def get_user_token_stats(token: str):
    """è·å–ç”¨æˆ·tokençš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ— éœ€è®¤è¯ï¼Œé€šè¿‡tokenæœ¬èº«éªŒè¯ï¼‰"""
    try:
        # éªŒè¯token
        token_info = await token_manager.get_token_by_value(token)
        if not token_info:
            raise HTTPException(status_code=404, detail="Token not found")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = await token_manager.get_token_stats(token_info['id'])
        recent_usage = await token_manager.get_recent_usage(token_info['id'], limit=50)
        
        return {
            "token_info": token_info,
            "stats": stats,
            "recent_usage": recent_usage
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting user token stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """
    å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚ã€‚
    æ¥æ”¶ OpenAI æ ¼å¼çš„è¯·æ±‚,å°†å…¶è½¬æ¢ä¸º LMArena æ ¼å¼,
    é€šè¿‡ WebSocket å‘é€ç»™æ²¹çŒ´è„šæœ¬,ç„¶åæµå¼è¿”å›ç»“æœã€‚
    """
    global last_activity_time
    last_activity_time = datetime.now() # æ›´æ–°æ´»åŠ¨æ—¶é—´
    logger.info(f"APIè¯·æ±‚å·²æ”¶åˆ°,æ´»åŠ¨æ—¶é—´å·²æ›´æ–°ä¸º: {last_activity_time.strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        openai_req = await request.json()
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ JSON è¯·æ±‚ä½“")
    
    # --- Token Authentication ---
    auth_header = request.headers.get('Authorization')
    if not auth_header or not auth_header.startswith('Bearer '):
        raise HTTPException(
            status_code=401,
            detail="æœªæä¾› Tokenã€‚è¯·åœ¨ Authorization å¤´éƒ¨ä¸­ä»¥ 'Bearer YOUR_TOKEN' æ ¼å¼æä¾›ã€‚"
        )
    
    provided_token = auth_header.split(' ')[1]
    
    # éªŒè¯token
    token_info = await token_manager.validate_token(provided_token)
    if not token_info:
        raise HTTPException(
            status_code=401,
            detail="æä¾›çš„ Token æ— æ•ˆæˆ–å·²è¢«ç¦ç”¨ã€‚"
        )
    
    # è·å–è¯·æ±‚çš„IPå’ŒUser-Agent
    client_ip = request.client.host if request.client else "Unknown"
    user_agent = request.headers.get('User-Agent', 'Unknown')
    
    # è·å–åœ°ç†ä½ç½®å’Œå¹³å°ä¿¡æ¯
    country, city = await geo_platform_service.get_location(client_ip)
    platform = geo_platform_service.detect_platform(user_agent)
    
    logger.info(f"TokenéªŒè¯æˆåŠŸ: {token_info['user_id']} | IP: {client_ip} | å¹³å°: {platform}")

    model_name = openai_req.get("model")
    
    # ä¼˜å…ˆä» MODEL_ENDPOINT_MAP è·å–æ¨¡å‹ç±»å‹
    model_type = "text"  # é»˜è®¤ç±»å‹
    endpoint_mapping = MODEL_ENDPOINT_MAP.get(model_name)
    if endpoint_mapping:
        if isinstance(endpoint_mapping, dict) and "type" in endpoint_mapping:
            model_type = endpoint_mapping.get("type", "text")
        elif isinstance(endpoint_mapping, list) and endpoint_mapping:
            first_mapping = endpoint_mapping[0] if isinstance(endpoint_mapping[0], dict) else {}
            if "type" in first_mapping:
                model_type = first_mapping.get("type", "text")
    
    # å›é€€åˆ° models.json
    model_info = MODEL_NAME_TO_ID_MAP.get(model_name, {})
    if not (endpoint_mapping and (isinstance(endpoint_mapping, dict) and "type" in endpoint_mapping or
            isinstance(endpoint_mapping, list) and endpoint_mapping and "type" in endpoint_mapping[0])):
        model_type = model_info.get("type", "text")

    # --- æ–°å¢ï¼šåŸºäºæ¨¡å‹ç±»å‹çš„åˆ¤æ–­é€»è¾‘ ---
    if model_type == 'image':
        logger.info(f"æ£€æµ‹åˆ°æ¨¡å‹ '{model_name}' ç±»å‹ä¸º 'image'ï¼Œå°†é€šè¿‡ä¸»èŠå¤©æ¥å£å¤„ç†ã€‚")
        # å¯¹äºå›¾åƒæ¨¡å‹ï¼Œæˆ‘ä»¬ä¸å†è°ƒç”¨ç‹¬ç«‹çš„å¤„ç†å™¨ï¼Œè€Œæ˜¯å¤ç”¨ä¸»èŠå¤©é€»è¾‘ï¼Œ
        # å› ä¸º _process_lmarena_stream ç°åœ¨å·²ç»èƒ½å¤„ç†å›¾ç‰‡æ•°æ®ã€‚
        # è¿™æ„å‘³ç€å›¾åƒç”Ÿæˆç°åœ¨åŸç”Ÿæ”¯æŒæµå¼å’Œéæµå¼å“åº”ã€‚
        pass # ç»§ç»­æ‰§è¡Œä¸‹é¢çš„é€šç”¨èŠå¤©é€»è¾‘
    # --- æ–‡ç”Ÿå›¾é€»è¾‘ç»“æŸ ---

    # å¦‚æœä¸æ˜¯å›¾åƒæ¨¡å‹ï¼Œåˆ™æ‰§è¡Œæ­£å¸¸çš„æ–‡æœ¬ç”Ÿæˆé€»è¾‘
    load_config()  # å®æ—¶åŠ è½½æœ€æ–°é…ç½®ï¼Œç¡®ä¿ä¼šè¯IDç­‰ä¿¡æ¯æ˜¯æœ€æ–°çš„
    # --- API Key éªŒè¯ ---
    api_key = CONFIG.get("api_key")
    if api_key:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            raise HTTPException(
                status_code=401,
                detail="æœªæä¾› API Keyã€‚è¯·åœ¨ Authorization å¤´éƒ¨ä¸­ä»¥ 'Bearer YOUR_KEY' æ ¼å¼æä¾›ã€‚"
            )
        
        provided_key = auth_header.split(' ')[1]
        if provided_key != api_key:
            raise HTTPException(
                status_code=401,
                detail="æä¾›çš„ API Key ä¸æ­£ç¡®ã€‚"
            )

    # --- å¢å¼ºçš„è¿æ¥æ£€æŸ¥ä¸è‡ªåŠ¨é‡è¯•é€»è¾‘ ---
    if not browser_ws:
        if CONFIG.get("enable_auto_retry", False):
            logger.warning("æ²¹çŒ´è„šæœ¬æœªè¿æ¥ï¼Œä½†è‡ªåŠ¨é‡è¯•å·²å¯ç”¨ã€‚è¯·æ±‚å°†è¢«æš‚å­˜ã€‚")
            
            # åˆ›å»ºä¸€ä¸ª future æ¥ç­‰å¾…å“åº”
            future = asyncio.get_event_loop().create_future()
            
            # å°†è¯·æ±‚çš„å…³é”®ä¿¡æ¯æ”¾å…¥æš‚å­˜é˜Ÿåˆ—
            await pending_requests_queue.put({
                "future": future,
                "request_data": openai_req
            })
            
            logger.info(f"ä¸€ä¸ªæ–°è¯·æ±‚å·²è¢«æ”¾å…¥æš‚å­˜é˜Ÿåˆ—ã€‚å½“å‰é˜Ÿåˆ—å¤§å°: {pending_requests_queue.qsize()}")

            try:
                # ä»é…ç½®ä¸­è¯»å–è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸º60ç§’
                timeout = CONFIG.get("retry_timeout_seconds", 120)
                
                # ç­‰å¾… future å®Œæˆï¼ˆå³ï¼Œé‡è¯•æˆåŠŸåï¼Œå“åº”è¢«è®¾ç½®è¿›æ¥ï¼‰
                return await asyncio.wait_for(future, timeout=timeout)
            except asyncio.TimeoutError:
                logger.warning(f"ä¸€ä¸ªæš‚å­˜çš„è¯·æ±‚ç­‰å¾…äº† {timeout} ç§’åè¶…æ—¶ã€‚")
                raise HTTPException(
                    status_code=503,
                    detail=f"æµè§ˆå™¨ä¸æœåŠ¡å™¨è¿æ¥æ–­å¼€ï¼Œå¹¶åœ¨ {timeout} ç§’å†…æœªèƒ½æ¢å¤ã€‚è¯·æ±‚å¤±è´¥ã€‚"
                )

        else: # å¦‚æœæœªå¯ç”¨é‡è¯•ï¼Œåˆ™ç«‹å³å¤±è´¥
            raise HTTPException(
                status_code=503,
                detail="æ²¹çŒ´è„šæœ¬å®¢æˆ·ç«¯æœªè¿æ¥ã€‚è¯·ç¡®ä¿ LMArena é¡µé¢å·²æ‰“å¼€å¹¶æ¿€æ´»è„šæœ¬ã€‚"
            )

    if IS_REFRESHING_FOR_VERIFICATION and not browser_ws:
        raise HTTPException(
            status_code=503,
            detail="æ­£åœ¨ç­‰å¾…æµè§ˆå™¨åˆ·æ–°ä»¥å®ŒæˆäººæœºéªŒè¯ï¼Œè¯·åœ¨å‡ ç§’é’Ÿåé‡è¯•ã€‚"
        )

    # --- æ¨¡å‹ä¸ä¼šè¯IDæ˜ å°„é€»è¾‘ ---
    session_id, message_id = None, None
    mode_override, battle_target_override = None, None

    if model_name and model_name in MODEL_ENDPOINT_MAP:
        mapping_entry = MODEL_ENDPOINT_MAP[model_name]
        selected_mapping = None

        if isinstance(mapping_entry, list) and mapping_entry:
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„è½®è¯¢ç­–ç•¥é€‰æ‹©æ˜ å°„
            global MODEL_ROUND_ROBIN_INDEX, MODEL_ROUND_ROBIN_LOCK
            
            # å…³é”®ä¿®å¤ï¼šä½¿ç”¨é”ç¡®ä¿åŸå­æ“ä½œï¼Œé¿å…å¹¶å‘ç«æ€
            with MODEL_ROUND_ROBIN_LOCK:
                if model_name not in MODEL_ROUND_ROBIN_INDEX:
                    MODEL_ROUND_ROBIN_INDEX[model_name] = 0
                
                current_index = MODEL_ROUND_ROBIN_INDEX[model_name]
                selected_mapping = mapping_entry[current_index]
                
                # è¯Šæ–­æ—¥å¿—ï¼šæ˜¾ç¤ºå¹¶å‘è½®è¯¢çŠ¶æ€
                logger.info(f"[CONCURRENT_ROUND_ROBIN] æ¨¡å‹ '{model_name}' è½®è¯¢çŠ¶æ€:")
                logger.info(f"  - æ€»æ˜ å°„æ•°: {len(mapping_entry)}")
                logger.info(f"  - å½“å‰é€‰æ‹©ç´¢å¼•: {current_index}")
                logger.info(f"  - é€‰æ‹©çš„æ˜ å°„: #{current_index + 1}/{len(mapping_entry)}")
                logger.info(f"  - Session IDå6ä½: ...{mapping_entry[current_index].get('session_id', 'N/A')[-6:]}")
                logger.info(f"  - æ´»è·ƒè¯·æ±‚é€šé“æ•°: {len(response_channels)} (å¹¶å‘è¯·æ±‚)")
                
                # æ›´æ–°ç´¢å¼•ï¼Œå¾ªç¯åˆ°ä¸‹ä¸€ä¸ªï¼ˆåŸå­æ“ä½œå†…å®Œæˆï¼‰
                MODEL_ROUND_ROBIN_INDEX[model_name] = (current_index + 1) % len(mapping_entry)
                next_index = MODEL_ROUND_ROBIN_INDEX[model_name]
                logger.info(f"  - ä¸‹æ¬¡è¯·æ±‚å°†ä½¿ç”¨ç´¢å¼•: {next_index}")
            
            logger.info(f"âœ… ä¸ºæ¨¡å‹ '{model_name}' ä»IDåˆ—è¡¨ä¸­è½®è¯¢é€‰æ‹©äº†æ˜ å°„ #{current_index + 1}/{len(mapping_entry)}ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰")
        elif isinstance(mapping_entry, dict):
            selected_mapping = mapping_entry
            logger.info(f"ä¸ºæ¨¡å‹ '{model_name}' æ‰¾åˆ°äº†å•ä¸ªç«¯ç‚¹æ˜ å°„ï¼ˆæ—§æ ¼å¼ï¼‰ã€‚")
        
        if selected_mapping:
            session_id = selected_mapping.get("session_id")
            message_id = selected_mapping.get("message_id")
            # å…³é”®ï¼šåŒæ—¶è·å–æ¨¡å¼ä¿¡æ¯
            mode_override = selected_mapping.get("mode") # å¯èƒ½ä¸º None
            battle_target_override = selected_mapping.get("battle_target") # å¯èƒ½ä¸º None
            log_msg = f"å°†ä½¿ç”¨ Session ID: ...{session_id[-6:] if session_id else 'N/A'}"
            if mode_override:
                log_msg += f" (æ¨¡å¼: {mode_override}"
                if mode_override == 'battle':
                    log_msg += f", ç›®æ ‡: {battle_target_override or 'A'}"
                log_msg += ")"
            logger.info(log_msg)

    # å¦‚æœç»è¿‡ä»¥ä¸Šå¤„ç†ï¼Œsession_id ä»ç„¶æ˜¯ Noneï¼Œåˆ™è¿›å…¥å…¨å±€å›é€€é€»è¾‘
    if not session_id:
        if CONFIG.get("use_default_ids_if_mapping_not_found", True):
            session_id = CONFIG.get("session_id")
            message_id = CONFIG.get("message_id")
            # å½“ä½¿ç”¨å…¨å±€IDæ—¶ï¼Œä¸è®¾ç½®æ¨¡å¼è¦†ç›–ï¼Œè®©å…¶ä½¿ç”¨å…¨å±€é…ç½®
            mode_override, battle_target_override = None, None
            logger.info(f"æ¨¡å‹ '{model_name}' æœªæ‰¾åˆ°æœ‰æ•ˆæ˜ å°„ï¼Œæ ¹æ®é…ç½®ä½¿ç”¨å…¨å±€é»˜è®¤ Session ID: ...{session_id[-6:] if session_id else 'N/A'}")
        else:
            logger.error(f"æ¨¡å‹ '{model_name}' æœªåœ¨ 'model_endpoint_map.json' ä¸­æ‰¾åˆ°æœ‰æ•ˆæ˜ å°„ï¼Œä¸”å·²ç¦ç”¨å›é€€åˆ°é»˜è®¤IDã€‚")
            raise HTTPException(
                status_code=400,
                detail=f"æ¨¡å‹ '{model_name}' æ²¡æœ‰é…ç½®ç‹¬ç«‹çš„ä¼šè¯IDã€‚è¯·åœ¨ 'model_endpoint_map.json' ä¸­æ·»åŠ æœ‰æ•ˆæ˜ å°„æˆ–åœ¨ 'config.jsonc' ä¸­å¯ç”¨ 'use_default_ids_if_mapping_not_found'ã€‚"
            )

    # --- éªŒè¯æœ€ç»ˆç¡®å®šçš„ä¼šè¯ä¿¡æ¯ ---
    if not session_id or not message_id or "YOUR_" in session_id or "YOUR_" in message_id:
        raise HTTPException(
            status_code=400,
            detail="æœ€ç»ˆç¡®å®šçš„ä¼šè¯IDæˆ–æ¶ˆæ¯IDæ— æ•ˆã€‚è¯·æ£€æŸ¥ 'model_endpoint_map.json' å’Œ 'config.jsonc' ä¸­çš„é…ç½®ï¼Œæˆ–è¿è¡Œ `id_updater.py` æ¥æ›´æ–°é»˜è®¤å€¼ã€‚"
        )

    if not model_name or model_name not in MODEL_NAME_TO_ID_MAP:
        logger.warning(f"è¯·æ±‚çš„æ¨¡å‹ '{model_name}' ä¸åœ¨ models.json ä¸­ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹IDã€‚")

    request_id = str(uuid.uuid4())
    response_channels[request_id] = asyncio.Queue()
    
    # æ–°å¢ï¼šä¿å­˜è¯·æ±‚å…ƒæ•°æ®ç”¨äºæ–­çº¿æ¢å¤
    request_metadata[request_id] = {
        "openai_request": openai_req.copy(),  # ä¿å­˜å®Œæ•´çš„OpenAIè¯·æ±‚
        "model_name": model_name,
        "session_id": session_id,
        "message_id": message_id,
        "mode_override": mode_override,
        "battle_target_override": battle_target_override,
        "created_at": datetime.now().isoformat(),
        "token_info": token_info,  # ä¿å­˜tokenä¿¡æ¯ç”¨äºæ—¥å¿—è®°å½•
        "client_ip": client_ip,
        "user_agent": user_agent,
        "country": country,
        "city": city,
        "platform": platform
    }
    
    logger.info(f"API CALL [ID: {request_id[:8]}]: å·²åˆ›å»ºå“åº”é€šé“ã€‚")
    logger.debug(f"API CALL [ID: {request_id[:8]}]: è¯·æ±‚å…ƒæ•°æ®å·²ä¿å­˜åˆ°request_metadata")
    
    # è®°å½•è¯·æ±‚å¼€å§‹åˆ°ç›‘æ§ç³»ç»Ÿï¼ˆå¢åŠ è¯¦ç»†ä¿¡æ¯ï¼‰
    # è®¡ç®—è¾“å…¥çš„å¤§æ¦‚tokenæ•°
    input_token_estimate = 0
    for msg in openai_req.get("messages", []):
        content = msg.get("content", "")
        if isinstance(content, str):
            input_token_estimate += len(content) // 4
        elif isinstance(content, list):
            for part in content:
                if isinstance(part, dict) and part.get("type") == "text":
                    input_token_estimate += len(part.get("text", "")) // 4
    
    monitoring_service.request_start(
        request_id=request_id,
        model=model_name or "unknown",
        messages_count=len(openai_req.get("messages", [])),
        session_id=session_id[-6:] if session_id else None,
        mode=mode_override or CONFIG.get("id_updater_last_mode", "direct_chat"),
        messages=openai_req.get("messages", []),  # æ·»åŠ æ¶ˆæ¯å†…å®¹
        params={  # æ·»åŠ è¯·æ±‚å‚æ•°
            "temperature": openai_req.get("temperature"),
            "top_p": openai_req.get("top_p"),
            "max_tokens": openai_req.get("max_tokens"),
            "streaming": openai_req.get("stream", False)
        }
    )
    
    # å¹¿æ’­è¯·æ±‚å¼€å§‹äº‹ä»¶åˆ°ç›‘æ§é¢æ¿
    await monitoring_service.broadcast_to_monitors({
        "type": "request_start",
        "request_id": request_id,
        "model": model_name,
        "timestamp": time.time()
    })

    # --- æ–°å¢ï¼šå›¾ç‰‡hashè®¡ç®—è¾…åŠ©å‡½æ•° ---
    def calculate_image_hash(base64_data: str) -> str:
        """è®¡ç®—å›¾ç‰‡å†…å®¹çš„SHA256 hashï¼ˆç”¨äºç¼“å­˜é”®ï¼‰"""
        import hashlib
        # ç§»é™¤data URIå‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if ',' in base64_data:
            _, data_only = base64_data.split(',', 1)
        else:
            data_only = base64_data
        # è®¡ç®—hashï¼ˆä½¿ç”¨base64å­—ç¬¦ä¸²ï¼Œé¿å…è§£ç å¼€é”€ï¼‰
        return hashlib.sha256(data_only.encode('utf-8')).hexdigest()
    
    try:
        # --- é™„ä»¶é¢„å¤„ç†ï¼ˆåŒ…æ‹¬æ–‡ä»¶åºŠä¸Šä¼ ï¼‰ ---
        # åœ¨ä¸æµè§ˆå™¨é€šä¿¡å‰ï¼Œå…ˆå¤„ç†å¥½æ‰€æœ‰é™„ä»¶ã€‚å¦‚æœå¤±è´¥ï¼Œåˆ™ç«‹å³è¿”å›é”™è¯¯ã€‚
        # å¤„ç†è¯·æ±‚ä¸­æ‰€æœ‰è§’è‰²ï¼ˆuserã€assistantã€systemï¼‰çš„base64å›¾ç‰‡ -> è½¬æ¢ä¸ºå›¾åºŠé“¾æ¥å‘é€åˆ°LMArena
        if CONFIG.get("file_bed_enabled"):
            # 1. è¿‡æ»¤å‡ºå·²å¯ç”¨ä¸”æœªè¢«ä¸´æ—¶ç¦ç”¨çš„ç«¯ç‚¹ï¼ˆæ£€æŸ¥æ¢å¤æ—¶é—´ï¼‰
            all_endpoints = CONFIG.get("file_bed_endpoints", [])
            current_time = time.time()
            
            # è‡ªåŠ¨æ¢å¤è¶…æ—¶çš„ç«¯ç‚¹
            endpoints_to_recover = []
            for endpoint_name, disable_time in list(DISABLED_ENDPOINTS.items()):
                if current_time - disable_time > FILEBED_RECOVERY_TIME:
                    endpoints_to_recover.append(endpoint_name)
            
            for endpoint_name in endpoints_to_recover:
                del DISABLED_ENDPOINTS[endpoint_name]
                logger.info(f"[FILEBED] å›¾åºŠç«¯ç‚¹ '{endpoint_name}' å·²è‡ªåŠ¨æ¢å¤")
            
            active_endpoints = [ep for ep in all_endpoints if ep.get("enabled") and ep.get("name") not in DISABLED_ENDPOINTS]

            if not active_endpoints:
                logger.warning("æ–‡ä»¶åºŠå·²å¯ç”¨ï¼Œä½†æ²¡æœ‰å¯ç”¨çš„æ´»åŠ¨ç«¯ç‚¹ã€‚å°†è·³è¿‡ä¸Šä¼ ã€‚")
            else:
                # --- æ–°å¢ï¼šå›¾åºŠé€‰æ‹©ç­–ç•¥ ---
                global ROUND_ROBIN_INDEX
                strategy = CONFIG.get("file_bed_selection_strategy", "random")
                
                messages_to_process = openai_req.get("messages", [])
                logger.info(f"ğŸ“‹ æ–‡ä»¶åºŠå·²å¯ç”¨ï¼Œæ‰¾åˆ° {len(active_endpoints)} ä¸ªæ´»åŠ¨ç«¯ç‚¹ (ç­–ç•¥: {strategy})")
                logger.info(f"ğŸ“‹ å‡†å¤‡å¤„ç† {len(messages_to_process)} æ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡")
                
                # ç»Ÿè®¡å„è§’è‰²çš„å›¾ç‰‡æ•°é‡
                role_image_count = {}

                for msg_index, message in enumerate(messages_to_process):
                    role = message.get("role", "unknown")
                    content = message.get("content")
                    
                    logger.debug(f"  æ£€æŸ¥æ¶ˆæ¯ #{msg_index + 1} (è§’è‰²: {role})")
                    
                    # å¤„ç†å­—ç¬¦ä¸²å†…å®¹ä¸­çš„Markdownå›¾ç‰‡ï¼ˆå¸¸è§äºassistantè§’è‰²ï¼‰
                    if isinstance(content, str):
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…Markdownæ ¼å¼çš„å›¾ç‰‡ï¼ˆåŒ…æ‹¬base64å’Œhttp URLï¼‰
                        import re
                        # åŒ¹é… ![...](url) æ ¼å¼ï¼ŒåŒ…æ‹¬base64å’Œhttp URL
                        markdown_image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
                        markdown_matches = re.findall(markdown_image_pattern, content)
                        
                        # è¿‡æ»¤å‡ºéœ€è¦ä¸Šä¼ çš„å›¾ç‰‡ï¼ˆåªå¤„ç†base64æ ¼å¼ï¼‰
                        base64_matches = [(alt, url) for alt, url in markdown_matches if url.startswith('data:')]
                        
                        if base64_matches:
                            logger.info(f"  ğŸ“· åœ¨ {role} è§’è‰²çš„å­—ç¬¦ä¸²å†…å®¹ä¸­å‘ç° {len(base64_matches)} ä¸ªéœ€è¦ä¸Šä¼ çš„Markdownæ ¼å¼base64å›¾ç‰‡")
                            markdown_matches = base64_matches  # åªå¤„ç†base64å›¾ç‰‡
                            
                            for match_index, (alt_text, base64_url) in enumerate(markdown_matches):
                                role_image_count[role] = role_image_count.get(role, 0) + 1
                                
                                # æ–°å¢ï¼šæ£€æŸ¥å›¾åºŠURLç¼“å­˜
                                image_hash = calculate_image_hash(base64_url)
                                current_time = time.time()
                                
                                # æ¸…ç†è¿‡æœŸç¼“å­˜
                                if image_hash in FILEBED_URL_CACHE:
                                    cached_url, cache_time = FILEBED_URL_CACHE[image_hash]
                                    if current_time - cache_time < FILEBED_URL_CACHE_TTL:
                                        # ç¼“å­˜å‘½ä¸­ä¸”æœªè¿‡æœŸ
                                        old_markdown = f"![{alt_text}]({base64_url})"
                                        new_markdown = f"![{alt_text}]({cached_url})"
                                        content = content.replace(old_markdown, new_markdown)
                                        message["content"] = content
                                        
                                        show_full_urls = CONFIG.get("debug_show_full_urls", False)
                                        url_display = cached_url if show_full_urls else cached_url[:CONFIG.get("url_display_length", 200)]
                                        logger.info(f"    âš¡ {role} è§’è‰²å­—ç¬¦ä¸²å›¾ç‰‡ä½¿ç”¨ç¼“å­˜URL: {url_display}{'...' if not show_full_urls and len(cached_url) > CONFIG.get('url_display_length', 200) else ''} (å‰©ä½™: {FILEBED_URL_CACHE_TTL - (current_time - cache_time):.0f}ç§’)")
                                        continue  # è·³è¿‡ä¸Šä¼ 
                                    else:
                                        # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                                        del FILEBED_URL_CACHE[image_hash]
                                        logger.debug(f"    ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸç¼“å­˜: {image_hash[:8]}...")
                                
                                # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©å’Œæ’åºç«¯ç‚¹
                                if strategy == "failover":
                                    # æ•…éšœè½¬ç§»ï¼šå›ºå®šä½¿ç”¨å½“å‰ç´¢å¼•çš„å›¾åºŠï¼Œå¤±è´¥åæ‰åˆ‡æ¢
                                    start_index = ROUND_ROBIN_INDEX % len(active_endpoints)
                                    endpoints_to_try = active_endpoints[start_index:] + active_endpoints[:start_index]
                                elif strategy == "round_robin":
                                    # è½®è¯¢ï¼šæ¯æ¬¡éƒ½åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå›¾åºŠ
                                    start_index = ROUND_ROBIN_INDEX % len(active_endpoints)
                                    endpoints_to_try = active_endpoints[start_index:] + active_endpoints[:start_index]
                                    ROUND_ROBIN_INDEX += 1 # åªæœ‰è½®è¯¢æ¨¡å¼æ‰ç«‹å³å¢åŠ ç´¢å¼•
                                else: # é»˜è®¤ä¸º random
                                    endpoints_to_try = random.sample(active_endpoints, len(active_endpoints))

                                upload_successful = False
                                last_error = "æ²¡æœ‰å¯ç”¨çš„å›¾åºŠç«¯ç‚¹ã€‚"
                                
                                for i, endpoint in enumerate(endpoints_to_try):
                                    endpoint_name = endpoint.get("name", "Unknown")
                                    
                                    # è·³è¿‡å·²ä¸´æ—¶ç¦ç”¨çš„
                                    if endpoint_name in DISABLED_ENDPOINTS:
                                        continue
                                    
                                    logger.info(f"    ğŸ“¤ ä¸Šä¼  {role} è§’è‰²å­—ç¬¦ä¸²ä¸­çš„ç¬¬ {match_index + 1} ä¸ªbase64å›¾ç‰‡ï¼Œå°è¯•ä½¿ç”¨ '{endpoint_name}' ç«¯ç‚¹...")
                                    
                                    final_url, error_message = await upload_to_file_bed(
                                        file_name=f"{role}_string_{msg_index}_{match_index}_{uuid.uuid4()}.png",
                                        file_data=base64_url,
                                        endpoint=endpoint
                                    )
                                    
                                    if not error_message:
                                        # æ›¿æ¢åŸå†…å®¹ä¸­çš„base64ä¸ºä¸Šä¼ åçš„URL
                                        old_markdown = f"![{alt_text}]({base64_url})"
                                        new_markdown = f"![{alt_text}]({final_url})"
                                        content = content.replace(old_markdown, new_markdown)
                                        message["content"] = content  # æ›´æ–°æ¶ˆæ¯å†…å®¹
                                        
                                        # æ–°å¢ï¼šå°†ä¸Šä¼ ç»“æœå­˜å…¥ç¼“å­˜
                                        FILEBED_URL_CACHE[image_hash] = (final_url, time.time())
                                        # é™åˆ¶ç¼“å­˜å¤§å°
                                        if len(FILEBED_URL_CACHE) > FILEBED_URL_CACHE_MAX_SIZE:
                                            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                                            sorted_items = sorted(FILEBED_URL_CACHE.items(), key=lambda x: x[1][1])
                                            for old_hash, _ in sorted_items[:FILEBED_URL_CACHE_MAX_SIZE // 4]:
                                                del FILEBED_URL_CACHE[old_hash]
                                            logger.debug(f"    ğŸ§¹ ç¼“å­˜å·²æ»¡ï¼Œæ¸…ç†äº† {FILEBED_URL_CACHE_MAX_SIZE // 4} ä¸ªæ—§æ¡ç›®")
                                        
                                        # æ”¹è¿›URLæ˜¾ç¤º
                                        show_full_urls = CONFIG.get("debug_show_full_urls", False)
                                        url_display = final_url if show_full_urls else final_url[:CONFIG.get("url_display_length", 200)]
                                        logger.info(f"    âœ… {role} è§’è‰²å­—ç¬¦ä¸²ä¸­çš„å›¾ç‰‡æˆåŠŸä¸Šä¼ åˆ° '{endpoint_name}': {url_display}{'...' if not show_full_urls and len(final_url) > CONFIG.get('url_display_length', 200) else ''} (å·²ç¼“å­˜)")
                                        upload_successful = True
                                        break
                                    else:
                                        logger.warning(f"    âš ï¸ ç«¯ç‚¹ '{endpoint_name}' ä¸Šä¼ å¤±è´¥: {error_message}ã€‚ä¸´æ—¶ç¦ç”¨å¹¶å°è¯•ä¸‹ä¸€ä¸ª...")
                                        DISABLED_ENDPOINTS[endpoint_name] = time.time()  # è®°å½•ç¦ç”¨æ—¶é—´
                                        logger.info(f"[FILEBED] {endpoint_name} å·²ç¦ç”¨ï¼Œå½“å‰ç¦ç”¨æ•°: {len(DISABLED_ENDPOINTS)}")
                                        last_error = error_message
                                        # å¦‚æœæ˜¯æ•…éšœè½¬ç§»æ¨¡å¼ï¼Œä¸”å½“å‰å›ºå®šçš„å›¾åºŠå¤±è´¥äº†ï¼Œåˆ™æ›´æ–°ç´¢å¼•
                                        if strategy == "failover" and i == 0:
                                            ROUND_ROBIN_INDEX += 1
                                            logger.info(f"    ğŸ”„ [Failover] é»˜è®¤å›¾åºŠ '{endpoint_name}' å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªã€‚")
                                
                                if not upload_successful:
                                    logger.error(f"    âŒ {role} è§’è‰²å­—ç¬¦ä¸²ä¸­çš„å›¾ç‰‡ä¸Šä¼ å¤±è´¥")
                                    raise IOError(f"æ‰€æœ‰æ´»åŠ¨å›¾åºŠç«¯ç‚¹å‡ä¸Šä¼ å¤±è´¥ã€‚æœ€åé”™è¯¯: {last_error}")
                    
                    # å¤„ç†åˆ—è¡¨å†…å®¹ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                    elif isinstance(content, list):
                        image_count_in_msg = 0
                        for part_index, part in enumerate(content):
                            if part.get("type") == "image_url":
                                url_content = part.get("image_url", {}).get("url")
                                
                                if url_content and url_content.startswith("data:"):
                                    image_count_in_msg += 1
                                    role_image_count[role] = role_image_count.get(role, 0) + 1
                                    
                                    # æ–°å¢ï¼šæ£€æŸ¥å›¾åºŠURLç¼“å­˜
                                    image_hash = calculate_image_hash(url_content)
                                    current_time = time.time()
                                    
                                    # æ¸…ç†è¿‡æœŸç¼“å­˜
                                    if image_hash in FILEBED_URL_CACHE:
                                        cached_url, cache_time = FILEBED_URL_CACHE[image_hash]
                                        if current_time - cache_time < FILEBED_URL_CACHE_TTL:
                                            # ç¼“å­˜å‘½ä¸­ä¸”æœªè¿‡æœŸ
                                            part["image_url"]["url"] = cached_url
                                            
                                            show_full_urls = CONFIG.get("debug_show_full_urls", False)
                                            url_display = cached_url if show_full_urls else cached_url[:CONFIG.get("url_display_length", 200)]
                                            logger.info(f"    âš¡ {role} è§’è‰²åˆ—è¡¨å›¾ç‰‡ä½¿ç”¨ç¼“å­˜URL: {url_display}{'...' if not show_full_urls and len(cached_url) > CONFIG.get('url_display_length', 200) else ''} (å‰©ä½™: {FILEBED_URL_CACHE_TTL - (current_time - cache_time):.0f}ç§’)")
                                            continue  # è·³è¿‡ä¸Šä¼ 
                                        else:
                                            # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                                            del FILEBED_URL_CACHE[image_hash]
                                            logger.debug(f"    ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸç¼“å­˜: {image_hash[:8]}...")
                                    
                                    # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©å’Œæ’åºç«¯ç‚¹
                                    if strategy == "failover":
                                        start_index = ROUND_ROBIN_INDEX % len(active_endpoints)
                                        endpoints_to_try = active_endpoints[start_index:] + active_endpoints[:start_index]
                                    elif strategy == "round_robin":
                                        start_index = ROUND_ROBIN_INDEX % len(active_endpoints)
                                        endpoints_to_try = active_endpoints[start_index:] + active_endpoints[:start_index]
                                        ROUND_ROBIN_INDEX += 1
                                    else:
                                        endpoints_to_try = random.sample(active_endpoints, len(active_endpoints))

                                    upload_successful = False
                                    last_error = "æ²¡æœ‰å¯ç”¨çš„å›¾åºŠç«¯ç‚¹ã€‚"

                                    for i, endpoint in enumerate(endpoints_to_try):
                                        endpoint_name = endpoint.get("name", "Unknown")
                                        
                                        # è·³è¿‡å·²ä¸´æ—¶ç¦ç”¨çš„
                                        if endpoint_name in DISABLED_ENDPOINTS:
                                            continue

                                        logger.info(f"    ğŸ“¤ ä¸Šä¼  {role} è§’è‰²åˆ—è¡¨ä¸­çš„ç¬¬ {image_count_in_msg} ä¸ªbase64å›¾ç‰‡ï¼Œå°è¯•ä½¿ç”¨ '{endpoint_name}' ç«¯ç‚¹...")
                                        
                                        final_url, error_message = await upload_to_file_bed(
                                            file_name=f"{role}_list_{msg_index}_{part_index}_{uuid.uuid4()}.png",
                                            file_data=url_content,
                                            endpoint=endpoint
                                        )

                                        if not error_message:
                                            part["image_url"]["url"] = final_url
                                            
                                            # æ–°å¢ï¼šå°†ä¸Šä¼ ç»“æœå­˜å…¥ç¼“å­˜
                                            FILEBED_URL_CACHE[image_hash] = (final_url, time.time())
                                            # é™åˆ¶ç¼“å­˜å¤§å°
                                            if len(FILEBED_URL_CACHE) > FILEBED_URL_CACHE_MAX_SIZE:
                                                # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                                                sorted_items = sorted(FILEBED_URL_CACHE.items(), key=lambda x: x[1][1])
                                                for old_hash, _ in sorted_items[:FILEBED_URL_CACHE_MAX_SIZE // 4]:
                                                    del FILEBED_URL_CACHE[old_hash]
                                                logger.debug(f"    ğŸ§¹ ç¼“å­˜å·²æ»¡ï¼Œæ¸…ç†äº† {FILEBED_URL_CACHE_MAX_SIZE // 4} ä¸ªæ—§æ¡ç›®")
                                            
                                            # æ”¹è¿›URLæ˜¾ç¤º
                                            show_full_urls = CONFIG.get("debug_show_full_urls", False)
                                            url_display = final_url if show_full_urls else final_url[:CONFIG.get("url_display_length", 200)]
                                            logger.info(f"    âœ… {role} è§’è‰²åˆ—è¡¨ä¸­çš„å›¾ç‰‡æˆåŠŸä¸Šä¼ åˆ° '{endpoint_name}': {url_display}{'...' if not show_full_urls and len(final_url) > CONFIG.get('url_display_length', 200) else ''} (å·²ç¼“å­˜)")
                                            upload_successful = True
                                            break
                                        else:
                                            logger.warning(f"    âš ï¸ ç«¯ç‚¹ '{endpoint_name}' ä¸Šä¼ å¤±è´¥: {error_message}ã€‚ä¸´æ—¶ç¦ç”¨å¹¶å°è¯•ä¸‹ä¸€ä¸ª...")
                                            DISABLED_ENDPOINTS[endpoint_name] = time.time()  # è®°å½•ç¦ç”¨æ—¶é—´
                                            logger.info(f"[FILEBED] {endpoint_name} å·²ç¦ç”¨ï¼Œå½“å‰ç¦ç”¨æ•°: {len(DISABLED_ENDPOINTS)}")
                                            last_error = error_message
                                            if strategy == "failover" and i == 0:
                                                ROUND_ROBIN_INDEX += 1
                                                logger.info(f"    ğŸ”„ [Failover] é»˜è®¤å›¾åºŠ '{endpoint_name}' å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªã€‚")
                                    
                                    if not upload_successful:
                                        logger.error(f"    âŒ {role} è§’è‰²åˆ—è¡¨ä¸­çš„å›¾ç‰‡ä¸Šä¼ å¤±è´¥")
                                        raise IOError(f"æ‰€æœ‰æ´»åŠ¨å›¾åºŠç«¯ç‚¹å‡ä¸Šä¼ å¤±è´¥ã€‚æœ€åé”™è¯¯: {last_error}")

                # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                if role_image_count:
                    logger.info(f"âœ… å›¾ç‰‡é¢„å¤„ç†å®Œæˆã€‚å„è§’è‰²å›¾ç‰‡ç»Ÿè®¡ï¼š{role_image_count}")
                else:
                    logger.info("âœ… å›¾ç‰‡é¢„å¤„ç†å®Œæˆï¼ˆæœªå‘ç°éœ€è¦ä¸Šä¼ çš„base64å›¾ç‰‡ï¼‰")

        # 1. è½¬æ¢è¯·æ±‚ (æ­¤æ—¶å·²ä¸åŒ…å«éœ€è¦ä¸Šä¼ çš„é™„ä»¶)
        lmarena_payload = await convert_openai_to_lmarena_payload(
            openai_req,
            session_id,
            message_id,
            mode_override=mode_override,
            battle_target_override=battle_target_override
        )
        
        # å…³é”®è¡¥å……ï¼šå¦‚æœæ¨¡å‹æ˜¯å›¾ç‰‡ç±»å‹ï¼Œåˆ™å‘æ²¹çŒ´è„šæœ¬æ˜ç¡®æŒ‡å‡º
        if model_type == 'image':
            lmarena_payload['is_image_request'] = True
        
        # 2. åŒ…è£…æˆå‘é€ç»™æµè§ˆå™¨çš„æ¶ˆæ¯
        message_to_browser = {
            "request_id": request_id,
            "payload": lmarena_payload
        }
        
        
        # 3. é€šè¿‡ WebSocket å‘é€
        logger.info(f"API CALL [ID: {request_id[:8]}]: æ­£åœ¨é€šè¿‡ WebSocket å‘é€è½½è·åˆ°æ²¹çŒ´è„šæœ¬ã€‚")
        await browser_ws.send_text(json.dumps(message_to_browser))

        # 4. æ ¹æ® stream å‚æ•°å†³å®šè¿”å›ç±»å‹
        is_stream = openai_req.get("stream", False)

        if is_stream:
            # è¿”å›æµå¼å“åº”ï¼ˆä¼˜åŒ–ç¼“å†²è®¾ç½®ï¼‰
            response = StreamingResponse(
                stream_generator(request_id, model_name or "default_model"),
                media_type="text/event-stream",
                headers={
                    'Cache-Control': 'no-cache',
                    'Connection': 'keep-alive',
                    'X-Accel-Buffering': 'no',  # ç¦ç”¨nginxç¼“å†²
                    'Transfer-Encoding': 'chunked'  # æ˜ç¡®ä½¿ç”¨åˆ†å—ä¼ è¾“
                }
            )
            # ç¦ç”¨FastAPIçš„å“åº”ç¼“å†²
            response.headers['X-Content-Type-Options'] = 'nosniff'
            return response
        else:
            # è¿”å›éæµå¼å“åº”
            result = await non_stream_response(request_id, model_name or "default_model")
            # è®°å½•è¯·æ±‚æˆåŠŸï¼ˆå·²åœ¨non_stream_responseå†…éƒ¨å¤„ç†ï¼‰
            await monitoring_service.broadcast_to_monitors({
                "type": "request_end",
                "request_id": request_id,
                "success": True
            })
            return result
    except (ValueError, IOError) as e:
        # æ•è·é™„ä»¶å¤„ç†é”™è¯¯
        logger.error(f"API CALL [ID: {request_id[:8]}]: é™„ä»¶é¢„å¤„ç†å¤±è´¥: {e}")
        # è®°å½•è¯·æ±‚å¤±è´¥
        monitoring_service.request_end(request_id, success=False, error=str(e))
        await monitoring_service.broadcast_to_monitors({
            "type": "request_end",
            "request_id": request_id,
            "success": False
        })
        if request_id in response_channels:
            del response_channels[request_id]
        # æ¸…ç†å…ƒæ•°æ®
        if request_id in request_metadata:
            del request_metadata[request_id]
            logger.debug(f"API CALL [ID: {request_id[:8]}]: å·²æ¸…ç†è¯·æ±‚å…ƒæ•°æ®")
        # è¿”å›ä¸€ä¸ªæ ¼å¼æ­£ç¡®çš„JSONé”™è¯¯å“åº”
        return JSONResponse(
            status_code=500,
            content={"error": {"message": f"[Luma API Error] Attachment processing failed: {e}", "type": "attachment_error"}}
        )
    except Exception as e:
        # æ•è·æ‰€æœ‰å…¶ä»–é”™è¯¯
        # è®°å½•è¯·æ±‚å¤±è´¥
        monitoring_service.request_end(request_id, success=False, error=str(e))
        await monitoring_service.broadcast_to_monitors({
            "type": "request_end",
            "request_id": request_id,
            "success": False
        })
        if request_id in response_channels:
            del response_channels[request_id]
        # æ¸…ç†å…ƒæ•°æ®
        if request_id in request_metadata:
            del request_metadata[request_id]
            logger.debug(f"API CALL [ID: {request_id[:8]}]: å·²æ¸…ç†è¯·æ±‚å…ƒæ•°æ®")
        logger.error(f"API CALL [ID: {request_id[:8]}]: å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        # ç¡®ä¿ä¹Ÿè¿”å›æ ¼å¼æ­£ç¡®çš„JSON
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_server_error"}}
        )

# --- å†…éƒ¨é€šä¿¡ç«¯ç‚¹ ---
@app.post("/internal/start_id_capture")
async def start_id_capture():
    """
    æ¥æ”¶æ¥è‡ª id_updater.py çš„é€šçŸ¥ï¼Œå¹¶é€šè¿‡ WebSocket æŒ‡ä»¤
    æ¿€æ´»æ²¹çŒ´è„šæœ¬çš„ ID æ•è·æ¨¡å¼ã€‚
    """
    if not browser_ws:
        logger.warning("ID CAPTURE: æ”¶åˆ°æ¿€æ´»è¯·æ±‚ï¼Œä½†æ²¡æœ‰æµè§ˆå™¨è¿æ¥ã€‚")
        raise HTTPException(status_code=503, detail="Browser client not connected.")
    
    try:
        logger.info("ID CAPTURE: æ”¶åˆ°æ¿€æ´»è¯·æ±‚ï¼Œæ­£åœ¨é€šè¿‡ WebSocket å‘é€æŒ‡ä»¤...")
        await browser_ws.send_text(json.dumps({"command": "activate_id_capture"}))
        logger.info("ID CAPTURE: æ¿€æ´»æŒ‡ä»¤å·²æˆåŠŸå‘é€ã€‚")
        return JSONResponse({"status": "success", "message": "Activation command sent."})
    except Exception as e:
        logger.error(f"ID CAPTURE: å‘é€æ¿€æ´»æŒ‡ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to send command via WebSocket.")
# --- ç›‘æ§ç›¸å…³APIç«¯ç‚¹ ---
@app.websocket("/ws/monitor")
async def monitor_websocket(websocket: WebSocket):
    """ç›‘æ§é¢æ¿çš„WebSocketè¿æ¥"""
    await websocket.accept()
    monitoring_service.add_monitor_client(websocket)
    
    try:
        # å‘é€åˆå§‹æ•°æ®
        summary = monitoring_service.get_summary()
        await websocket.send_json({
            "type": "initial_data",
            "stats": summary['stats'],
            "model_stats": summary['model_stats'],
            "active_requests": summary['active_requests_list'],
            "browser_connected": browser_ws is not None,
            "mode": {
                "mode": CONFIG.get("id_updater_last_mode", "direct_chat"),
                "target": CONFIG.get("id_updater_battle_target", "A")
            }
        })
        
        while True:
            # ä¿æŒè¿æ¥
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        monitoring_service.remove_monitor_client(websocket)

@app.get("/monitor", response_class=HTMLResponse)
async def monitor_dashboard():
    """è¿”å›ç›‘æ§é¢æ¿HTMLé¡µé¢"""
    try:
        with open('monitor.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        return HTMLResponse(content=html_content)
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>ç›‘æ§é¢æ¿æ–‡ä»¶æœªæ‰¾åˆ°</h1><p>è¯·ç¡®ä¿ monitor.html æ–‡ä»¶åœ¨æ­£ç¡®çš„ä½ç½®ã€‚</p>",
            status_code=404
        )

@app.get("/api/monitor/stats")
async def get_monitor_stats():
    """è·å–ç›‘æ§ç»Ÿè®¡æ•°æ®"""
    summary = monitoring_service.get_summary()
    return {
        "stats": summary['stats'],
        "model_stats": summary['model_stats'],
        "browser_connected": browser_ws is not None,
        "mode": {
            "mode": CONFIG.get("id_updater_last_mode", "direct_chat"),
            "target": CONFIG.get("id_updater_battle_target", "A")
        }
    }

@app.get("/api/monitor/active")
async def get_active_requests():
    """è·å–æ´»è·ƒè¯·æ±‚åˆ—è¡¨"""
    return monitoring_service.get_active_requests()

@app.get("/api/monitor/logs/requests")
async def get_request_logs(limit: int = 50):
    """è·å–è¯·æ±‚æ—¥å¿—"""
    return monitoring_service.log_manager.read_recent_logs("requests", limit)

@app.get("/api/monitor/logs/errors")
async def get_error_logs(limit: int = 30):
    """è·å–é”™è¯¯æ—¥å¿—"""  
    return monitoring_service.log_manager.read_recent_logs("errors", limit)

@app.get("/api/monitor/recent")
async def get_recent_data():
    """è·å–æœ€è¿‘çš„è¯·æ±‚å’Œé”™è¯¯"""
    return {
        "recent_requests": monitoring_service.get_recent_requests(50),
        "recent_errors": monitoring_service.get_recent_errors(30)
    }

@app.get("/api/monitor/performance")
async def get_performance_metrics():
    """è·å–æ€§èƒ½æŒ‡æ ‡"""
    metrics = {
        "download_semaphore": {
            "max_concurrent": MAX_CONCURRENT_DOWNLOADS,
            "current_active": MAX_CONCURRENT_DOWNLOADS - DOWNLOAD_SEMAPHORE._value if DOWNLOAD_SEMAPHORE else 0,
            "available": DOWNLOAD_SEMAPHORE._value if DOWNLOAD_SEMAPHORE else MAX_CONCURRENT_DOWNLOADS
        },
        "aiohttp_session": {
            "connector_limit": aiohttp_session.connector.limit if aiohttp_session else 0,
            "connector_limit_per_host": aiohttp_session.connector.limit_per_host if aiohttp_session else 0,
            "connector_active": len(aiohttp_session.connector._conns) if aiohttp_session and hasattr(aiohttp_session.connector, '_conns') else 0
        },
        "cache_stats": {
            "image_cache_size": len(IMAGE_BASE64_CACHE),
            "image_cache_max": IMAGE_CACHE_MAX_SIZE,
            "downloaded_urls": len(downloaded_urls_set),
            "response_channels": len(response_channels),
            "disabled_endpoints": len(DISABLED_ENDPOINTS)
        },
        "config": {
            "max_concurrent_downloads": CONFIG.get("max_concurrent_downloads", 50),
            "download_timeout": CONFIG.get("download_timeout", {}),
            "connection_pool": CONFIG.get("connection_pool", {}),
            "memory_management": CONFIG.get("memory_management", {})
        }
    }
    return metrics



# æ–°å¢ï¼šè·å–è¯·æ±‚è¯¦æƒ…çš„APIç«¯ç‚¹
@app.get("/api/request/{request_id}")
async def get_request_details(request_id: str):
    """è·å–ç‰¹å®šè¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯"""
    details = monitoring_service.get_request_details(request_id)
    if details:
        return details
    else:
        raise HTTPException(status_code=404, detail="è¯·æ±‚è¯¦æƒ…æœªæ‰¾åˆ°")

# æ–°å¢ï¼šä¸‹è½½æ—¥å¿—æ–‡ä»¶çš„ç«¯ç‚¹
@app.get("/api/logs/download")
async def download_logs(log_type: str = "requests"):
    """ä¸‹è½½æ—¥å¿—æ–‡ä»¶"""
    from fastapi.responses import FileResponse
    import os
    
    if log_type == "requests":
        log_path = MonitorConfig.LOG_DIR / MonitorConfig.REQUEST_LOG_FILE
    elif log_type == "errors":
        log_path = MonitorConfig.LOG_DIR / MonitorConfig.ERROR_LOG_FILE
    else:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ—¥å¿—ç±»å‹")
    
    if not log_path.exists():
        raise HTTPException(status_code=404, detail="æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=str(log_path),
        filename=f"{log_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl",
        media_type="application/json"
    )


# æ–°å¢ï¼šå›¾ç‰‡åº“APIç«¯ç‚¹
@app.get("/api/images/list")
async def get_image_list():
    """è·å–downloaded_imagesç›®å½•ä¸­çš„å›¾ç‰‡åˆ—è¡¨ï¼ˆåŒ…æ‹¬å­æ–‡ä»¶å¤¹ï¼‰"""
    import os
    from datetime import datetime
    
    images = []
    image_dir = IMAGE_SAVE_DIR
    
    if image_dir.exists():
        # éå†ä¸»ç›®å½•å’Œæ‰€æœ‰å­ç›®å½•
        for item in image_dir.iterdir():
            # å¤„ç†ä¸»ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆå…¼å®¹æ—§æ–‡ä»¶ï¼‰
            if item.is_file() and item.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']:
                stat = item.stat()
                images.append({
                    "filename": item.name,
                    "size": stat.st_size,
                    "modified": stat.st_mtime,
                    "url": f"/api/images/{item.name}",
                    "folder": None  # ä¸»ç›®å½•æ–‡ä»¶
                })
            # å¤„ç†æ—¥æœŸæ–‡ä»¶å¤¹
            elif item.is_dir() and item.name.isdigit() and len(item.name) == 8:  # YYYYMMDDæ ¼å¼
                for file_path in item.iterdir():
                    if file_path.is_file() and file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']:
                        stat = file_path.stat()
                        images.append({
                            "filename": file_path.name,
                            "size": stat.st_size,
                            "modified": stat.st_mtime,
                            "url": f"/api/images/{item.name}/{file_path.name}",
                            "folder": item.name  # æ—¥æœŸæ–‡ä»¶å¤¹å
                        })
    
    # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    images.sort(key=lambda x: x['modified'], reverse=True)
    
    return {
        "total": len(images),
        "images": images
    }

@app.get("/api/images/{filename:path}")
async def get_image(filename: str):
    """è·å–å•ä¸ªå›¾ç‰‡æ–‡ä»¶ï¼ˆæ”¯æŒå­æ–‡ä»¶å¤¹ï¼‰"""
    from fastapi.responses import FileResponse
    
    # æ”¯æŒ "filename" æˆ– "folder/filename" æ ¼å¼
    file_path = IMAGE_SAVE_DIR / filename
    
    if not file_path.exists() or not file_path.is_file():
        raise HTTPException(status_code=404, detail="å›¾ç‰‡æœªæ‰¾åˆ°")
    
    # è·å–æ­£ç¡®çš„åª’ä½“ç±»å‹
    content_type = mimetypes.guess_type(str(file_path))[0] or 'application/octet-stream'
    
    return FileResponse(
        path=str(file_path),
        media_type=content_type,
        filename=filename
    )


# --- æ–°å¢ï¼šå¤„ç†æš‚å­˜è¯·æ±‚çš„åå°ä»»åŠ¡ ---
async def process_pending_requests():
    """åœ¨åå°å¤„ç†æš‚å­˜é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰è¯·æ±‚ã€‚"""
    while not pending_requests_queue.empty():
        pending_item = await pending_requests_queue.get()
        future = pending_item["future"]
        request_data = pending_item["request_data"]
        original_request_id = pending_item.get("original_request_id")  # å¯èƒ½ä¸ºNone
        
        if original_request_id:
            logger.info(f"æ­£åœ¨æ¢å¤è¯·æ±‚ {original_request_id[:8]}...")
        else:
            logger.info("æ­£åœ¨é‡è¯•ä¸€ä¸ªæš‚å­˜çš„è¯·æ±‚...")

        try:
            # é‡æ–°è°ƒç”¨ chat_completions çš„æ ¸å¿ƒé€»è¾‘
            response = await handle_single_completion(request_data)
            
            # å°†æˆåŠŸçš„ç»“æœè®¾ç½®åˆ° future ä¸­ï¼Œä»¥å”¤é†’ç­‰å¾…çš„å®¢æˆ·ç«¯
            future.set_result(response)
            
            if original_request_id:
                logger.info(f"âœ… è¯·æ±‚ {original_request_id[:8]} å·²æˆåŠŸæ¢å¤å¹¶è¿”å›å“åº”ã€‚")
                # æ¸…ç†åŸå§‹è¯·æ±‚çš„å…ƒæ•°æ®å’Œé€šé“
                if original_request_id in response_channels:
                    del response_channels[original_request_id]
                if original_request_id in request_metadata:
                    del request_metadata[original_request_id]
            else:
                logger.info("âœ… ä¸€ä¸ªæš‚å­˜çš„è¯·æ±‚å·²æˆåŠŸé‡è¯•å¹¶è¿”å›å“åº”ã€‚")

        except Exception as e:
            logger.error(f"é‡è¯•æš‚å­˜è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            # å°†é”™è¯¯è®¾ç½®åˆ° future ä¸­ï¼Œä»¥ä¾¿å®¢æˆ·ç«¯çŸ¥é“è¯·æ±‚å¤±è´¥äº†
            future.set_exception(e)
            
            # æ¸…ç†å¤±è´¥è¯·æ±‚çš„æ•°æ®
            if original_request_id:
                if original_request_id in response_channels:
                    del response_channels[original_request_id]
                if original_request_id in request_metadata:
                    del request_metadata[original_request_id]
        
        # æ·»åŠ çŸ­æš‚çš„å»¶è¿Ÿï¼Œé¿å…åŒæ—¶å‘é€è¿‡å¤šè¯·æ±‚
        await asyncio.sleep(1)

async def handle_single_completion(openai_req: dict):
    """
    å¤„ç†å•ä¸ªèŠå¤©è¡¥å…¨è¯·æ±‚çš„æ ¸å¿ƒé€»è¾‘ï¼Œå¯è¢«ä¸»ç«¯ç‚¹å’Œé‡è¯•ä»»åŠ¡å¤ç”¨ã€‚
    """
    # è¿™ä¸ªå‡½æ•°åŒ…å«äº† chat_completions ä¸­çš„å¤§éƒ¨åˆ†é€»è¾‘ï¼Œé™¤äº†è¿æ¥æ£€æŸ¥å’Œæš‚å­˜éƒ¨åˆ†
    model_name = openai_req.get("model")
    
    # ... (ä» chat_completions å¤åˆ¶å¹¶ç²˜è´´æ¨¡å‹/ä¼šè¯IDçš„é€»è¾‘) ...
    # --- æ¨¡å‹ä¸ä¼šè¯IDæ˜ å°„é€»è¾‘ ---
    session_id, message_id = None, None
    mode_override, battle_target_override = None, None

    if model_name and model_name in MODEL_ENDPOINT_MAP:
        mapping_entry = MODEL_ENDPOINT_MAP[model_name]
        selected_mapping = None

        if isinstance(mapping_entry, list) and mapping_entry:
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„è½®è¯¢ç­–ç•¥é€‰æ‹©æ˜ å°„ï¼ˆä¸ä¸»å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
            global MODEL_ROUND_ROBIN_INDEX, MODEL_ROUND_ROBIN_LOCK
            
            # å…³é”®ä¿®å¤ï¼šä½¿ç”¨é”ç¡®ä¿åŸå­æ“ä½œ
            with MODEL_ROUND_ROBIN_LOCK:
                if model_name not in MODEL_ROUND_ROBIN_INDEX:
                    MODEL_ROUND_ROBIN_INDEX[model_name] = 0
                
                current_index = MODEL_ROUND_ROBIN_INDEX[model_name]
                selected_mapping = mapping_entry[current_index]
                
                # è¯Šæ–­æ—¥å¿—ï¼šé‡è¯•æ¢å¤æ—¶çš„è½®è¯¢çŠ¶æ€
                logger.info(f"[RETRY_ROUND_ROBIN] æ¨¡å‹ '{model_name}' é‡è¯•è¯·æ±‚è½®è¯¢:")
                logger.info(f"  - é€‰æ‹©ç´¢å¼•: {current_index}/{len(mapping_entry)}")
                logger.info(f"  - Session IDå6ä½: ...{mapping_entry[current_index].get('session_id', 'N/A')[-6:]}")
                
                # æ›´æ–°ç´¢å¼•ï¼Œå¾ªç¯åˆ°ä¸‹ä¸€ä¸ªï¼ˆåŸå­æ“ä½œå†…å®Œæˆï¼‰
                MODEL_ROUND_ROBIN_INDEX[model_name] = (current_index + 1) % len(mapping_entry)
        elif isinstance(mapping_entry, dict):
            selected_mapping = mapping_entry
        
        if selected_mapping:
            session_id = selected_mapping.get("session_id")
            message_id = selected_mapping.get("message_id")
            mode_override = selected_mapping.get("mode")
            battle_target_override = selected_mapping.get("battle_target")

    if not session_id:
        if CONFIG.get("use_default_ids_if_mapping_not_found", True):
            session_id = CONFIG.get("session_id")
            message_id = CONFIG.get("message_id")
            mode_override, battle_target_override = None, None
        else:
            raise ValueError(f"æ¨¡å‹ '{model_name}' æœªæ‰¾åˆ°æœ‰æ•ˆæ˜ å°„ï¼Œä¸”å·²ç¦ç”¨å›é€€ã€‚")
    
    if not session_id or not message_id or "YOUR_" in session_id or "YOUR_" in message_id:
        raise ValueError("æœ€ç»ˆç¡®å®šçš„ä¼šè¯IDæˆ–æ¶ˆæ¯IDæ— æ•ˆã€‚")

    request_id = str(uuid.uuid4())
    response_channels[request_id] = asyncio.Queue()
    
    monitoring_service.request_start(
        request_id=request_id,
        model=model_name or "unknown",
        messages_count=len(openai_req.get("messages", [])),
        session_id=session_id[-6:] if session_id else None,
        mode=mode_override or CONFIG.get("id_updater_last_mode", "direct_chat"),
        messages=openai_req.get("messages", []),
        params={
            "temperature": openai_req.get("temperature"),
            "top_p": openai_req.get("top_p"),
            "max_tokens": openai_req.get("max_tokens"),
            "streaming": openai_req.get("stream", False)
        }
    )
    
    await monitoring_service.broadcast_to_monitors({
        "type": "request_start", "request_id": request_id, "model": model_name, "timestamp": time.time()
    })

    try:
        messages_to_process = openai_req.get("messages", [])
        for message in messages_to_process:
            content = message.get("content")
            if isinstance(content, list):
                for i, part in enumerate(content):
                    if part.get("type") == "image_url" and CONFIG.get("file_bed_enabled"):
                        # ... (æ–‡ä»¶åºŠä¸Šä¼ é€»è¾‘) ...
                        pass

        lmarena_payload = await convert_openai_to_lmarena_payload(
            openai_req, session_id, message_id,
            mode_override=mode_override, battle_target_override=battle_target_override
        )

        model_type = MODEL_NAME_TO_ID_MAP.get(model_name, {}).get("type", "text")
        if model_type == 'image':
            lmarena_payload['is_image_request'] = True

        message_to_browser = {"request_id": request_id, "payload": lmarena_payload}
        
        if not browser_ws:
            raise ConnectionError("Browser disconnected before sending the payload.")

        await browser_ws.send_text(json.dumps(message_to_browser))

        is_stream = openai_req.get("stream", False)
        if is_stream:
            # ä¼˜åŒ–çš„æµå¼å“åº”é…ç½®
            response = StreamingResponse(
                stream_generator(request_id, model_name or "default_model"),
                media_type="text/event-stream",
                headers={
                    'Cache-Control': 'no-cache',
                    'Connection': 'keep-alive',
                    'X-Accel-Buffering': 'no',  # ç¦ç”¨nginxç¼“å†²
                    'Transfer-Encoding': 'chunked'  # æ˜ç¡®ä½¿ç”¨åˆ†å—ä¼ è¾“
                }
            )
            response.headers['X-Content-Type-Options'] = 'nosniff'
            return response
        else:
            result = await non_stream_response(request_id, model_name or "default_model")
            await monitoring_service.broadcast_to_monitors({
                "type": "request_end", "request_id": request_id, "success": True
            })
            return result
            
    except Exception as e:
        monitoring_service.request_end(request_id, success=False, error=str(e))
        await monitoring_service.broadcast_to_monitors({
            "type": "request_end", "request_id": request_id, "success": False
        })
        if request_id in response_channels:
            del response_channels[request_id]
        # æ¸…ç†å…ƒæ•°æ®
        if request_id in request_metadata:
            del request_metadata[request_id]
            logger.debug(f"è¯·æ±‚ {request_id[:8]} å¤±è´¥ï¼Œå·²æ¸…ç†å…ƒæ•°æ®")
        
        raise e

# --- æ–°å¢ï¼šå›¾ç‰‡ä¸‹è½½è¾…åŠ©å‡½æ•° ---
async def _download_image_data_with_retry(url: str) -> Tuple[Optional[bytes], Optional[str]]:
    """ä¼˜åŒ–çš„å¼‚æ­¥å›¾ç‰‡ä¸‹è½½å™¨ï¼Œå¸¦é‡è¯•å’Œå¹¶å‘æ§åˆ¶"""
    global aiohttp_session, DOWNLOAD_SEMAPHORE
    
    if not DOWNLOAD_SEMAPHORE:
        DOWNLOAD_SEMAPHORE = Semaphore(MAX_CONCURRENT_DOWNLOADS)
    
    last_error = None
    max_retries = CONFIG.get("download_timeout", {}).get("max_retries", 2)
    retry_delays = [1, 2]  # å‡å°‘é‡è¯•å»¶è¿Ÿ
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
    async with DOWNLOAD_SEMAPHORE:
        for retry_count in range(max_retries):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Referer': 'https://lmarena.ai/'
                }
                
                if not aiohttp_session:
                    # åˆ›å»ºç´§æ€¥ä¼šè¯
                    connector = aiohttp.TCPConnector(ssl=False, limit=100, limit_per_host=30)
                    aiohttp_session = aiohttp.ClientSession(connector=connector)
                
                # ä¼˜åŒ–çš„è¶…æ—¶è®¾ç½®ï¼ˆä»é…ç½®è¯»å–ï¼‰
                timeout_config = CONFIG.get("download_timeout", {})
                timeout = aiohttp.ClientTimeout(
                    total=timeout_config.get("total", 30),
                    connect=timeout_config.get("connect", 5),
                    sock_read=timeout_config.get("sock_read", 10)
                )
                
                # æ·»åŠ æ€§èƒ½æ—¥å¿—
                import time as time_module
                start_time = time_module.time()
                
                async with aiohttp_session.get(
                    url,
                    timeout=timeout,
                    headers=headers,
                    allow_redirects=True
                ) as response:
                    if response.status == 200:
                        data = await response.read()
                        download_time = time_module.time() - start_time
                        
                        # è®°å½•æ…¢é€Ÿä¸‹è½½
                        slow_threshold = CONFIG.get("performance_monitoring", {}).get("slow_threshold_seconds", 10)
                        if download_time > slow_threshold:
                            logger.warning(f"[DOWNLOAD] ä¸‹è½½è€—æ—¶è¾ƒé•¿: {download_time:.2f}ç§’")
                        else:
                            logger.debug(f"[DOWNLOAD] ä¸‹è½½æˆåŠŸ: {download_time:.2f}ç§’")
                        
                        return data, None
                    else:
                        last_error = f"HTTP {response.status}"
                        
            except asyncio.TimeoutError:
                last_error = f"è¶…æ—¶ï¼ˆç¬¬{retry_count+1}æ¬¡å°è¯•ï¼‰"
                logger.warning(f"[DOWNLOAD] ä¸‹è½½è¶…æ—¶: {url[:100]}...")
            except aiohttp.ClientError as e:
                last_error = f"ç½‘ç»œé”™è¯¯: {str(e)}"
                logger.warning(f"[DOWNLOAD] ç½‘ç»œé”™è¯¯: {e.__class__.__name__}")
            except Exception as e:
                last_error = str(e)
                logger.error(f"[DOWNLOAD] æœªçŸ¥é”™è¯¯: {e}")
            
            if retry_count < max_retries - 1:
                await asyncio.sleep(retry_delays[retry_count])
    
    return None, last_error

# --- å†…å­˜ç›‘æ§ä»»åŠ¡ ---
async def memory_monitor():
    """ä¼˜åŒ–çš„å†…å­˜ç›‘æ§ä»»åŠ¡"""
    import gc
    import psutil
    import os
    import time as time_module
    
    process = psutil.Process(os.getpid())
    last_gc_time = time_module.time()
    
    while True:
        try:
            # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹çš„ç›‘æ§ï¼‰
            await asyncio.sleep(60)
            
            # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / (1024 * 1024)
            
            # è·å–ä¸‹è½½å¹¶å‘çŠ¶æ€
            active_downloads = MAX_CONCURRENT_DOWNLOADS - DOWNLOAD_SEMAPHORE._value if DOWNLOAD_SEMAPHORE else 0
            
            # è®°å½•å†…å­˜çŠ¶æ€ï¼ˆæ›´è¯¦ç»†çš„ä¿¡æ¯ï¼‰
            logger.info(f"[MEM_MONITOR] å†…å­˜: {memory_mb:.2f}MB | "
                       f"æ´»è·ƒä¸‹è½½: {active_downloads}/{MAX_CONCURRENT_DOWNLOADS} | "
                       f"å“åº”é€šé“: {len(response_channels)} | "
                       f"è¯·æ±‚å…ƒæ•°æ®: {len(request_metadata)} | "
                       f"ç¼“å­˜å›¾ç‰‡: {len(IMAGE_BASE64_CACHE)} | "
                       f"å›¾åºŠURLç¼“å­˜: {len(FILEBED_URL_CACHE)} | "
                       f"ä¸‹è½½å†å²: {len(downloaded_urls_set)}")
            
            # æ–°å¢ï¼šæ¸…ç†è¿‡æœŸçš„å›¾åºŠURLç¼“å­˜
            if len(FILEBED_URL_CACHE) > 0:
                current_time = time_module.time()
                expired_hashes = []
                for img_hash, (url, cache_time) in FILEBED_URL_CACHE.items():
                    if current_time - cache_time > FILEBED_URL_CACHE_TTL:
                        expired_hashes.append(img_hash)
                
                if expired_hashes:
                    for img_hash in expired_hashes:
                        del FILEBED_URL_CACHE[img_hash]
                    logger.info(f"[MEM_MONITOR] æ¸…ç†äº† {len(expired_hashes)} ä¸ªè¿‡æœŸçš„å›¾åºŠURLç¼“å­˜")
            
            # æ–°å¢ï¼šç›‘æ§å’Œæ¸…ç†è¶…æ—¶çš„è¯·æ±‚å…ƒæ•°æ®
            if len(request_metadata) > 10:  # å¦‚æœå…ƒæ•°æ®è¿‡å¤šï¼Œå¯èƒ½æœ‰å†…å­˜æ³„æ¼
                logger.warning(f"[MEM_MONITOR] request_metadataæ•°é‡è¾ƒå¤š: {len(request_metadata)}")
                logger.warning(f"[MEM_MONITOR] å¼€å§‹æ¸…ç†è¶…æ—¶çš„è¯·æ±‚å…ƒæ•°æ®...")
                
                # å®ç°è¶…æ—¶æ¸…ç†é€»è¾‘
                current_time = datetime.now()
                timeout_threshold = CONFIG.get("metadata_timeout_minutes", 30)  # é»˜è®¤30åˆ†é’Ÿè¶…æ—¶
                stale_request_ids = []
                
                for req_id, metadata in request_metadata.items():
                    created_at_str = metadata.get("created_at")
                    if created_at_str:
                        try:
                            created_at = datetime.fromisoformat(created_at_str)
                            age_minutes = (current_time - created_at).total_seconds() / 60
                            
                            if age_minutes > timeout_threshold:
                                stale_request_ids.append(req_id)
                                logger.info(f"[MEM_MONITOR] å‘ç°è¶…æ—¶å…ƒæ•°æ®: {req_id[:8]} (å­˜æ´»: {age_minutes:.1f}åˆ†é’Ÿ)")
                        except (ValueError, TypeError) as e:
                            logger.warning(f"[MEM_MONITOR] æ— æ³•è§£æå…ƒæ•°æ®æ—¶é—´: {req_id[:8]}, é”™è¯¯: {e}")
                            stale_request_ids.append(req_id)  # æ— æ•ˆæ—¶é—´æˆ³ä¹Ÿæ¸…ç†
                
                # æ¸…ç†è¶…æ—¶çš„å…ƒæ•°æ®
                for req_id in stale_request_ids:
                    del request_metadata[req_id]
                    # åŒæ—¶æ¸…ç†å¯¹åº”çš„å“åº”é€šé“ï¼ˆå¦‚æœè¿˜å­˜åœ¨ï¼‰
                    if req_id in response_channels:
                        del response_channels[req_id]
                        logger.debug(f"[MEM_MONITOR] ä¸€å¹¶æ¸…ç†å“åº”é€šé“: {req_id[:8]}")
                
                if stale_request_ids:
                    logger.info(f"[MEM_MONITOR] å·²æ¸…ç† {len(stale_request_ids)} ä¸ªè¶…æ—¶çš„è¯·æ±‚å…ƒæ•°æ®")
                else:
                    logger.info(f"[MEM_MONITOR] æœªå‘ç°è¶…æ—¶å…ƒæ•°æ®ï¼Œä½†æ•°é‡ä»ç„¶è¾ƒå¤šï¼Œå¯èƒ½æ˜¯æ­£å¸¸æƒ…å†µ")
            else:
                logger.debug(f"[MEM_MONITOR] request_metadata: {len(request_metadata)}")
            
            # ä»é…ç½®è¯»å–å†…å­˜ç®¡ç†é˜ˆå€¼
            mem_config = CONFIG.get("memory_management", {})
            gc_threshold = mem_config.get("gc_threshold_mb", 500)
            cache_config = mem_config.get("cache_config", {})
            
            # æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µåŠ¨æ€è°ƒæ•´
            if memory_mb > gc_threshold:
                current_time = time_module.time()
                # é˜²æ­¢è¿‡äºé¢‘ç¹çš„GC
                if current_time - last_gc_time > 300:  # 5åˆ†é’Ÿæœ€å¤šGCä¸€æ¬¡
                    logger.warning(f"[MEM_MONITOR] è§¦å‘åƒåœ¾å›æ”¶ (å†…å­˜: {memory_mb:.2f}MB > {gc_threshold}MB)")
                    
                    # æ¸…ç†å›¾ç‰‡ç¼“å­˜
                    cache_max = cache_config.get("image_cache_max_size", 500)
                    cache_keep = cache_config.get("image_cache_keep_size", 200)
                    if len(IMAGE_BASE64_CACHE) > cache_max:
                        # ä¿ç•™æœ€æ–°çš„æŒ‡å®šæ•°é‡
                        sorted_items = sorted(IMAGE_BASE64_CACHE.items(),
                                            key=lambda x: x[1][1], reverse=True)
                        IMAGE_BASE64_CACHE.clear()
                        for url, data in sorted_items[:cache_keep]:
                            IMAGE_BASE64_CACHE[url] = data
                        logger.info(f"[MEM_MONITOR] æ¸…ç†å›¾ç‰‡ç¼“å­˜: {len(sorted_items)} -> {cache_keep}")
                    
                    # æ¸…ç†ä¸‹è½½è®°å½•
                    url_history_max = cache_config.get("url_history_max", 2000)
                    url_history_keep = cache_config.get("url_history_keep", 1000)
                    if len(downloaded_urls_set) > url_history_max:
                        downloaded_urls_set.clear()
                        # ä¿ç•™æœ€è¿‘çš„è®°å½•
                        downloaded_urls_set.update(list(downloaded_image_urls)[-url_history_keep:])
                        logger.info(f"[MEM_MONITOR] æ¸…ç†ä¸‹è½½è®°å½•: {url_history_max} -> {url_history_keep}")
                    
                    # æ‰§è¡Œåƒåœ¾å›æ”¶
                    gc.collect()
                    last_gc_time = current_time
                    
                    # å†æ¬¡æ£€æŸ¥å†…å­˜
                    new_memory_mb = process.memory_info().rss / (1024 * 1024)
                    logger.info(f"[MEM_MONITOR] GCåå†…å­˜: {memory_mb:.2f}MB -> {new_memory_mb:.2f}MB "
                               f"(é‡Šæ”¾: {memory_mb - new_memory_mb:.2f}MB)")
                    
        except Exception as e:
            logger.error(f"[MEM_MONITOR] é”™è¯¯: {e}")

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # å»ºè®®ä» config.jsonc ä¸­è¯»å–ç«¯å£ï¼Œæ­¤å¤„ä¸ºä¸´æ—¶ç¡¬ç¼–ç 
    api_port = 5102
    logger.info(f"ğŸš€ LMArena Bridge v2.0 API æœåŠ¡å™¨æ­£åœ¨å¯åŠ¨...")
    logger.info(f"   - ç›‘å¬åœ°å€: http://127.0.0.1:{api_port}")
    logger.info(f"   - WebSocket ç«¯ç‚¹: ws://127.0.0.1:{api_port}/ws")
    
    uvicorn.run(app, host="0.0.0.0", port=api_port)
